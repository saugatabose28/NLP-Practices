{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqpYf84yarQI"
   },
   "source": [
    "# COMP 4446 / 5046 Lab 5, 2024\n",
    "\n",
    "Based on:\n",
    "- [\"Text classification from scratch\"](https://keras.io/examples/nlp/text_classification_from_scratch/) by Mark Omernick, Francois Chollet\n",
    "- [\"Bidirectional LSTM on IMDB\"](https://keras.io/examples/nlp/bidirectional_lstm_imdb/) by Francois Chollet\n",
    "- [\"GPT2 Text Generation with KerasNLP\"](https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/) by Chen Qian\n",
    "- [\"GPT text generation from scratch with KerasNLP\"](https://keras.io/examples/generative/text_generation_gpt/) by Jesse Chan\n",
    "\n",
    "Make sure you are using a GPU for this lab (otherwise some of the later steps will take a long time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X24BBYtWarQK"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, we will revisit classifiers and RNNs, but using the Keras library, which is built on top of Tensorflow (rather than PyTorch).\n",
    "\n",
    "We'll see two examples of getting data and making a model to do movie review prediction on IMDB data.\n",
    "\n",
    "First, we'll work from raw text - a set of text files on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyHNHczmarQL"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "isJPP7FRcYDO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/keras-team/keras-nlp.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VQbiuBuXarQM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.8.0\n",
      "  Downloading tensorflow_gpu-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[K     |███████████████████▌            | 303.2 MB 150.5 MB/s eta 0:00:02     |████████████▌                   | 194.1 MB 137.0 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 497.6 MB 9.8 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (24.3.7)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (0.36.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (3.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (1.16.0)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 93.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (1.26.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (2.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (1.62.1)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (0.5.4)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (1.4.0)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (68.0.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (18.1.1)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 3.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (4.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (1.16.0)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 70.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 71.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-gpu==2.8.0) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-gpu==2.8.0) (0.41.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 55.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 83.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (3.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (2.31.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "\u001b[K     |████████████████████████████████| 189 kB 111.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (3.6)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 97.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.4.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (3.16.2)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 108.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow-gpu==2.8.0) (2.1.3)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, tf-estimator-nightly, tensorboard, keras-preprocessing, keras, tensorflow-gpu\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.1.0\n",
      "    Uninstalling keras-3.1.0:\n",
      "      Successfully uninstalled keras-3.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.1 requires keras>=3.0.0, but you have keras 2.8.0 which is incompatible.\n",
      "tensorflow 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.8.0 which is incompatible.\u001b[0m\n",
      "Successfully installed cachetools-5.3.3 google-auth-2.29.0 google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 oauthlib-3.2.2 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.4.0 rsa-4.9 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-gpu-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import keras_nlp\n",
    "\n",
    "!pip install tensorflow-gpu==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kohvZqLxarQM"
   },
   "source": [
    "## Load the data: IMDB movie review sentiment classification\n",
    "\n",
    "Let's download the data and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4VYQpT8iarQN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  15.0M      0  0:00:05  0:00:05 --:--:-- 19.5M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_ZFmGAParQN"
   },
   "source": [
    "The `aclImdb` folder contains a `train` and `test` subfolder, plus a few text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rOHhquKIarQO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README\timdb.vocab  imdbEr.txt\ttest  train\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkL_w2VQ58kD"
   },
   "source": [
    "Inside each folder are subfolders for positive and negative data, plus files with precomputed features, and source URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "orGalmZOarQO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MscZ-nJyarQP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  pos\t\turls_neg.txt  urls_unsup.txt\n",
      "neg\t\t unsupBow.feat\turls_pos.txt\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbZ2QxEqarQP"
   },
   "source": [
    "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n",
    " which represents one review (either positive or negative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YjuKMrirarQP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/6248_7.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iBKQJsFarQP"
   },
   "source": [
    "We are only interested in the `pos` and `neg` subfolders, so let's delete the other subfolder that has text files in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZtAgqyUiarQP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'aclImdb/train/unsup': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6VH8ZO3arQP"
   },
   "source": [
    "You can use the utility `keras.utils.text_dataset_from_directory` to\n",
    "generate a labeled `tf.data.Dataset` object from a set of text files on disk filed into class-specific folders.\n",
    "\n",
    "Let's use it to generate the training, validation, and test datasets. The validation and training datasets are generated from two subsets of the `train` directory, with 20% of samples going to the validation dataset and 80% going to the training dataset. When using the `validation_split` & `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation & training splits you get have no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qSciyMhQarQP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25001 files belonging to 2 classes.\n",
      "Number of batches in raw_train_ds: 625\n",
      "Number of batches in raw_val_ds: 157\n",
      "Number of batches in raw_test_ds: 782\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9FBZUeSarQQ"
   },
   "source": [
    "Let's look at a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_YNS57gZarQQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'I\\'ve seen tons of science fiction from the 70s; some horrendously bad, and others thought provoking and truly frightening. Soylent Green fits into the latter category. Yes, at times it\\'s a little campy, and yes, the furniture is good for a giggle or two, but some of the film seems awfully prescient. Here we have a film, 9 years before Blade Runner, that dares to imagine the future as somthing dark, scary, and nihilistic. Both Charlton Heston and Edward G. Robinson fare far better in this than The Ten Commandments, and Robinson\\'s assisted-suicide scene is creepily prescient of Kevorkian and his ilk. Some of the attitudes are dated (can you imagine a filmmaker getting away with the \"women as furniture\" concept in our oh-so-politically-correct-90s?), but it\\'s rare to find a film from the Me Decade that actually can make you think. This is one I\\'d love to see on the big screen, because even in a widescreen presentation, I don\\'t think the overall scope of this film would receive its due. Check it out.'\n",
      "1\n",
      "b'First than anything, I\\'m not going to praise I\\xc3\\xb1arritu\\'s short film, even I\\'m Mexican and proud of his success in mainstream Hollywood.<br /><br />In another hand, I see most of the reviews focuses on their favorite (and not so) short films; but we are forgetting that there is a subtle bottom line that circles the whole compilation, and maybe it will not be so pleasant for American people. (Even if that was not the main purpose of the producers) <br /><br />What i\\'m talking about is that most of the short films does not show the suffering that WASP people went through because the terrorist attack on September 11th, but the suffering of the Other people.<br /><br />Do you need proofs about what i\\'m saying? Look, in the Bosnia short film, the message is: \"You cry because of the people who died in the Towers, but we (The Others = East Europeans) are crying long ago for the crimes committed against our women and nobody pay attention to us like the whole world has done to you\".<br /><br />Even though the Burkina Fasso story is more in comedy, there is a the same thought: \"You are angry because Osama Bin Laden punched you in an evil way, but we (The Others = Africans) should be more angry, because our people is dying of hunger, poverty and AIDS long time ago, and nobody pay attention to us like the whole world has done to you\".<br /><br />Look now at the Sean Penn short: The fall of the Twin Towers makes happy to a lonely (and alienated) man. So the message is that the Power and the Greed (symbolized by the Towers) must fall for letting the people see the sun rise and the flowers blossom? It is remarkable that this terrible bottom line has been proposed by an American. There is so much irony in this short film that it is close to be subversive.<br /><br />Well, the Ken Loach (very know because his anti-capitalism ideology) is much more clearly and shameless in going straight to the point: \"You are angry because your country has been attacked by evil forces, but we (The Others = Latin Americans) suffered at a similar date something worst, and nobody remembers our grief as the whole world has done to you\".<br /><br />It is like if the creative of this project wanted to say to Americans: \"You see now, America? You are not the only that have become victim of the world violence, you are not alone in your pain and by the way, we (the Others = the Non Americans) have been suffering a lot more than you from long time ago; so, we are in solidarity with you in your pain... and by the way, we are sorry because you have had some taste of your own medicine\" Only the Mexican and the French short films showed some compassion and sympathy for American people; the others are like a slap on the face for the American State, that is not equal to American People.'\n",
      "1\n",
      "b'Blood Castle (aka Scream of the Demon Lover, Altar of Blood, Ivanna--the best, but least exploitation cinema-sounding title, and so on) is a very traditional Gothic Romance film. That means that it has big, creepy castles, a headstrong young woman, a mysterious older man, hints of horror and the supernatural, and romance elements in the contemporary sense of that genre term. It also means that it is very deliberately paced, and that the film will work best for horror mavens who are big fans of understatement. If you love films like Robert Wise\\'s The Haunting (1963), but you also have a taste for late 1960s/early 1970s Spanish and Italian horror, you may love Blood Castle, as well.<br /><br />Baron Janos Dalmar (Carlos Quiney) lives in a large castle on the outskirts of a traditional, unspecified European village. The locals fear him because legend has it that whenever he beds a woman, she soon after ends up dead--the consensus is that he sets his ferocious dogs on them. This is quite a problem because the Baron has a very healthy appetite for women. At the beginning of the film, yet another woman has turned up dead and mutilated.<br /><br />Meanwhile, Dr. Ivanna Rakowsky (Erna Sch\\xc3\\xbcrer) has appeared in the center of the village, asking to be taken to Baron Dalmar\\'s castle. She\\'s an out-of-towner who has been hired by the Baron for her expertise in chemistry. Of course, no one wants to go near the castle. Finally, Ivanna finds a shady individual (who becomes even shadier) to take her. Once there, an odd woman who lives in the castle, Olga (Cristiana Galloni), rejects Ivanna and says that she shouldn\\'t be there since she\\'s a woman. Baron Dalmar vacillates over whether she should stay. She ends up staying, but somewhat reluctantly. The Baron has hired her to try to reverse the effects of severe burns, which the Baron\\'s brother, Igor, is suffering from.<br /><br />Unfortunately, the Baron\\'s brother appears to be just a lump of decomposing flesh in a vat of bizarre, blackish liquid. And furthermore, Ivanna is having bizarre, hallucinatory dreams. Just what is going on at the castle? Is the Baron responsible for the crimes? Is he insane? <br /><br />I wanted to like Blood Castle more than I did. As I mentioned, the film is very deliberate in its pacing, and most of it is very understated. I can go either way on material like that. I don\\'t care for The Haunting (yes, I\\'m in a very small minority there), but I\\'m a big fan of 1960s and 1970s European horror. One of my favorite directors is Mario Bava. I also love Dario Argento\\'s work from that period. But occasionally, Blood Castle moved a bit too slow for me at times. There are large chunks that amount to scenes of not very exciting talking alternated with scenes of Ivanna slowly walking the corridors of the castle.<br /><br />But the atmosphere of the film is decent. Director Jos\\xc3\\xa9 Luis Merino managed more than passable sets and locations, and they\\'re shot fairly well by Emanuele Di Cola. However, Blood Castle feels relatively low budget, and this is a Roger Corman-produced film, after all (which usually means a low-budget, though often surprisingly high quality \"quickie\"). So while there is a hint of the lushness of Bava\\'s colors and complex set decoration, everything is much more minimalist. Of course, it doesn\\'t help that the Retromedia print I watched looks like a 30-year old photograph that\\'s been left out in the sun too long. It appears \"washed out\", with compromised contrast.<br /><br />Still, Merino and Di Cola occasionally set up fantastic visuals. For example, a scene of Ivanna walking in a darkened hallway that\\'s shot from an exaggerated angle, and where an important plot element is revealed through shadows on a wall only. There are also a couple Ingmar Bergmanesque shots, where actors are exquisitely blocked to imply complex relationships, besides just being visually attractive and pulling your eye deep into the frame.<br /><br />The performances are fairly good, and the women--especially Sch\\xc3\\xbcrer--are very attractive. Merino exploits this fact by incorporating a decent amount of nudity. Sch\\xc3\\xbcrer went on to do a number of films that were as much soft corn porn as they were other genres, with English titles such as Sex Life in a Woman\\'s Prison (1974), Naked and Lustful (1974), Strip Nude for Your Killer (1975) and Erotic Exploits of a Sexy Seducer (1977). Blood Castle is much tamer, but in addition to the nudity, there are still mild scenes suggesting rape and bondage, and of course the scenes mixing sex and death.<br /><br />The primary attraction here, though, is probably the story, which is much a slow-burning romance as anything else. The horror elements, the mystery elements, and a somewhat unexpected twist near the end are bonuses, but in the end, Blood Castle is a love story, about a couple overcoming various difficulties and antagonisms (often with physical threats or harms) to be together.'\n",
      "1\n",
      "b\"I was talked into watching this movie by a friend who blubbered on about what a cute story this was.<br /><br />Yuck.<br /><br />I want my two hours back, as I could have done SO many more productive things with my time...like, for instance, twiddling my thumbs. I see nothing redeeming about this film at all, save for the eye-candy aspect of it...<br /><br />3/10 (and that's being generous)\"\n",
      "0\n",
      "b\"Michelle Rodriguez is the defining actress who could be the charging force for other actresses to look out for. She has the audacity to place herself in a rarely seen tough-girl role very early in her career (and pull it off), which is a feat that should be recognized. Although her later films pigeonhole her to that same role, this film was made for her ruggedness.<br /><br />Her character is a romanticized student/fighter/lover, struggling to overcome her disenchanted existence in the projects, which is a little overdone in film...but not by a girl. That aspect of this film isn't very original, but the story goes in depth when the heated relationships that this girl has to deal with come to a boil and her primal rage takes over.<br /><br />I haven't seen an actress take such an aggressive stance in movie-making yet, and I'm glad that she's getting that original twist out there in Hollywood. This film got a 7 from me because of the average story of ghetto youth, but it has such a great actress portraying a rarely-seen role in a minimal budget movie. Great work.\"\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 06:05:05.830667: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QARikvbParQQ"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We will use the `TextVectorization` layer for word splitting & indexing. In the process, we will also remove `<br />` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5Xf2fLmQarQQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 06:05:42.170907: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "# Having looked at our data above, we see that the raw text contains HTML break\n",
    "# tags of the form '<br />'. These tags will not be removed by the default\n",
    "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# create a custom standardization function.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d5lQyCAarQQ"
   },
   "source": [
    "## Two options to vectorize the data\n",
    "\n",
    "There are 2 ways we can use our text vectorization layer:\n",
    "\n",
    "**Option 1: Make it part of the model**, so as to obtain a model that processes raw strings, like this:\n",
    "\n",
    "```python\n",
    "text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "x = vectorize_layer(text_input)\n",
    "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
    "...\n",
    "```\n",
    "\n",
    "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then feed it into a model that expects integer sequences as inputs.\n",
    "\n",
    "An important difference between the two is that option 2 enables you to do\n",
    "**asynchronous CPU processing and buffering** of your data when training on a GPU. So if you're training a model on a GPU, you probably want to go with this option to get the best performance. This is what we will do below.\n",
    "\n",
    "If we were to export our model to production, we'd ship a model that accepts rawstrings as input, like in the code snippet for option 1 above. This can be done after training. We do this in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tSYb7P0barQR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   11  1600    10   506    17     2   239   102    11    25   121   107\n",
      "    11   252     4  3550  1470   766    97    25   222     4   123   278\n",
      "     2   418    19    11    25   121   107     3    11    25   107    46\n",
      "    79   513   154   610   539    11  1456  8762     6   881     9     3\n",
      "    12    13     1    99    72     2   112     7   506     2   101    24\n",
      "   418     3     2    19     7    41     4   402     5  1194     2     1\n",
      "   314   135    24    37   486   413    12     9   155     4  1010     1\n",
      "   164    38    33   793  5861    31    11    68   130     7    44    22\n",
      "     1     6   415     4    49  2100     3     4   111   273   137   881\n",
      "    10   506   503    11    59   237   103   453 16096    36  4458   821\n",
      "   132  1242     8     4  9663     5 18606    70   850   139    10  4037\n",
      "   170     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "0\n",
      "[  11 1073  214  106  634  145    4   17   12   11  194   16   31    2\n",
      "   49  150    8    9   59   27  277  145   11  412  252    9   50    2\n",
      "   17 1023    3   11   65  417   57  312   48   65    1   13 1181   82\n",
      "   11   97   25  329   12   58  380  140    1  771    6  814    9   31\n",
      "   45   18   63   65   57 2180 1185    6  143   35 2559    9   45    3\n",
      "   25 1776    1   40  347   29  123   70   21 1230  231   35   13    2\n",
      "  721    8    2   17   16  451 1161   12   71  215    4  363    5  204\n",
      " 1858  242   88  116   85  721  118   12   39  653   13 4081   20   39\n",
      "   30    2  125    5    2   17 1990  296   54   59  859  161   45    2\n",
      "  857  120   10  382   12   67  162    6   27 5174    6  125  515  300\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 06:05:45.502292: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   10     7   240   157     1 15001 13365    19    11   235    21   241\n",
      "   133    26    43    10  2924     6   119   511     1     3     1  5617\n",
      "     6     2  1398     1   304    18    11   161    46     5   143   135\n",
      "    63  2206   704   158     9     7     4   318   402   265   291    32\n",
      " 13365   115    26   262     2    62     5    33   701  2476   250    32\n",
      "   975   721     8     2   789    19   444  2079     1    48   155    10\n",
      "    17   269     7    12     9    80  3167     2   527     5    78    12\n",
      "    10  2476     1    16    40     7   345    16   136    14    39     1\n",
      "    39  6017    19   377  4148   992    78     3   379  2394     8     2\n",
      "   701     1     8    39  3807  7222     9     7   157    49   232    36\n",
      "  2079     1    54     7  1274     6    27  1905    15    10   207    54\n",
      "   120 14815   533    18  3103     8    93    54     7 10683     1     3\n",
      "   326   299     8    10    19    18    91   701 10641    24    21   405\n",
      "     6 19997     2    78    34  1111   201  6908 15001    43   206    56\n",
      "    16   157    49    17    12   919  1031  1261     6     2 18855    52\n",
      "  1762   187    10    17  1942  1120     3    28     7    21   241    48\n",
      "     2   172     7   256     6  2808     7    26   256     6   119   168\n",
      "     2 14394     3 12673     5     2   992    78    40     7    26   256\n",
      "     6   119   168     2   117     5    33   701  2476     3  9900    12\n",
      "    16     2   117     5     2   144   835  2476     7    26   256     6\n",
      "   361   168    88     2  1350     3   992  1532  2415     2  3558    40\n",
      "     7    26   256     6  9094     2  1261    16   511  2625     3     1\n",
      "  3893     2    81  5617   802  2542  1425     2     1  2415    12   180\n",
      "   364    25     6  1663    16   668    40  5229    44    34   607     6\n",
      "  2503  2954     8  1471    26  2226    42    88  3051 11187    76  1202\n",
      "     3  1383     5    64  2603   100     4   132     3  4500     6  1545\n",
      " 10067    32  8262    72  8907    78   520    26    80  2226    42  4403\n",
      "   364     3 11654   328     8   992  2043    10    13    31   851    15\n",
      "    69    18   228    27    99  1279    15    33   869 13077    35    41\n",
      "   474     6 13545    46  4062    36   255     6   255   156     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "1\n",
      "[   10    17    13     2   239    17   121    90    20     2  1183    11\n",
      "    38  7553    51    70    10    17     2  3049  2685   360     2    17\n",
      "     7  1141     2  2303  2685     3   126    31    11    13  1565     6\n",
      "  4809     2    78    12    90    10     1    83    21   415   634     5\n",
      "   122   117   145    10    17     2    60    49   171    13    50     2\n",
      "    17  1023    10    17     7  1969  8841  8443  1481   480  8841 10628\n",
      "     3     1  8841  1001   574    83    21   415   122    58   145    10\n",
      "    17    22    74  2569     1   178     6   118   133    10    17  1833\n",
      "    73     2  1111  1911     2  3049  1911     2   538   604    11   382\n",
      "     7 12609     2    78   771     6  2533   524    56     2  1140  1396\n",
      "   102    47    31    12     3    68   849    56     6     2  1590    11\n",
      "   382    47    13     4   674 16790  4761   635   795    30     4   246\n",
      "    54  2838     4 14122    30     9    36  1270 16790   242   855  2266\n",
      "    22    68   159   638     9     2   112    13   506    99     1  1481\n",
      "     7   159     4    49    17     3    10    41    65     6   137     3\n",
      "  2403     9     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 06:05:45.795457: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-03-20 06:05:45.801316: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVxJQaiyarQR"
   },
   "source": [
    "## Build a model\n",
    "\n",
    "The first model we introduce uses a Convolutional Neural Network. This has not been covered in lectures (you can read more about it on [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network#Building_blocks)). The general idea is the a convolution is a type of weighted average that is applied in multiple locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tD_6m5a6arQR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a hidden layer (linear + ReLU):\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "# Dropout makes some values 0 during training, which tends to help improve how well the model generalised\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzXh1wgWarQR"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We'll run just 3 epochs / iterations so you can see how it works. Later, if you have more time you can run it for longer to see how well it can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "636ufpx9arQR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 325ms/step - accuracy: 0.6075 - loss: 0.6126 - val_accuracy: 0.8704 - val_loss: 0.3042\n",
      "Epoch 2/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 317ms/step - accuracy: 0.8856 - loss: 0.2764 - val_accuracy: 0.8792 - val_loss: 0.3122\n",
      "Epoch 3/3\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 315ms/step - accuracy: 0.9449 - loss: 0.1500 - val_accuracy: 0.8762 - val_loss: 0.3598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f23474ebe50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdPBTqouarQR"
   },
   "source": [
    "## Evaluate the model on the test set\n",
    "\n",
    "We also evaluate on the test data. The accuracy is lower than on the training data because the test data is unseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oMt-xBZWarQR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 68ms/step - accuracy: 0.8696 - loss: 0.3709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.380510538816452, 0.8656853437423706]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDnulw0VarQR"
   },
   "source": [
    "## Make an end-to-end model\n",
    "\n",
    "If you want to obtain a model capable of processing raw strings, you can create a new model using the weights we just trained, but a different input type and an extra vectorisation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NBPZTZYXarQR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 68ms/step - accuracy: 0.8693 - loss: 0.3721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3802413046360016, 0.8656853437423706]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A string input\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PaAnP6QbzJJ"
   },
   "source": [
    "# LSTM\n",
    "\n",
    "Now let's see how to construct an LSTM in Keras.\n",
    "\n",
    "First, we'll set up some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GULwBe7mbyoX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrDie0iCb4yU"
   },
   "source": [
    "Keras has code for the LSTM already, so we don't need to implement all the details ourselves.\n",
    "\n",
    "The code below sets up the model, using integers as input (ie., like at the start above, we assume someone else is converting our input words into word IDs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "O3MElaM3b47K"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,658,945</span> (10.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,658,945\u001b[0m (10.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,658,945</span> (10.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,658,945\u001b[0m (10.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add a bidirectional LSTM\n",
    "# return_sequences determines whether we get just the last vector or all:\n",
    "#    True - give us an output for each word\n",
    "#    False - give us just one output\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "O3MElaM3b47K"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,658,945</span> (10.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,658,945\u001b[0m (10.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,658,945</span> (10.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,658,945\u001b[0m (10.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add a bidirectional LSTM\n",
    "# return_sequences determines whether we get just the last vector or all:\n",
    "#    True - give us an output for each word\n",
    "#    False - give us just one output\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCMLC_L6b9nj"
   },
   "source": [
    "Load the IMDB movie review sentiment data. This time we are going to use the library provided version of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GAUu0BvSb9um"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "# Use pad_sequence to standardize sequence length:\n",
    "# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1zpmPANcCrd"
   },
   "source": [
    "Train and evaluate the model. You will see similar results to the approach above. To do better, we would need to train for longer and explore other variations on the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hGD8LbckcCxF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 560ms/step - accuracy: 0.7276 - loss: 0.5165 - val_accuracy: 0.8043 - val_loss: 0.4279\n",
      "Epoch 2/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 546ms/step - accuracy: 0.8759 - loss: 0.3037 - val_accuracy: 0.8645 - val_loss: 0.3204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f23449fca30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdjC-_gW46gc"
   },
   "source": [
    "# Task 1 - Multilayer LSTM\n",
    "\n",
    "Modify the LSTM architecture to have two layers. You can do this by having an extra layer that produces output at each position, which is fed into the next layer as an input.\n",
    "\n",
    "You should print the model summary to show for credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_sW8bik146wZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │     \u001b[38;5;34m2,560,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,757,761</span> (10.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,757,761\u001b[0m (10.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,757,761</span> (10.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,757,761\u001b[0m (10.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add a bidirectional LSTM\n",
    "# return_sequences determines whether we get just the last vector or all:\n",
    "#    True - give us an output for each word\n",
    "#    False - give us just one output\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEOTAg-kcHIR"
   },
   "source": [
    "# Text generation\n",
    "\n",
    "Now, we are going to turn to text generation to see some variations on inference.\n",
    "\n",
    "You will learn to load a pre-trained  Language Model (LM) - [GPT-2](https://openai.com/research/better-language-models) (originally invented by OpenAI), finetune it to a specific text style, and generate text based on users' input (also known as prompt). You will also see how GPT2 adapts quickly to non-English languages, such as Chinese.\n",
    "\n",
    "This examples uses [Keras Core](https://keras.io/keras_core/) to work in any of `\"tensorflow\"`, `\"jax\"` or `\"torch\"`. Support for Keras Core is baked into KerasNLP, simply change the `\"KERAS_BACKEND\"` environment variable to select the backend of your choice. We select the JAX backend below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pOgwGNqLcHW6"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd-WEdGzcuLA"
   },
   "source": [
    "Large Language Models are complex to build and expensive to train from scratch. Luckily there are pretrained LLMs available for use right away. Keras provides a large number of pre-trained checkpoints that allow you to experiment with very good models without needing to train them yourself.\n",
    "\n",
    "Keras also  has a Sampler class that implements generation algorithms such as Top-K, Beam and contrastive search. These samplers can be used to generate text with custom models.\n",
    "\n",
    "## Load a pre-trained GPT-2 model and generate some text\n",
    "\n",
    "KerasNLP provides a number of pre-trained models, such as [Google\n",
    "Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
    "and [GPT-2](https://openai.com/research/better-language-models). You can see\n",
    "the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n",
    "\n",
    "It's very easy to load the GPT-2 model as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X-ENeVeHcz8G"
   },
   "outputs": [],
   "source": [
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5kQdj4Vc1sF"
   },
   "source": [
    "Once the model is loaded, you can use it to generate some text right away. Run\n",
    "the cells below to give it a try. It's as simple as calling a single function\n",
    "*generate()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5x-8z1bkc1yz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710920555.953907      97 service.cc:145] XLA service 0x5627f8105c30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1710920555.953987      97 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "2024-03-20 07:42:36.301874: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1710920572.369620      97 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "My trip to Yosemite was the most amazing experience. I was so excited to see what was out there for the first time. The view is beautiful, the views are breathtaking, and the people are wonderful. I was able to see the beautiful views of Mount El Capitan and the beautiful mountains of Yosemite. It's a great place to visit if you love to see Yosemite and the surrounding region. I would recommend it to anyone who loves to see the beauty and beauty of the Yosemite area.\n",
      "\n",
      "I was really excited to see the view and the people. I have always been a big fan of the Yosemite Valley. It is so beautiful and it has a lot to offer. I was able to get to know some of the people and see what is happening there. The view was beautiful, and the people were wonderful to meet and meet. I am a big fan of the Sierra Club, and I have seen many of the Sierra Club's hikes there. I was able to get to know\n",
      "TOTAL TIME ELAPSED: 37.02s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVEJgX-uc4XO"
   },
   "source": [
    "Try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1CNgp9zWc4fD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-2 output:\n",
      "That Italian restaurant is a little more than a restaurant in itself. It's a place that serves food and drinks to a very specific group of people in Italy. The food is very good, and the drinks are very good and the drinks are really good. The menu is very clear and the menu is very simple, so I'm not sure if it's a good or a bad thing for us to be here. It's not like I'm going to be here for the rest of my life, but I think it's going to be great. I think it will be a good place to go for dinner, and I think the food will be a good thing for everyone.\n",
      "\n",
      "I'm not sure how much more it will change for everyone, but it will be great.\n",
      "\n",
      "This is what I'm looking forward to seeing next time I'm out there.\n",
      "TOTAL TIME ELAPSED: 25.38s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY5eaLhoc8aQ"
   },
   "source": [
    "Notice how much faster the second call is. This is because the computational\n",
    "graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and\n",
    "re-used in the 2nd behind the scenes.\n",
    "\n",
    "The quality of the generated text looks OK, but we can improve it via\n",
    "fine-tuning.\n",
    "\n",
    "## More on the GPT-2 model from KerasNLP\n",
    "\n",
    "Next up, we will actually fine-tune the model to update its parameters, but\n",
    "before we do, let's take a look at the full set of tools we have to for working\n",
    "with for GPT2.\n",
    "\n",
    "The code of GPT2 can be found\n",
    "[here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n",
    "Conceptually the `GPT2CausalLM` can be hierarchically broken down into several\n",
    "modules in KerasNLP, all of which have a *from_preset()* function that loads a\n",
    "pretrained model:\n",
    "\n",
    "- `keras_nlp.models.GPT2Tokenizer`: The tokenizer used by GPT2 model, which is a\n",
    "    [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n",
    "- `keras_nlp.models.GPT2CausalLMPreprocessor`: the preprocessor used by GPT2\n",
    "    causal LM training. It does the tokenization along with other preprocessing\n",
    "    works such as creating the label and appending the end token.\n",
    "- `keras_nlp.models.GPT2Backbone`: the GPT2 model, which is a stack of\n",
    "    `keras_nlp.layers.TransformerDecoder`. This is usually just referred as\n",
    "    `GPT2`.\n",
    "- `keras_nlp.models.GPT2CausalLM`: wraps `GPT2Backbone`, it multiplies the\n",
    "    output of `GPT2Backbone` by embedding matrix to generate logits over\n",
    "    vocab tokens.\n",
    "\n",
    "## Finetune on News data\n",
    "\n",
    "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one step further to finetune the model so that it generates text in a specific style, short or long, strict or casual. In this tutorial, we will use a news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3WgcqFIcdBlB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.9.3)\n",
      "Requirement already satisfied: dm-tree in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: array-record in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (0.5.0)\n",
      "Requirement already satisfied: promise in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: absl-py in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (3.20.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (4.66.2)\n",
      "Requirement already satisfied: toml in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: wrapt in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: click in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: psutil in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (5.9.8)\n",
      "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow_datasets) (1.5.2)\n",
      "Requirement already satisfied: typing_extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.10.0)\n",
      "Requirement already satisfied: zipp in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.16.2)\n",
      "Requirement already satisfied: fsspec in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2024.2.0)\n",
      "Requirement already satisfied: importlib_resources in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.63.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "news_ds = tfds.load(\"ag_news_subset\", split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kasniHyddBs_"
   },
   "source": [
    "Let's take a look inside sample data from the ag_news TensorFlow Dataset. Thereare two features:\n",
    "\n",
    "- **__description__**: text of the article.\n",
    "- **__label__**: the category (used in earlier labs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "acAey1ypdDc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 22:20:28.485897: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for description, label in news_ds:\n",
    "    print(description.numpy())\n",
    "    print(label.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW2QrA8DdDmB"
   },
   "source": [
    "In our case, we are performing next word prediction in a language model, so we\n",
    "only need the 'description' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8gKCksWpdGeL"
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    news_ds.map(lambda description, _: description)\n",
    "    .batch(32)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64jFwdc5dIGb"
   },
   "source": [
    "Now you can finetune the model using the familiar *fit()* function. Note that\n",
    "`preprocessor` will be automatically called inside `fit` method since\n",
    "`GPT2CausalLM` is a `keras_nlp.models.Task` instance.\n",
    "\n",
    "This step takes quite a bit of GPU memory and a long time if we were to train\n",
    "it all the way to a fully trained state. Here we just use part of the dataset\n",
    "for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sf0aMwludIME"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 22:20:56.237295: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710973286.137327     197 service.cc:145] XLA service 0x7f692c01f780 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1710973286.137401     197 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "W0000 00:00:1710973287.700278     197 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2024-03-20 22:23:55.984666: E external/local_xla/xla/service/slow_operation_alarm.cc:65] \n",
      "********************************\n",
      "[Compiling module a_inference_one_step_on_data_54406__.46897] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2024-03-20 22:26:56.104673: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 5m0.120079446s\n",
      "\n",
      "********************************\n",
      "[Compiling module a_inference_one_step_on_data_54406__.46897] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "I0000 00:00:1710973616.106409     197 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "# Linearly decaying learning rate.\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mumi73GxdLbj"
   },
   "source": [
    "After fine-tuning is finished, you can again generate text using the same\n",
    "*generate()* function. This time, the text will be closer to news writing\n",
    "style, and the generated length will be close to our preset length in the\n",
    "training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hGjimV5RdMyR"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_802/3235595532.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I like basketball\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGPT-2 output:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmwJZYoNdQTj"
   },
   "source": [
    "## Into the Sampling Method\n",
    "\n",
    "In KerasNLP, there are a few sampling methods, e.g., contrastive search,\n",
    "Top-K and beam sampling. By default, our `GPT2CausalLM` uses Top-k search, but you can choose your own sampling method.\n",
    "\n",
    "Much like optimizer and activations, there are two ways to specify your custom sampler:\n",
    "\n",
    "- Use a string identifier, such as \"greedy\", you are using the default\n",
    "configuration via this way.\n",
    "- Pass a `keras_nlp.samplers.Sampler` instance, you can use custom configuration via this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz33PqKidR6B"
   },
   "outputs": [],
   "source": [
    "# Use a string identifier.\n",
    "gpt2_lm.compile(sampler=\"top_k\")\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n",
    "greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
    "gpt2_lm.compile(sampler=greedy_sampler)\n",
    "\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzRQyPkZIq8_"
   },
   "source": [
    "# Task 2 - Varying Samplers\n",
    "\n",
    "Try several other samplers:\n",
    "\n",
    "- `BeamSampler`, keeps track of the `num_beams` most probable sequences at each timestep, and predicts the best next token from all sequences.\n",
    "- `ContrastiveSampler`, reweights options based on their similarity to previously generated outputs (to decrease repetition).\n",
    "- `RandomSampler`, at each time step, samples the next token using the softmax probabilities provided by the model.\n",
    "- `TopKSampler`, select the top`k` most probable tokens and then sample from them.\n",
    "- `TopPSampler`, like Top-K, except it selects enough samples to cover `P` percent of the distribution (which could be more or less than the `k` value from the previous approach).\n",
    "\n",
    "For more information about these, see: https://keras.io/api/keras_nlp/samplers/\n",
    "\n",
    "To complete the task, show that you have implemented all five and used them to generate output.\n",
    "\n",
    "You should experiment with the parameters for each sampler and see what patterns you observe in terms of the outputs (this is not marked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PXwxuVbSKgvz"
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkzKA9ZXdTvF"
   },
   "source": [
    "\n",
    "# [optional] Finetune on Chinese Poem Dataset\n",
    "\n",
    "We can also finetune GPT2 on non-English datasets. For readers knowing Chinese,\n",
    "this part illustrates how to fine-tune GPT2 on Chinese poem dataset to teach our\n",
    "model to become a poet!\n",
    "\n",
    "Because GPT2 uses byte-pair encoder, and the original pretraining dataset\n",
    "contains some Chinese characters, we can use the original vocab to finetune on\n",
    "Chinese dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u4Gp57_dWVx"
   },
   "outputs": [],
   "source": [
    "!# Load chinese poetry dataset.\n",
    "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eZ29NMgdZXB"
   },
   "source": [
    "Load text from the json file. We only use (全唐诗 - \"All Tang poetry\") for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D24IdbYxdZ7S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "poem_collection = []\n",
    "for file in os.listdir(\"chinese-poetry/全唐诗\"):\n",
    "    if \".json\" not in file or \"poet\" not in file:\n",
    "        continue\n",
    "    full_filename = \"%s/%s\" % (\"chinese-poetry/全唐诗\", file)\n",
    "    with open(full_filename, \"r\") as f:\n",
    "        content = json.load(f)\n",
    "        poem_collection.extend(content)\n",
    "\n",
    "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikCTFUULdd2o"
   },
   "source": [
    "Let's take a look at sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Bo5CpEbdeUl"
   },
   "outputs": [],
   "source": [
    "print(paragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUmeA-pidfmM"
   },
   "source": [
    "This is translated by Google Translate as \"Stay in Qingjidian, Chaoyang Lidi City. In the good years, people enjoy their work and sing songs on the ridges.\"\n",
    "\n",
    "Similar to the news example, we convert to TF dataset, and only use partial data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCcL5DnIdg1w"
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
    "    .batch(16)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Running through the whole dataset takes long, only take `500` and run 1\n",
    "# epochs for demo purposes.\n",
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-4,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFOHjs_Ldi4L"
   },
   "source": [
    "Let's check the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-qjA-aMdkCp"
   },
   "outputs": [],
   "source": [
    "output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200) # Input is \"Rainy and windy last night\"\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "text_classification_from_scratch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
