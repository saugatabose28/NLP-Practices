{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqpYf84yarQI"
   },
   "source": [
    "# COMP 4446 / 5046 Lab 5, 2024\n",
    "\n",
    "Based on:\n",
    "- [\"Text classification from scratch\"](https://keras.io/examples/nlp/text_classification_from_scratch/) by Mark Omernick, Francois Chollet\n",
    "- [\"Bidirectional LSTM on IMDB\"](https://keras.io/examples/nlp/bidirectional_lstm_imdb/) by Francois Chollet\n",
    "- [\"GPT2 Text Generation with KerasNLP\"](https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/) by Chen Qian\n",
    "- [\"GPT text generation from scratch with KerasNLP\"](https://keras.io/examples/generative/text_generation_gpt/) by Jesse Chan\n",
    "\n",
    "Make sure you are using a GPU for this lab (otherwise some of the later steps will take a long time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X24BBYtWarQK"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, we will revisit classifiers and RNNs, but using the Keras library, which is built on top of Tensorflow (rather than PyTorch).\n",
    "\n",
    "We'll see two examples of getting data and making a model to do movie review prediction on IMDB data.\n",
    "\n",
    "First, we'll work from raw text - a set of text files on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyHNHczmarQL"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isJPP7FRcYDO"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/keras-team/keras-nlp.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQbiuBuXarQM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kohvZqLxarQM"
   },
   "source": [
    "## Load the data: IMDB movie review sentiment classification\n",
    "\n",
    "Let's download the data and inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VYQpT8iarQN"
   },
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_ZFmGAParQN"
   },
   "source": [
    "The `aclImdb` folder contains a `train` and `test` subfolder, plus a few text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOHhquKIarQO"
   },
   "outputs": [],
   "source": [
    "!ls aclImdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkL_w2VQ58kD"
   },
   "source": [
    "Inside each folder are subfolders for positive and negative data, plus files with precomputed features, and source URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orGalmZOarQO"
   },
   "outputs": [],
   "source": [
    "!ls aclImdb/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MscZ-nJyarQP"
   },
   "outputs": [],
   "source": [
    "!ls aclImdb/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbZ2QxEqarQP"
   },
   "source": [
    "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each of\n",
    " which represents one review (either positive or negative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjuKMrirarQP"
   },
   "outputs": [],
   "source": [
    "!cat aclImdb/train/pos/6248_7.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iBKQJsFarQP"
   },
   "source": [
    "We are only interested in the `pos` and `neg` subfolders, so let's delete the other subfolder that has text files in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtAgqyUiarQP"
   },
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6VH8ZO3arQP"
   },
   "source": [
    "You can use the utility `keras.utils.text_dataset_from_directory` to\n",
    "generate a labeled `tf.data.Dataset` object from a set of text files on disk filed into class-specific folders.\n",
    "\n",
    "Let's use it to generate the training, validation, and test datasets. The validation and training datasets are generated from two subsets of the `train` directory, with 20% of samples going to the validation dataset and 80% going to the training dataset. When using the `validation_split` & `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation & training splits you get have no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSciyMhQarQP"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "raw_train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    ")\n",
    "raw_test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9FBZUeSarQQ"
   },
   "source": [
    "Let's look at a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YNS57gZarQQ"
   },
   "outputs": [],
   "source": [
    "# It's important to take a look at your raw data to ensure your normalization\n",
    "# and tokenization will work as expected. We can do that by taking a few\n",
    "# examples from the training set and looking at them.\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(text_batch.numpy()[i])\n",
    "        print(label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QARikvbParQQ"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We will use the `TextVectorization` layer for word splitting & indexing. In the process, we will also remove `<br />` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Xf2fLmQarQQ"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "# Having looked at our data above, we see that the raw text contains HTML break\n",
    "# tags of the form '<br />'. These tags will not be removed by the default\n",
    "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# create a custom standardization function.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "max_features = 20000\n",
    "embedding_dim = 128\n",
    "sequence_length = 500\n",
    "\n",
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences.\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Now that the vectorize_layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
    "\n",
    "# Let's make a text-only dataset (no labels):\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# Let's call `adapt`:\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d5lQyCAarQQ"
   },
   "source": [
    "## Two options to vectorize the data\n",
    "\n",
    "There are 2 ways we can use our text vectorization layer:\n",
    "\n",
    "**Option 1: Make it part of the model**, so as to obtain a model that processes raw strings, like this:\n",
    "\n",
    "```python\n",
    "text_input = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "x = vectorize_layer(text_input)\n",
    "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
    "...\n",
    "```\n",
    "\n",
    "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then feed it into a model that expects integer sequences as inputs.\n",
    "\n",
    "An important difference between the two is that option 2 enables you to do\n",
    "**asynchronous CPU processing and buffering** of your data when training on a GPU. So if you're training a model on a GPU, you probably want to go with this option to get the best performance. This is what we will do below.\n",
    "\n",
    "If we were to export our model to production, we'd ship a model that accepts rawstrings as input, like in the code snippet for option 1 above. This can be done after training. We do this in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSYb7P0barQR"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVxJQaiyarQR"
   },
   "source": [
    "## Build a model\n",
    "\n",
    "The first model we introduce uses a Convolutional Neural Network. This has not been covered in lectures (you can read more about it on [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network#Building_blocks)). The general idea is the a convolution is a type of weighted average that is applied in multiple locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tD_6m5a6arQR"
   },
   "outputs": [],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# We add a hidden layer (linear + ReLU):\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "# Dropout makes some values 0 during training, which tends to help improve how well the model generalised\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzXh1wgWarQR"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We'll run just 3 epochs / iterations so you can see how it works. Later, if you have more time you can run it for longer to see how well it can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "636ufpx9arQR"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Fit the model using the train and test datasets.\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdPBTqouarQR"
   },
   "source": [
    "## Evaluate the model on the test set\n",
    "\n",
    "We also evaluate on the test data. The accuracy is lower than on the training data because the test data is unseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMt-xBZWarQR"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDnulw0VarQR"
   },
   "source": [
    "## Make an end-to-end model\n",
    "\n",
    "If you want to obtain a model capable of processing raw strings, you can create a new model using the weights we just trained, but a different input type and an extra vectorisation step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBPZTZYXarQR"
   },
   "outputs": [],
   "source": [
    "# A string input\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "# Turn strings into vocab indices\n",
    "indices = vectorize_layer(inputs)\n",
    "# Turn vocab indices into predictions\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "end_to_end_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "end_to_end_model.evaluate(raw_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PaAnP6QbzJJ"
   },
   "source": [
    "# LSTM\n",
    "\n",
    "Now let's see how to construct an LSTM in Keras.\n",
    "\n",
    "First, we'll set up some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GULwBe7mbyoX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrDie0iCb4yU"
   },
   "source": [
    "Keras has code for the LSTM already, so we don't need to implement all the details ourselves.\n",
    "\n",
    "The code below sets up the model, using integers as input (ie., like at the start above, we assume someone else is converting our input words into word IDs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3MElaM3b47K"
   },
   "outputs": [],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add a bidirectional LSTM\n",
    "# return_sequences determines whether we get just the last vector or all:\n",
    "#    True - give us an output for each word\n",
    "#    False - give us just one output\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCMLC_L6b9nj"
   },
   "source": [
    "Load the IMDB movie review sentiment data. This time we are going to use the library provided version of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAUu0BvSb9um"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "# Use pad_sequence to standardize sequence length:\n",
    "# this will truncate sequences longer than 200 words and zero-pad sequences shorter than 200 words.\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1zpmPANcCrd"
   },
   "source": [
    "Train and evaluate the model. You will see similar results to the approach above. To do better, we would need to train for longer and explore other variations on the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGD8LbckcCxF"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdjC-_gW46gc"
   },
   "source": [
    "# Task 1 - Multilayer LSTM\n",
    "\n",
    "Modify the LSTM architecture to have two layers. You can do this by having an extra layer that produces output at each position, which is fed into the next layer as an input.\n",
    "\n",
    "You should print the model summary to show for credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_sW8bik146wZ"
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEOTAg-kcHIR"
   },
   "source": [
    "# Text generation\n",
    "\n",
    "Now, we are going to turn to text generation to see some variations on inference.\n",
    "\n",
    "You will learn to load a pre-trained  Language Model (LM) - [GPT-2](https://openai.com/research/better-language-models) (originally invented by OpenAI), finetune it to a specific text style, and generate text based on users' input (also known as prompt). You will also see how GPT2 adapts quickly to non-English languages, such as Chinese.\n",
    "\n",
    "This examples uses [Keras Core](https://keras.io/keras_core/) to work in any of `\"tensorflow\"`, `\"jax\"` or `\"torch\"`. Support for Keras Core is baked into KerasNLP, simply change the `\"KERAS_BACKEND\"` environment variable to select the backend of your choice. We select the JAX backend below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOgwGNqLcHW6"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd-WEdGzcuLA"
   },
   "source": [
    "Large Language Models are complex to build and expensive to train from scratch. Luckily there are pretrained LLMs available for use right away. Keras provides a large number of pre-trained checkpoints that allow you to experiment with very good models without needing to train them yourself.\n",
    "\n",
    "Keras also  has a Sampler class that implements generation algorithms such as Top-K, Beam and contrastive search. These samplers can be used to generate text with custom models.\n",
    "\n",
    "## Load a pre-trained GPT-2 model and generate some text\n",
    "\n",
    "KerasNLP provides a number of pre-trained models, such as [Google\n",
    "Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
    "and [GPT-2](https://openai.com/research/better-language-models). You can see\n",
    "the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n",
    "\n",
    "It's very easy to load the GPT-2 model as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-ENeVeHcz8G"
   },
   "outputs": [],
   "source": [
    "# To speed up training and generation, we use preprocessor of length 128\n",
    "# instead of full length 1024.\n",
    "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
    "    \"gpt2_base_en\",\n",
    "    sequence_length=128,\n",
    ")\n",
    "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\", preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5kQdj4Vc1sF"
   },
   "source": [
    "Once the model is loaded, you can use it to generate some text right away. Run\n",
    "the cells below to give it a try. It's as simple as calling a single function\n",
    "*generate()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x-8z1bkc1yz"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVEJgX-uc4XO"
   },
   "source": [
    "Try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CNgp9zWc4fD"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY5eaLhoc8aQ"
   },
   "source": [
    "Notice how much faster the second call is. This is because the computational\n",
    "graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and\n",
    "re-used in the 2nd behind the scenes.\n",
    "\n",
    "The quality of the generated text looks OK, but we can improve it via\n",
    "fine-tuning.\n",
    "\n",
    "## More on the GPT-2 model from KerasNLP\n",
    "\n",
    "Next up, we will actually fine-tune the model to update its parameters, but\n",
    "before we do, let's take a look at the full set of tools we have to for working\n",
    "with for GPT2.\n",
    "\n",
    "The code of GPT2 can be found\n",
    "[here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n",
    "Conceptually the `GPT2CausalLM` can be hierarchically broken down into several\n",
    "modules in KerasNLP, all of which have a *from_preset()* function that loads a\n",
    "pretrained model:\n",
    "\n",
    "- `keras_nlp.models.GPT2Tokenizer`: The tokenizer used by GPT2 model, which is a\n",
    "    [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n",
    "- `keras_nlp.models.GPT2CausalLMPreprocessor`: the preprocessor used by GPT2\n",
    "    causal LM training. It does the tokenization along with other preprocessing\n",
    "    works such as creating the label and appending the end token.\n",
    "- `keras_nlp.models.GPT2Backbone`: the GPT2 model, which is a stack of\n",
    "    `keras_nlp.layers.TransformerDecoder`. This is usually just referred as\n",
    "    `GPT2`.\n",
    "- `keras_nlp.models.GPT2CausalLM`: wraps `GPT2Backbone`, it multiplies the\n",
    "    output of `GPT2Backbone` by embedding matrix to generate logits over\n",
    "    vocab tokens.\n",
    "\n",
    "## Finetune on News data\n",
    "\n",
    "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one step further to finetune the model so that it generates text in a specific style, short or long, strict or casual. In this tutorial, we will use a news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WgcqFIcdBlB"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "news_ds = tfds.load(\"ag_news_subset\", split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kasniHyddBs_"
   },
   "source": [
    "Let's take a look inside sample data from the ag_news TensorFlow Dataset. Thereare two features:\n",
    "\n",
    "- **__description__**: text of the article.\n",
    "- **__label__**: the category (used in earlier labs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acAey1ypdDc5"
   },
   "outputs": [],
   "source": [
    "for description, label in news_ds:\n",
    "    print(description.numpy())\n",
    "    print(label.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW2QrA8DdDmB"
   },
   "source": [
    "In our case, we are performing next word prediction in a language model, so we\n",
    "only need the 'description' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gKCksWpdGeL"
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    news_ds.map(lambda description, _: description)\n",
    "    .batch(32)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64jFwdc5dIGb"
   },
   "source": [
    "Now you can finetune the model using the familiar *fit()* function. Note that\n",
    "`preprocessor` will be automatically called inside `fit` method since\n",
    "`GPT2CausalLM` is a `keras_nlp.models.Task` instance.\n",
    "\n",
    "This step takes quite a bit of GPU memory and a long time if we were to train\n",
    "it all the way to a fully trained state. Here we just use part of the dataset\n",
    "for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sf0aMwludIME"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "# Linearly decaying learning rate.\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-5,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mumi73GxdLbj"
   },
   "source": [
    "After fine-tuning is finished, you can again generate text using the same\n",
    "*generate()* function. This time, the text will be closer to news writing\n",
    "style, and the generated length will be close to our preset length in the\n",
    "training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGjimV5RdMyR"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmwJZYoNdQTj"
   },
   "source": [
    "## Into the Sampling Method\n",
    "\n",
    "In KerasNLP, there are a few sampling methods, e.g., contrastive search,\n",
    "Top-K and beam sampling. By default, our `GPT2CausalLM` uses Top-k search, but you can choose your own sampling method.\n",
    "\n",
    "Much like optimizer and activations, there are two ways to specify your custom sampler:\n",
    "\n",
    "- Use a string identifier, such as \"greedy\", you are using the default\n",
    "configuration via this way.\n",
    "- Pass a `keras_nlp.samplers.Sampler` instance, you can use custom configuration via this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz33PqKidR6B"
   },
   "outputs": [],
   "source": [
    "# Use a string identifier.\n",
    "gpt2_lm.compile(sampler=\"top_k\")\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)\n",
    "\n",
    "# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n",
    "greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
    "gpt2_lm.compile(sampler=greedy_sampler)\n",
    "\n",
    "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
    "print(\"\\nGPT-2 output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzRQyPkZIq8_"
   },
   "source": [
    "# Task 2 - Varying Samplers\n",
    "\n",
    "Try several other samplers:\n",
    "\n",
    "- `BeamSampler`, keeps track of the `num_beams` most probable sequences at each timestep, and predicts the best next token from all sequences.\n",
    "- `ContrastiveSampler`, reweights options based on their similarity to previously generated outputs (to decrease repetition).\n",
    "- `RandomSampler`, at each time step, samples the next token using the softmax probabilities provided by the model.\n",
    "- `TopKSampler`, select the top`k` most probable tokens and then sample from them.\n",
    "- `TopPSampler`, like Top-K, except it selects enough samples to cover `P` percent of the distribution (which could be more or less than the `k` value from the previous approach).\n",
    "\n",
    "For more information about these, see: https://keras.io/api/keras_nlp/samplers/\n",
    "\n",
    "To complete the task, show that you have implemented all five and used them to generate output.\n",
    "\n",
    "You should experiment with the parameters for each sampler and see what patterns you observe in terms of the outputs (this is not marked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PXwxuVbSKgvz"
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkzKA9ZXdTvF"
   },
   "source": [
    "\n",
    "# [optional] Finetune on Chinese Poem Dataset\n",
    "\n",
    "We can also finetune GPT2 on non-English datasets. For readers knowing Chinese,\n",
    "this part illustrates how to fine-tune GPT2 on Chinese poem dataset to teach our\n",
    "model to become a poet!\n",
    "\n",
    "Because GPT2 uses byte-pair encoder, and the original pretraining dataset\n",
    "contains some Chinese characters, we can use the original vocab to finetune on\n",
    "Chinese dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u4Gp57_dWVx"
   },
   "outputs": [],
   "source": [
    "!# Load chinese poetry dataset.\n",
    "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eZ29NMgdZXB"
   },
   "source": [
    "Load text from the json file. We only use (全唐诗 - \"All Tang poetry\") for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D24IdbYxdZ7S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "poem_collection = []\n",
    "for file in os.listdir(\"chinese-poetry/全唐诗\"):\n",
    "    if \".json\" not in file or \"poet\" not in file:\n",
    "        continue\n",
    "    full_filename = \"%s/%s\" % (\"chinese-poetry/全唐诗\", file)\n",
    "    with open(full_filename, \"r\") as f:\n",
    "        content = json.load(f)\n",
    "        poem_collection.extend(content)\n",
    "\n",
    "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikCTFUULdd2o"
   },
   "source": [
    "Let's take a look at sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Bo5CpEbdeUl"
   },
   "outputs": [],
   "source": [
    "print(paragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUmeA-pidfmM"
   },
   "source": [
    "This is translated by Google Translate as \"Stay in Qingjidian, Chaoyang Lidi City. In the good years, people enjoy their work and sing songs on the ridges.\"\n",
    "\n",
    "Similar to the news example, we convert to TF dataset, and only use partial data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCcL5DnIdg1w"
   },
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
    "    .batch(16)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Running through the whole dataset takes long, only take `500` and run 1\n",
    "# epochs for demo purposes.\n",
    "train_ds = train_ds.take(500)\n",
    "num_epochs = 1\n",
    "\n",
    "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
    "    5e-4,\n",
    "    decay_steps=train_ds.cardinality() * num_epochs,\n",
    "    end_learning_rate=0.0,\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "gpt2_lm.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gpt2_lm.fit(train_ds, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFOHjs_Ldi4L"
   },
   "source": [
    "Let's check the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-qjA-aMdkCp"
   },
   "outputs": [],
   "source": [
    "output = gpt2_lm.generate(\"昨夜雨疏风骤\", max_length=200) # Input is \"Rainy and windy last night\"\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "text_classification_from_scratch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
