{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11\n",
    "\n",
    "This week we are going to look at another library: Vowpal Wabbit. This one is optimised for working with HUGE amounts of data. It is rarely used in academic research on NLP, but I've heard it is frequently used in industry for simple tasks that need fast processing.\n",
    "\n",
    "These materials are based on a set of [tutorials](https://github.com/hal3/vwnlp/tree/master) by Hal Daum√© III.\n",
    "\n",
    "## Set up\n",
    "\n",
    "First, we install the library.\n",
    "\n",
    "Note - we will use a Python interface to it for the convenience of running this lab. Most actual users use the command line tool `vw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vowpalwabbit in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (9.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install vowpalwabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get some data. We're going to use a Sentiment Analysis dataset from a [2004 paper](https://aclanthology.org/P04-1035/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'data': File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3053k  100 3053k    0     0  11.1M      0 --:--:-- --:--:-- --:--:-- 11.0M\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!curl -o data/review_polarity.tar.gz http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
    "!tar zxC data -f data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the beginning of one of the positive reviews and one of the negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
      "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
      "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \n"
     ]
    }
   ],
   "source": [
    "!head -n3 data/txt_sentoken/pos/cv000_29590.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n"
     ]
    }
   ],
   "source": [
    "!head -n3 data/txt_sentoken/neg/cv000_29416.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change this data to the `vw` format. Luckily this data is already tokenized, so we don't have to deal with text preprocessing.\n",
    "\n",
    "The vw format is quite flexible, and we'll see additional fun things you can do later, but for now, the basic file format is one-example per line, with the label first and then a vertical bar (pipe) and then all of the features. For example, for the two above files, we'd want to create two `vw` examples like:\n",
    "\n",
    "    +1 | films adapted from comic books have had plenty of success , whether they're ...\n",
    "    -1 | plot : two teen couples go to a church party , drink and then drive . they get into ...\n",
    "    \n",
    "However, there's an issue here. There are two **reserved characters** in the `vw` format: colon (`:`) and pipe (`|`). This means if those two characters appear in a review then we need to convert them to something else.\n",
    "\n",
    "The function below will make a suitable change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot COLON two teen couples go to a church party ,\n"
     ]
    }
   ],
   "source": [
    "def textToVW(lines):\n",
    "    return ' '.join([l.strip() for l in lines]).replace(':','COLON').replace('|','PIPE')\n",
    "\n",
    "def fileToVW(inputFile):\n",
    "    return textToVW(open(inputFile,'r').readlines())\n",
    "\n",
    "data = fileToVW('data/txt_sentoken/neg/cv000_29416.txt')[:50]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see the first few words of the negative review, with ':' replaced by COLON (note that all the other text in the reviews is lowercase, so these new words will not look like the regular words `colon` and `pipe`).\n",
    "\n",
    "Now we just need to read in all the positive examples and all the negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 total examples read\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def readTextFilesInDirectory(directory):\n",
    "    return [fileToVW(directory + os.sep + f)\n",
    "            for f in os.listdir(directory)\n",
    "            if  f.endswith('.txt')]\n",
    "\n",
    "examples = ['+1 | ' + s for s in readTextFilesInDirectory('data/txt_sentoken/pos')] + \\\n",
    "           ['-1 | ' + s for s in readTextFilesInDirectory('data/txt_sentoken/neg')]\n",
    "\n",
    "print('%d total examples read' % len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got all the files, we put \"`+1 | `\" at the beginning of the positive ones and \"`-1 | `\" at the beginning of the negative ones.\n",
    "\n",
    "We'll now generate some training data and some test data. To achieve this, we're going to permute the examples (after putting in a random seed for reproducability), and then taking the first 80% and train and the last 20% as test.\n",
    "\n",
    "The fact that we're permuting the data is **very important**. By default, `vw` uses an online learning strategy, and if we did something silly like putting all the positive examples before the negative examples, learning would take a LONG time. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---++-+-+++-+-+++--+++-+++++++-++--+-+----++++-++\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(examples)   # this does in-place shuffling\n",
    "# print out the labels of the first 50 examples to be sure they're sane:\n",
    "print(''.join([s[0] for s in examples[:50]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write the first 1600 to a training file and the last 400 to a test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1600 data/sentiment.tr\n",
      "    400 data/sentiment.te\n",
      "   2000 total\n"
     ]
    }
   ],
   "source": [
    "def writeToVWFile(filename, examples):\n",
    "    with open(filename, 'w') as h:\n",
    "        for ex in examples:\n",
    "            print(ex, file=h)\n",
    "\n",
    "writeToVWFile('data/sentiment.tr', examples[:1600])\n",
    "writeToVWFile('data/sentiment.te', examples[1600:])\n",
    "\n",
    "!wc -l data/sentiment.tr data/sentiment.te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, everything is properly set up and we can run `vw`!\n",
    "\n",
    "# <a id='run'></a>  Running VW for the First Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using no cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000        -1.0000     1043\n",
      "0.468750 0.312500           32           32.0        -1.0000         1.0000     1587\n",
      "0.546875 0.625000           64           64.0         1.0000        -1.0000      472\n",
      "0.476562 0.406250          128          128.0        -1.0000        -1.0000      480\n",
      "0.433594 0.390625          256          256.0         1.0000         1.0000     1645\n",
      "0.371094 0.308594          512          512.0        -1.0000        -1.0000      988\n",
      "0.300781 0.230469         1024         1024.0         1.0000         1.0000      689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 1\", enable_logging=True)\n",
    "\n",
    "for line in model.get_log():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output consists of two parts:\n",
    "\n",
    "1. The header, which displays some information about the parameters `vw` is using to do the learning (number of bits, learning rate, ..., number of sources). We'll discuss (some) of these later.\n",
    "2. The progress list (the lines with lots of numbers); much more on this below.\n",
    "\n",
    "One important note is that when we set up the workspace, we used the argument `binary`, which instructs `vw` to report all losses as zero-one loss.\n",
    "\n",
    "Let's look first at the first four lines of the progress list:\n",
    "\n",
    "    average  since         example        example  current  current  current\n",
    "    loss     last          counter         weight    label  predict features\n",
    "    1.000000 1.000000            1            1.0         1.0000        -1.0000      782\n",
    "    1.000000 1.000000            2            2.0        -1.0000         1.0000      627\n",
    "    0.500000 0.000000            4            4.0        -1.0000        -1.0000      440\n",
    "    0.625000 0.750000            8            8.0         1.0000        -1.0000     1073\n",
    "    \n",
    "The columns are labeled, which gives some clue as to what's being printed out. The way `vw` works internally is that it processes one example at a time. At every $2^k$th example (examples 1, 2, 4, 8, 16, ...), it prints out a status update. This way you get lots of updates early (as a sanity check) and fewer as time goes on. The third column gives you the example number. The fourth column tells you the total \"weight\" of examples so far; right now all examples have a weight of 1.0, but for some problems (e.g., imbalanced data), you might want to give different weight to different examples. The fifth column tells you the true current label (+1 or -1) and the sixth column tells you the models' current prediction. Lastly, it tells you how many features there are in this example.\n",
    "\n",
    "The first two columns deserve some explanation. In \"default\" mode, `vw` reports \"progressive validation loss.\" This means that when `vw` sees a training example, it *first* makes a prediction. It then computes a loss on that single prediction. Only after that does it \"learn\". The average loss computed in this case is the **progressive validation loss.** It has a nice property that it's a good estimate of test loss, *provided you only make one pass over the data*, **and** it's efficient to compute. The first column tells you the average progressive loss over the *entire* run of `vw`; the second column tells you the average progressive loss *since the last time `vw` printed something*.\n",
    "\n",
    "In practice, this second column is what you want to look at for telling how well your model is doing.\n",
    "\n",
    "# Your Second Run of VW\n",
    "\n",
    "There are a couple of things we need to do to get a useful system. The first is that for most data sets, a single online pass over the data is insufficient -- we need to run more than one. The second is that we actually need to store the model somewhere so that we can make predictions on test data! We'll go through these in order.\n",
    "\n",
    "## <a id='passes'></a>  Running More than One Pass\n",
    "\n",
    "On the surface, running more than one pass seems like an easy thing to ask `vw` to do. It's a bit more complicated than it might appear.\n",
    "\n",
    "The first issue is that one of the main speed bottlenecks for `vw` is file IO. Reading, and parsing, your input data is incredibly time consuming. In order to get around this, when multiple passes over the data are requested, `vw` will create and use a **cache file**, which is basically a second copy of your data stored in a `vw`-friendly, efficient, binary format. So if you want to run more than one pass, you have to tell `vw` to create a cache file.\n",
    "\n",
    "Here's an example running 5 passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000      788\n",
      "0.562500 0.500000           32           32.0        -1.0000         1.0000      314\n",
      "0.500000 0.437500           64           64.0        -1.0000         1.0000      770\n",
      "0.484375 0.468750          128          128.0        -1.0000         1.0000     1353\n",
      "0.449219 0.414062          256          256.0         1.0000         1.0000      909\n",
      "0.355469 0.261719          512          512.0         1.0000         1.0000      914\n",
      "0.285156 0.214844         1024         1024.0         1.0000         1.0000      701\n",
      "0.264317 0.264317         2048         2048.0        -1.0000        -1.0000      735 h\n",
      "0.226374 0.188596         4096         4096.0        -1.0000        -1.0000      691 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 5 -c -k\", enable_logging=True)\n",
    "\n",
    "for line in model.get_log():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this command, we added three new command-line options:\n",
    "\n",
    "* `--passes 5`: this is the most obvious one: it tells `vw` to run five passes over the data.\n",
    "* `-c`: this tells `vw` to automatically create and use a cache file\n",
    "* `-k`: by default, if `vw` uses a cache file, it *first* checks to see if the file exists. If the cache file already exists, it completely ignores the data file (`sentiment.tr`) and *just* uses the cache file. This is great if your data never changes because it makes the first pass slightly faster, but it can trip you up if you make changes to your data and forget. `-k` tells `vw` to \"kill\" the old cache file: even if it exists, it should be recreated from scratch.\n",
    "\n",
    "(Warning: if you're running multiple jobs on the same file in parallel, you will get clashes on the cache file. You should either create a single cache file ahead of time and use it for all jobs [remove `-k` in that case], *or* you should explicitly give your own file names to the cache by saying `--cache myfilename0.cache` instead of `-c`.)\n",
    "\n",
    "If you're particularly attentive, you might have noticed that there are a few \"`h`\"s in the progress list (and in the printing of the average loss at the end).\n",
    "\n",
    "This is **holdout** loss. Remember all that discussion of progressive validation loss? Well, it's useless when you're making more than one pass. That's because on the second pass, you'll already have trained on all the training data, so your model is going to be exceptionally good at making predictions.\n",
    "\n",
    "`vw`'s default solution to this is to holdout a fraction of the training data as validation data. By default, it will hold out **every 10th example** as test. The holdout loss (signaled by the `h`) is then the average loss, *limited to these 10% of the training examples*. (Note that on the first pass, it still prints progressive validation loss because this is a safe thing to do.)\n",
    "\n",
    "## <a id='save'></a>  Saving the Model and Making Test Predictions\n",
    "\n",
    "Now that we know how to do several passes and get heldout losses, we might want to actually save the learned model to a file so we can make predictions on test data! This is easy: we just tell `vw` where to save the final model using `-f file` (`-f` means \"final\"). Let's do this, and crank up the number of passes to 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000      788\n",
      "0.562500 0.500000           32           32.0        -1.0000         1.0000      314\n",
      "0.500000 0.437500           64           64.0        -1.0000         1.0000      770\n",
      "0.484375 0.468750          128          128.0        -1.0000         1.0000     1353\n",
      "0.449219 0.414062          256          256.0         1.0000         1.0000      909\n",
      "0.355469 0.261719          512          512.0         1.0000         1.0000      914\n",
      "0.285156 0.214844         1024         1024.0         1.0000         1.0000      701\n",
      "0.264317 0.264317         2048         2048.0        -1.0000        -1.0000      735 h\n",
      "0.226374 0.188596         4096         4096.0        -1.0000        -1.0000      691 h\n",
      "0.216484 0.206593         8192         8192.0        -1.0000        -1.0000      583 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model\", enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we have a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 studio-lab-user users 563119 May 12 15:22 data/sentiment.model\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/sentiment.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you might have noticed is that even though we asked `vw` for 20 passes, it actually only did 9! This happens because by default `vw` does early stopping: if the holdout loss ceases to improve for three passes over the data, it stops optimizing and stores the *best* model found so far. We will later see how to adjust these defaults.\n",
    "\n",
    "## <a id='test'></a> Making Predictions\n",
    "\n",
    "Now we want to make predictions. In order to do this, we have to (a) tell `vw` to load a model, (b) tell it only to make predictions (and not to learn), and (c) tell it where to store the predictions. (Ok, technically we don't need to store the predictions anywhere if all we want to know is our error rate, but I'll assume we actually care about the output of our system.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = data/sentiment.te.pred\n",
      "using no cache\n",
      "Reading datafile = data/sentiment.te\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 4800\n",
      "power_t = 0.5\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "\n",
      "191 (1, 1)\n",
      "155 (-1, -1)\n",
      "16 (-1, 1)\n",
      "38 (1, -1)\n",
      "Accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "model = vowpalwabbit.Workspace(\"--binary -t -i data/sentiment.model --predictions data/sentiment.te.pred data/sentiment.te\", enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))\n",
    "\n",
    "counts = {}\n",
    "out = open(\"data/sentiment.te.pred\", 'w')\n",
    "for line in open(\"data/sentiment.te\"):\n",
    "    prediction = model.predict(line.strip())\n",
    "    print(prediction, file=out)\n",
    "    pair = (int(prediction), int(line.split(\"|\")[0]))\n",
    "    counts[pair] = 1 + counts.get(pair, 0)\n",
    "\n",
    "matching = 0\n",
    "total = 0\n",
    "for pair, count in counts.items():\n",
    "    print(count, pair)\n",
    "    total += count\n",
    "    if pair[0] == pair[1]:\n",
    "        matching += count\n",
    "\n",
    "print(\"Accuracy:\", matching / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through these options in turn:\n",
    "\n",
    "* `--binary`: as before, tell `vw` that this is a binary classification problem and to report loss as a zero-one value\n",
    "* `-t`: put `vw` in test mode. You might assume that because we're loading a model to start with, `vw` would be in test mode by default. You would be wrong. Sometimes it's useful to start from a pre-trained model and continue training later.\n",
    "* `-i data/sentiment.model`: tell `vw` to load an **i**nitial model from the specified file\n",
    "\n",
    "\n",
    "We can now take a look at the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "!head data/sentiment.te.pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yay, we've successfully made predictions!\n",
    "\n",
    "You're now in a position where you can successfully: download data, process it into `vw` format, train a predictor on it, and use that predictor to make test predictions.\n",
    "\n",
    "## Task 1\n",
    "\n",
    "Try running this model on the training data.\n",
    "\n",
    "This should give an accuracy of 98%+. Why would we want to do this? Sometimes it is informative to check training performance. If a model as simple as the one used here is getting 100% then it is probably overfitting. If it is getting a very low score then something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving performance\n",
    "\n",
    "`vw` has *lots* of command-line arguments. For some of them you have to learn a little bit about how `vw` works internally.\n",
    "\n",
    "### Increasing Representational Capacity (and memory usage)\n",
    "\n",
    "Let's start with our previous training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000      788\n",
      "0.562500 0.500000           32           32.0        -1.0000         1.0000      314\n",
      "0.500000 0.437500           64           64.0        -1.0000         1.0000      770\n",
      "0.484375 0.468750          128          128.0        -1.0000         1.0000     1353\n",
      "0.449219 0.414062          256          256.0         1.0000         1.0000      909\n",
      "0.355469 0.261719          512          512.0         1.0000         1.0000      914\n",
      "0.285156 0.214844         1024         1024.0         1.0000         1.0000      701\n",
      "0.264317 0.264317         2048         2048.0        -1.0000        -1.0000      735 h\n",
      "0.226374 0.188596         4096         4096.0        -1.0000        -1.0000      691 h\n",
      "0.216484 0.206593         8192         8192.0        -1.0000        -1.0000      583 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model\", enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, in order to be fast, `vw` never stores any strings. When it reads your input (in which your features were represented as strings!), it *immediately* hashes the strings to some feature index. By default, it uses $2^{18}$ possible feature indices; this magic number $18$ is the \"number of bits\" used to store the weights in the model. (This is something of a misnomer: it's really the number of parameters in the learning algorithm, which is roughly the number of floats.)\n",
    "\n",
    "What does this hashing accomplish? Well, it accomplishes speed because no string manipulation ever happens. However, it comes with two downsides. The first is that you can get hash collisions. In particular, you might have to different features (i.e., different strings) that hash to the same location. From the learning algorithm's perspective, this means these two features are indistinguishable.\n",
    "\n",
    "Remember that hash collisions are incredibly common. In a bag of words, we often have several hundred thousand unique features. The probability of collision when you have $k$ items into $N$ buckets is approximately $1-\\exp\\left[\\frac {k(k-1)} {2N}\\right]$. In this case, with $N=2^{18}$, even with only $2000$ unique features, the probability of collision is already 99.95%. With $100k$ features it's basically guaranteed.\n",
    "\n",
    "The solution is to increase the number of bits used in the representation. This will (a) reduce the number of collisions, (b) make `vw` take more RAM, (c) make `vw` somewhat slower, and (d) make the resulting models larger on disk. Currently, the maximum number of bits that `vw` will let you use is 31. I don't suggest using this; it means `vw` will consume about 8GB of memory while running and the resulting file may take as much as 2GB of disk space. [Runtime memory will be 4 times larger than disk space because the optimization algorithms need extra working memory.] But note that with 100k unique features, even with 31 bits, the probability of collision is 99.1%. It will happen.\n",
    "\n",
    "Let's try running with 24 bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000      788\n",
      "0.562500 0.500000           32           32.0        -1.0000         1.0000      314\n",
      "0.500000 0.437500           64           64.0        -1.0000         1.0000      770\n",
      "0.484375 0.468750          128          128.0        -1.0000         1.0000     1353\n",
      "0.441406 0.398438          256          256.0         1.0000         1.0000      909\n",
      "0.347656 0.253906          512          512.0         1.0000         1.0000      914\n",
      "0.279297 0.210938         1024         1024.0         1.0000         1.0000      701\n",
      "0.259912 0.259912         2048         2048.0        -1.0000        -1.0000      735 h\n",
      "0.226374 0.192982         4096         4096.0        -1.0000        -1.0000      691 h\n",
      "0.218681 0.210989         8192         8192.0        -1.0000        -1.0000      583 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, increasing the number of bits did not help test accuracy (in fact, it may have hurt!), but we will see that, when we add additional features, it becomes more important.\n",
    "\n",
    "###  Fun NLP-esque Features for Free\n",
    "\n",
    "One nice thing about `vw` is that it internally supports \"extra feature\" generation. The main useful features are: word prefixes and suffixes, spelling features, and ngram features.\n",
    "\n",
    "#### Word Affixes\n",
    "\n",
    "For NLP tasks that mostly depend on the *meaning* (aka \"semantics\") of words, we often don't care about the funny little things that come at the ends of words. For instance, for sentiment classification, the words `awesome` and `awesomeness` are likely to roughly mean the same thing. For other tasks, like part of speech tagging, it's the suffixes that might matter most: words that end in `-ness` are much more likely to be adjectives than anything else.\n",
    "\n",
    "`vw` can automatically generate word prefixes and suffixes for you, using the `--affix` feature. For instance, if you add \"`--affix +5,-3`\" to the command line, this says to automatically compute (and add as new features) 5-character prefixes (that's the `+`) and three character suffixes (that's the `-`).\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     1413\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1145\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1115\n",
      "0.625000 0.750000            8            8.0         1.0000        -1.0000      711\n",
      "0.687500 0.750000           16           16.0         1.0000         1.0000     1575\n",
      "0.593750 0.500000           32           32.0        -1.0000         1.0000      627\n",
      "0.515625 0.437500           64           64.0        -1.0000         1.0000     1539\n",
      "0.507812 0.500000          128          128.0        -1.0000         1.0000     2705\n",
      "0.445312 0.382812          256          256.0         1.0000         1.0000     1817\n",
      "0.355469 0.265625          512          512.0         1.0000         1.0000     1827\n",
      "0.282227 0.208984         1024         1024.0         1.0000         1.0000     1401\n",
      "0.295154 0.295154         2048         2048.0        -1.0000        -1.0000     1469 h\n",
      "0.239560 0.184211         4096         4096.0        -1.0000        -1.0000     1381 h\n",
      "0.224176 0.208791         8192         8192.0        -1.0000        -1.0000     1165 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --affix +6\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was (somewhat) helpful.\n",
    "\n",
    "#### Spelling Features\n",
    "\n",
    "Spelling features are *super* useful for tasks where things like capitalization, years, numbers, etc. matter a lot. In other words, tasks *not at all* like sentiment classification.\n",
    "\n",
    "In `vw`, the spelling features option tells it to generate new features based on the word forms seen. For example, a word \"Alice\" has the word form \"Aaaaa\" (meaning: a capital letter followed by four lowercase letters); \"VanBuren\" has the form \"AaaAaaaa\". The general rule is that digits 0-9 get mapped to \"0\", letters a-z to \"a\", letters A-Z to \"A\", period to \".\" and anything else to \"#\". Thus, \"xY9s,3.80vaq\" gets mapped to \"aA0a#0.00aaa\" and this new \"word form\" is used as a new feature.\n",
    "\n",
    "To turn on spelling features, you simply add `--spelling _` to the command line. We can do this with little expectation that it will help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     1413\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1145\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1115\n",
      "0.625000 0.750000            8            8.0         1.0000        -1.0000      711\n",
      "0.687500 0.750000           16           16.0         1.0000         1.0000     1575\n",
      "0.562500 0.437500           32           32.0        -1.0000         1.0000      627\n",
      "0.609375 0.656250           64           64.0        -1.0000         1.0000     1539\n",
      "0.531250 0.453125          128          128.0        -1.0000         1.0000     2705\n",
      "0.515625 0.500000          256          256.0         1.0000        -1.0000     1817\n",
      "0.443359 0.371094          512          512.0         1.0000         1.0000     1827\n",
      "0.408203 0.373047         1024         1024.0         1.0000         1.0000     1401\n",
      "0.330396 0.330396         2048         2048.0        -1.0000        -1.0000     1469 h\n",
      "0.281319 0.232456         4096         4096.0        -1.0000        -1.0000     1381 h\n",
      "0.257143 0.232967         8192         8192.0        -1.0000        -1.0000     1165 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \" --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --spelling _\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, indeed it did not help and it actually hurt slightly (see the bottom of the 'since last' column).\n",
    "\n",
    "You might wonder what the \"`_`\" in the arguments means; for now, don't worry about it. We'll come back to it below when we introduce namespaces.\n",
    "\n",
    "#### N-gram Features\n",
    "\n",
    "Our current representation for learning is bag of words. As we've seen at many points in the course, n-grams can be usefull to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Generating 2-grams for all namespaces.\n",
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     1412\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1144\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1114\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      710\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000     1574\n",
      "0.500000 0.375000           32           32.0        -1.0000        -1.0000      626\n",
      "0.531250 0.562500           64           64.0        -1.0000         1.0000     1538\n",
      "0.492188 0.453125          128          128.0        -1.0000         1.0000     2704\n",
      "0.429688 0.367188          256          256.0         1.0000         1.0000     1816\n",
      "0.333984 0.238281          512          512.0         1.0000         1.0000     1826\n",
      "0.266602 0.199219         1024         1024.0         1.0000         1.0000     1400\n",
      "0.251101 0.251101         2048         2048.0        -1.0000        -1.0000     1468 h\n",
      "0.204396 0.157895         4096         4096.0        -1.0000        -1.0000     1380 h\n",
      "0.192308 0.180220         8192         8192.0        -1.0000        -1.0000     1164 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 2\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was super useful!\n",
    "\n",
    "We can try trigram features too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Generating 3-grams for all namespaces.\n",
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     2116\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1714\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1669\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000     1063\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000     2359\n",
      "0.437500 0.250000           32           32.0        -1.0000         1.0000      937\n",
      "0.484375 0.531250           64           64.0        -1.0000         1.0000     2305\n",
      "0.476562 0.468750          128          128.0        -1.0000         1.0000     4054\n",
      "0.417969 0.359375          256          256.0         1.0000         1.0000     2722\n",
      "0.324219 0.230469          512          512.0         1.0000         1.0000     2737\n",
      "0.268555 0.212891         1024         1024.0         1.0000         1.0000     2098\n",
      "0.229075 0.229075         2048         2048.0        -1.0000        -1.0000     2200 h\n",
      "0.186813 0.144737         4096         4096.0        -1.0000        -1.0000     2068 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 3\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that didn't help any more.\n",
    "\n",
    "We can, however, now see that the number of bits matters. If we drop the number of bits back down to 18, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Generating 3-grams for all namespaces.\n",
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     2116\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1714\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1669\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000     1063\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000     2359\n",
      "0.437500 0.250000           32           32.0        -1.0000         1.0000      937\n",
      "0.468750 0.500000           64           64.0        -1.0000         1.0000     2305\n",
      "0.468750 0.468750          128          128.0        -1.0000         1.0000     4054\n",
      "0.421875 0.375000          256          256.0         1.0000         1.0000     2722\n",
      "0.337891 0.253906          512          512.0         1.0000         1.0000     2737\n",
      "0.289062 0.240234         1024         1024.0         1.0000         1.0000     2098\n",
      "0.237885 0.237885         2048         2048.0        -1.0000        -1.0000     2200 h\n",
      "0.200000 0.162281         4096         4096.0        -1.0000        -1.0000     2068 h\n",
      "0.181319 0.162637         8192         8192.0        -1.0000        -1.0000     1744 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 18 --ngram 3\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no better than we started with.\n",
    "\n",
    "More specifically: **if we hadn't increased the number of bits, we would have concluded that ngram features weren't useful!**\n",
    "\n",
    "Finally, `vw` can do \"skip ngrams\" too. This means that instead of only looking at bigrams of adjacent words, you can look at bigrams with some gap. For instance, if you say `--ngram 2 --skips 1`, this means \"compute all bigrams that have at most one gap in them.\" For our favorite sentence \"the monster ate a sandwich\", you would get the default bigram features (\"the_monster monster_ate ate_a a_sandwich\") and *also* the skips (\"the_ate monster_a ate_sandwich\").\n",
    "\n",
    "Note: as you increase to, say, four-grams, this automatically includes bigrams and trigrams. As you increase number of skips, you get all the lower order skips too.\n",
    "\n",
    "#### Changing the Loss Function\n",
    "\n",
    "By default, `vw` optimizes squared loss. This means that if the correct label for an example is +1, and the model predicts -1, the error is 4.0. However, if the model predicts +3, the error is still 4.0, even though it's making the right binary prediction. Squared loss has the nice property that it estimates means. But it's not necessarily the most natural loss for classification problems.\n",
    "\n",
    "Many people prefer logistic loss (which gives a nice probabilistic interpretation) or hinge loss (which, when combined with regularization, yields support vector machines). You can switch the loss function with a simple flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000      788\n",
      "0.562500 0.500000           32           32.0        -1.0000        -1.0000      314\n",
      "0.578125 0.593750           64           64.0        -1.0000         1.0000      770\n",
      "0.531250 0.484375          128          128.0        -1.0000         1.0000     1353\n",
      "0.449219 0.367188          256          256.0         1.0000         1.0000      909\n",
      "0.353516 0.257812          512          512.0         1.0000         1.0000      914\n",
      "0.281250 0.208984         1024         1024.0         1.0000         1.0000      701\n",
      "0.268722 0.268722         2048         2048.0        -1.0000        -1.0000      735 h\n",
      "0.221978 0.175439         4096         4096.0        -1.0000        -1.0000      691 h\n",
      "0.206593 0.191209         8192         8192.0        -1.0000        -1.0000      583 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alternatively switch to hinge loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000      788\n",
      "0.468750 0.312500           32           32.0        -1.0000        -1.0000      314\n",
      "0.484375 0.500000           64           64.0        -1.0000         1.0000      770\n",
      "0.476562 0.468750          128          128.0        -1.0000         1.0000     1353\n",
      "0.437500 0.398438          256          256.0         1.0000         1.0000      909\n",
      "0.359375 0.281250          512          512.0         1.0000         1.0000      914\n",
      "0.291992 0.224609         1024         1024.0         1.0000         1.0000      701\n",
      "0.295154 0.295154         2048         2048.0        -1.0000        -1.0000      735 h\n",
      "0.246154 0.197368         4096         4096.0        -1.0000        -1.0000      691 h\n",
      "0.232967 0.219780         8192         8192.0        -1.0000        -1.0000      583 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function hinge\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Holdout Settings\n",
    "\n",
    "Earlier, we mentioned that the default way `vw` works for doing multiple passes is: on the first pass, perform progressive validation; on subsequent passes, use every 10th example as a heldout \"validation\" example. And to stop optimizing when things don't improve for three passes.\n",
    "\n",
    "These are reasonable defaults, but somewhat at odds with the behavior we often want.\n",
    "\n",
    "First, we often *don't* want `vw` to do early stopping. If we tell it to do 20 passes, then it should do 20 passes. This is easy. We just say `--early_terminate 999`. This means that instead of needing 3 passes of no-improvement in order to terminate, it now needs 999. Since we never run that many passes, this is a good default to say \"don't stop early.\" However, it *will* still output only the best model found.\n",
    "\n",
    "More relevant, often in NLP we have training data, development data, and test data. And I want to get validation performance on the development data rather than every-10th-example. You can accomplish this with `--holdout_after N`. What this means is: instead of doing every-10th-example as validation, use the first (N-1) examples as training data, and anything after that as development data.\n",
    "\n",
    "Putting these together, we can do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = data/sentiment.model\n",
      "creating cache_file = data/sentiment.tr.cache\n",
      "Reading datafile = data/sentiment.tr\n",
      "num sources = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      707\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      573\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      558\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      356\n",
      "0.625000 0.750000           16           16.0         1.0000        -1.0000     1043\n",
      "0.468750 0.312500           32           32.0        -1.0000         1.0000     1587\n",
      "0.546875 0.625000           64           64.0         1.0000        -1.0000      472\n",
      "0.476562 0.406250          128          128.0        -1.0000        -1.0000      480\n",
      "0.433594 0.390625          256          256.0         1.0000         1.0000     1645\n",
      "0.371094 0.308594          512          512.0        -1.0000        -1.0000      988\n",
      "0.300781 0.230469         1024         1024.0         1.0000         1.0000      689\n",
      "0.241206 0.241206         2048         2048.0        -1.0000        -1.0000      855 h\n",
      "0.203518 0.165829         4096         4096.0         1.0000         1.0000      540 h\n",
      "0.174874 0.155779         8192         8192.0         1.0000         1.0000     1517 h\n",
      "0.165829 0.158291        16384        16384.0        -1.0000        -1.0000      653 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --early_terminate 999 --holdout_after 1401\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the important thing is that the first 1400 examples are used as training data, the the remaining examples (in this case, 200) are used as heldout data. The average loss reported is then *precisely* the average loss on this heldout data.\n",
    "\n",
    "#### Namespaces and quadratic features\n",
    "\n",
    "For this part of the tutorial to make sense, we have to make our task a little more interesting by using a *sentiment lexicon*: basically, a list of positive-ish and negative-ish words. There are [lots of sentiment lexicons](http://sentiment.christopherpotts.net/lexicons.html). We will use [the one from Bing Liu](http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). First, let's download it and decompress it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: sudo: not found\n",
      "Requirement already satisfied: unrar in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.4)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 24020  100 24020    0     0   254k      0 --:--:-- --:--:-- --:--:--  254k\n",
      "/usr/bin/sh: 1: unrar: not found\n",
      "ls: cannot access 'data/*/*words.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install unrar\n",
    "!rm -f data/*words.txt\n",
    "!curl -o data/opinion-lexicon-English.rar https://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
    "!unrar x data/opinion-lexicon-English.rar  data\n",
    "!ls -l data/*/*words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at some of the positive words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a+\n",
      "abound\n",
      "abounds\n",
      "abundance\n",
      "abundant\n",
      "accessable\n",
      "accessible\n",
      "acclaim\n",
      "acclaimed\n",
      "acclamation\n",
      "accolade\n",
      "accolades\n",
      "accommodative\n",
      "accomodative\n",
      "accomplish\n",
      "accomplished\n",
      "accomplishment\n",
      "accomplishments\n",
      "accurate\n",
      "accurately\n"
     ]
    }
   ],
   "source": [
    "!head -n50 data/positive-words.txt | grep -v '^;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, I dropped lines that begin with \"`;`\" because these are comments in the files.\n",
    "\n",
    "We now want to go back and generate some new data files for `vw` that include lexicon features. In particular, we will include *both* the bag of words representation *as well as* lexicon features. The lexicon features we will use are very simple: the log of the count of words in the document that are on the positive list, and the log of the count on the negative list. We use logs because getting more positive words has diminishing returns.\n",
    "\n",
    "To do this, we'll write a bit more python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadLexicon(filename):\n",
    "    with open(filename, encoding = \"ISO-8859-1\") as h:\n",
    "        return set([l.strip()\n",
    "                    for l in h.readlines()\n",
    "                    if  not l.startswith(';') and len(l) > 1])\n",
    "\n",
    "import math\n",
    "def countLexiconWords(text, lexicon):\n",
    "    return math.log(1.0 + len([w for w in text if w in lexicon]))\n",
    "\n",
    "positiveLexicon = loadLexicon('data/positive-words.txt')\n",
    "negativeLexicon = loadLexicon('data/negative-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a copy of the two lexicons and we want to generate `vw` examples.\n",
    "\n",
    "But we have two different types of features. We have the original bag of words features. And we have these lexicon features. We'd like to keep them separate.\n",
    "\n",
    "This is where feature namespaces come in. We're going to create examples with *two* namespaces, one for the bag of words (let's call it the `w` namespace) and one for the lexicon features (let's call that the `l` namespace). In `vw`, namespaces are separated by pipes, so an example might look like:\n",
    "\n",
    "    +1 |l pos:5 neg:2 |w some words might go here ...\n",
    "    \n",
    "In addition to having two namespaces, this example also shows how to use feature values. By default, all features in a `vw` example get a value of one. If you want to override this, you can say something like \"`pos:5`\", which means that there's a single feature (called \"`pos`\") that has a feature value of 5.\n",
    "\n",
    "Let's generate data like this. Some of the code is copied from the Getting Started tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 total examples read\n",
      "first example: +1 |l pos:3.17805 neg:3.61092 |w films adapted from comic books have h...\n",
      "last  example: -1 |l pos:3.21888 neg:3.68888 |w plot COLON a down-and-out girl moves ...\n"
     ]
    }
   ],
   "source": [
    "def textToVW(lines):\n",
    "    return ' '.join([l.strip() for l in lines]).replace(':','COLON').replace('|','PIPE')\n",
    "\n",
    "def fileToVW(inputFile, posLex, negLex):\n",
    "    text     = textToVW(open(inputFile,'r').readlines())\n",
    "    words    = text.split()\n",
    "    posCount = countLexiconWords(words, posLex)\n",
    "    negCount = countLexiconWords(words, negLex)\n",
    "    return '|l pos:%g neg:%g |w ' % (posCount,negCount) + text\n",
    "\n",
    "import os\n",
    "def readTextFilesInDirectory(directory):\n",
    "    return [fileToVW(directory + os.sep + f, positiveLexicon, negativeLexicon)\n",
    "            for f in os.listdir(directory)\n",
    "            if  f.endswith('.txt')]\n",
    "\n",
    "examples = ['+1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/pos')] + \\\n",
    "           ['-1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/neg')]\n",
    "\n",
    "print('{} total examples read'.format(len(examples)))\n",
    "print('first example: {}...'.format(examples[ 0][:70]))\n",
    "print('last  example: {}...'.format(examples[-1][:70]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least based on these two examples, this seems promising: the positive/negative lexicon features seem to correlate with the labels!\n",
    "\n",
    "Let's generate a new `vw` training file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(examples)   # this does in-place shuffling\n",
    "\n",
    "def writeToVWFile(filename, examples):\n",
    "    with open(filename, 'w') as h:\n",
    "        for ex in examples:\n",
    "            print(ex, file=h)\n",
    "\n",
    "writeToVWFile('data/sentiment-lex.tr', examples[:1600])\n",
    "writeToVWFile('data/sentiment-lex.te', examples[1600:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're in a position where we can train a model. Let's use exactly the same command as earlier, but with the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cache_file = data/sentiment-lex.tr.cache\n",
      "Reading datafile = data/sentiment-lex.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000      709\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000      575\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000      560\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000      358\n",
      "0.625000 0.750000           16           16.0         1.0000         1.0000      790\n",
      "0.562500 0.500000           32           32.0        -1.0000        -1.0000      316\n",
      "0.578125 0.593750           64           64.0        -1.0000         1.0000      772\n",
      "0.531250 0.484375          128          128.0        -1.0000         1.0000     1355\n",
      "0.445312 0.359375          256          256.0         1.0000         1.0000      911\n",
      "0.347656 0.250000          512          512.0         1.0000         1.0000      916\n",
      "0.280273 0.212891         1024         1024.0         1.0000         1.0000      703\n",
      "0.259912 0.259912         2048         2048.0        -1.0000        -1.0000      737 h\n",
      "0.217582 0.175439         4096         4096.0        -1.0000        -1.0000      693 h\n",
      "0.205495 0.193407         8192         8192.0        -1.0000        -1.0000      585 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disappointingly, we aren't doing any better.\n",
    "\n",
    "One thing we can do that is useful in some cases is *turn off* a subset of the namespaces. For instance, if we want to *only* use the lexicon features, we can tell `vw` to ignore the `w` namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring namespaces beginning with: w\n",
      "creating cache_file = data/sentiment-lex.tr.cache\n",
      "Reading datafile = data/sentiment-lex.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000        3\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000        3\n",
      "0.750000 0.500000            4            4.0        -1.0000        -1.0000        3\n",
      "0.875000 1.000000            8            8.0         1.0000        -1.0000        3\n",
      "0.687500 0.500000           16           16.0         1.0000         1.0000        3\n",
      "0.468750 0.250000           32           32.0        -1.0000         1.0000        3\n",
      "0.531250 0.593750           64           64.0        -1.0000        -1.0000        3\n",
      "0.531250 0.531250          128          128.0        -1.0000         1.0000        3\n",
      "0.484375 0.437500          256          256.0         1.0000         1.0000        3\n",
      "0.474609 0.464844          512          512.0         1.0000        -1.0000        3\n",
      "0.458008 0.441406         1024         1024.0         1.0000         1.0000        3\n",
      "0.475771 0.475771         2048         2048.0        -1.0000        -1.0000        3 h\n",
      "0.424176 0.372807         4096         4096.0        -1.0000        -1.0000        3 h\n",
      "0.393407 0.362637         8192         8192.0        -1.0000        -1.0000        3 h\n",
      "0.374725 0.356044        16384        16384.0        -1.0000         1.0000        3 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 --ignore w\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the error is way higher in this case. On the other hand, this is just using two features (plus a bias).\n",
    "\n",
    "##### Quadratic features\n",
    "\n",
    "The real magic comes from *feature combination*. For instance, the first example from above looks like:\n",
    "\n",
    "    +1 |l pos:4.54329 neg:3.4012 |w note COLON some may consider portions ...\n",
    "    \n",
    "There might be reason to believe that looking at *pairs* of features between the `l` and `w` namespaces would be useful. In this case, these features would be things like:\n",
    "\n",
    "    note_pos:4.5 note_neg:3.4 COLON_pos:4.5 COLON_neg:3.4 some_pos:4.5 ...\n",
    "    \n",
    "(I've rounded 4.54329 to 4.5 and 3.4012 to 3.4 for brevity.)\n",
    "\n",
    "This allows you to model interactions among features. `vw` will do this automatically for you with `-q` (quadratic) features. For example, you can ask for all pairs of features between these two namespaces as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: wl\n",
      "creating cache_file = data/sentiment-lex.tr.cache\n",
      "Reading datafile = data/sentiment-lex.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     2121\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1719\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1674\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000     1068\n",
      "0.562500 0.625000           16           16.0         1.0000         1.0000     2364\n",
      "0.468750 0.375000           32           32.0        -1.0000        -1.0000      942\n",
      "0.515625 0.562500           64           64.0        -1.0000         1.0000     2310\n",
      "0.460938 0.406250          128          128.0        -1.0000        -1.0000     4059\n",
      "0.378906 0.296875          256          256.0         1.0000         1.0000     2727\n",
      "0.300781 0.222656          512          512.0         1.0000         1.0000     2742\n",
      "0.255859 0.210938         1024         1024.0         1.0000         1.0000     2103\n",
      "0.233480 0.233480         2048         2048.0        -1.0000        -1.0000     2205 h\n",
      "0.193407 0.153509         4096         4096.0        -1.0000        -1.0000     2073 h\n",
      "0.185714 0.178022         8192         8192.0        -1.0000        -1.0000     1749 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go crazy if you want and add quadratic features between the `l` namespace and itself and the `w` namespace and itself too. However, it would be significantly slower *and* significantly worse, basically because there are now hundreds of thousands of features, and the model has overfit.\n",
    "\n",
    "Minor note: when creating quadratic features, you can use `:` as a wildcard to refer to \"any namespace\", for instance \"`-q l:`\" pairs `l` with all other namespaces; \"`-q ::`\" pairs all namespaces with all other namespaces.\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "Regularization is a sometime-helpful method for preventing your model from overfitting to the training data. Once you have a reasonable amount of data, regularization in `vw` is relatively **un**helpful, largely because the underlying learning algorithm is quite good. But for small or modest data set sizes, like this sentiment data, it is plausibly useful.\n",
    "\n",
    "`vw` has two built-in forms of regularization: $\\ell_2$ (\"Gaussian\") regularization and $\\ell_1$ (\"sparse\") regularization. You can combing them if you want to get \"elastic net\" regularization. Both forms for regularization require a strength parameter, which usually should be quite small and must be tuned carefully. Doing $\\ell_1$ has the advantage of often producing models with lots of zeros. Here are some runs with both, where the regularization strengths are ones that were identified as effective through experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: wl\n",
      "using l2 regularization = 0.0001\n",
      "final_regressor = data/sentiment-lex2.model\n",
      "creating cache_file = data/sentiment-lex.tr.cache\n",
      "Reading datafile = data/sentiment-lex.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     2121\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1719\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1674\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000     1068\n",
      "0.562500 0.625000           16           16.0         1.0000         1.0000     2364\n",
      "0.468750 0.375000           32           32.0        -1.0000        -1.0000      942\n",
      "0.515625 0.562500           64           64.0        -1.0000         1.0000     2310\n",
      "0.460938 0.406250          128          128.0        -1.0000        -1.0000     4059\n",
      "0.382812 0.304688          256          256.0         1.0000         1.0000     2727\n",
      "0.302734 0.222656          512          512.0         1.0000         1.0000     2742\n",
      "0.258789 0.214844         1024         1024.0         1.0000         1.0000     2103\n",
      "0.237885 0.237885         2048         2048.0        -1.0000        -1.0000     2205 h\n",
      "0.197802 0.157895         4096         4096.0        -1.0000        -1.0000     2073 h\n",
      "0.186813 0.175824         8192         8192.0        -1.0000        -1.0000     1749 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l2 0.0001 -f data/sentiment-lex2.model\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try $\\ell_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: wl\n",
      "using l1 regularization = 1e-06\n",
      "final_regressor = data/sentiment-lex1.model\n",
      "creating cache_file = data/sentiment-lex.tr.cache\n",
      "Reading datafile = data/sentiment-lex.tr\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "average  since         example        example        current        current  current\n",
      "loss     last          counter         weight          label        predict features\n",
      "1.000000 1.000000            1            1.0         1.0000        -1.0000     2121\n",
      "1.000000 1.000000            2            2.0        -1.0000         1.0000     1719\n",
      "0.500000 0.000000            4            4.0        -1.0000        -1.0000     1674\n",
      "0.500000 0.500000            8            8.0         1.0000         1.0000     1068\n",
      "0.562500 0.625000           16           16.0         1.0000         1.0000     2364\n",
      "0.468750 0.375000           32           32.0        -1.0000        -1.0000      942\n",
      "0.515625 0.562500           64           64.0        -1.0000         1.0000     2310\n",
      "0.460938 0.406250          128          128.0        -1.0000        -1.0000     4059\n",
      "0.382812 0.304688          256          256.0         1.0000         1.0000     2727\n",
      "0.302734 0.222656          512          512.0         1.0000         1.0000     2742\n",
      "0.258789 0.214844         1024         1024.0         1.0000         1.0000     2103\n",
      "0.246696 0.246696         2048         2048.0        -1.0000        -1.0000     2205 h\n",
      "0.204396 0.162281         4096         4096.0        -1.0000        -1.0000     2073 h\n",
      "0.192308 0.180220         8192         8192.0        -1.0000        -1.0000     1749 h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vowpalwabbit\n",
    "\n",
    "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l1 0.000001 -f data/sentiment-lex1.model\"\n",
    "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='summary'></a>Summary\n",
    "\n",
    "In this notebook, we've learned lots of ways to get extra features from `vw`. Here's a brief summary:\n",
    "\n",
    "* Using `-b 24` to increase the size of the model, something you should always do\n",
    "* `--affix +6,-2w` to add six character prefixes to features from all namespaces and two character suffixes to features from the w namespace\n",
    "* `--spelling w` to add spelling features to the w namespace (use `--spelling _`) to add spelling features to the default namespace\n",
    "* `--ngram 3 --skips 1` to add one-skip, trigram features to all namespaces\n",
    "* `--loss_function logistic/hinge` to switch the loss function\n",
    "* Using `--early_terminate 999 --holdout_after 1401` to treat the last 200 examples as development data and turn off early stopping\n",
    "* Using namespaces and `-q` for quadratic features (there's also `--cubic` for cubic features!)\n",
    "* Using `--l2` or `--l1` to regularize the model\n",
    "\n",
    "One important thing to remember is that arguments that affect the features that `vw` use get stored in saved models. This means that if you train with `-f model` and then test with `-t -i model`, when you load the model (`-i model`), you *also* load all of the feature generators. This ensures that training and testing use a consistent feature representation, and also means you don't have to remember what arguments you used to train the model.\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Identify which of the variations above does best on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: wl\n",
      "only testing\n",
      "predictions = data/sentiment.te.pred_new1\n",
      "using no cache\n",
      "Reading datafile = data/sentiment.te\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 4800\n",
      "power_t = 0.5\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "\n",
      "205 (-1, 1)\n",
      "189 (-1, -1)\n",
      "4 (1, -1)\n",
      "2 (1, 1)\n",
      "Accuracy: 0.4775\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "import vowpalwabbit\n",
    "\n",
    "model = vowpalwabbit.Workspace(\"--binary -t -i data/sentiment-lex1.model --predictions data/sentiment.te.pred_new1 data/sentiment.te\", enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))\n",
    "\n",
    "counts = {}\n",
    "out = open(\"data/sentiment.te.pred_new1\", 'w')\n",
    "for line in open(\"data/sentiment.te\"):\n",
    "    prediction = model.predict(line.strip())\n",
    "    print(prediction, file=out)\n",
    "    pair = (int(prediction), int(line.split(\"|\")[0]))\n",
    "    counts[pair] = 1 + counts.get(pair, 0)\n",
    "\n",
    "matching = 0\n",
    "total = 0\n",
    "for pair, count in counts.items():\n",
    "    print(count, pair)\n",
    "    total += count\n",
    "    if pair[0] == pair[1]:\n",
    "        matching += count\n",
    "\n",
    "print(\"Accuracy:\", matching / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating quadratic features for pairs: wl\n",
      "only testing\n",
      "predictions = data/sentiment.te.pred_new2\n",
      "using no cache\n",
      "Reading datafile = data/sentiment.te\n",
      "num sources = 1\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 4800\n",
      "power_t = 0.5\n",
      "Enabled learners: gd, scorer-identity, binary, count_label\n",
      "Input label = SIMPLE\n",
      "Output pred = SCALAR\n",
      "\n",
      "205 (-1, 1)\n",
      "188 (-1, -1)\n",
      "5 (1, -1)\n",
      "2 (1, 1)\n",
      "Accuracy: 0.475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import vowpalwabbit\n",
    "\n",
    "model = vowpalwabbit.Workspace(\"--binary -t -i data/sentiment-lex2.model --predictions data/sentiment.te.pred_new2 data/sentiment.te\", enable_logging=True)\n",
    "print(\"\\n\".join(model.get_log()))\n",
    "\n",
    "counts = {}\n",
    "out = open(\"data/sentiment.te.pred_new2\", 'w')\n",
    "for line in open(\"data/sentiment.te\"):\n",
    "    prediction = model.predict(line.strip())\n",
    "    print(prediction, file=out)\n",
    "    pair = (int(prediction), int(line.split(\"|\")[0]))\n",
    "    counts[pair] = 1 + counts.get(pair, 0)\n",
    "\n",
    "matching = 0\n",
    "total = 0\n",
    "for pair, count in counts.items():\n",
    "    print(count, pair)\n",
    "    total += count\n",
    "    if pair[0] == pair[1]:\n",
    "        matching += count\n",
    "\n",
    "print(\"Accuracy:\", matching / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
