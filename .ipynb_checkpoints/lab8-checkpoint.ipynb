{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEN4FYH06K1"
      },
      "source": [
        "# Lab 7 - HuggingFace\n",
        "\n",
        "This is based on the [HuggingFace NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1), which is written by many people (see the page for details) and released under an [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0.html). I have made changes to (1) focus on the aspects relevant to this unit of study, (2) have it fit within a single Notebook, and (3) add some exercises.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [ü§ó Transformers library](https://github.com/huggingface/transformers) is one of the most widely used libraries in ML today. It provides nice interfaces to many of the best models you can run yourself.\n",
        "\n",
        "The company that created it has also developed a range of other tools and resources:\n",
        "- [Models](https://huggingface.co/models)\n",
        "- [Datasets](https://huggingface.co/datasets)\n",
        "- [Example Apps](https://huggingface.co/spaces)\n",
        "- [Discussion Forums](https://huggingface.co/posts)\n",
        "- [Documentation and Explainers](https://huggingface.co/docs)\n"
      ],
      "metadata": {
        "id": "5d7O_MGw1C2Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8yVKdFZ06K2"
      },
      "source": [
        "# Set up\n",
        "\n",
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "piV0jd0006K2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcc5767-672a-4cc4-deee-f26f83ce699a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Off-the-Shelf Processing\n",
        "\n",
        "The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an  answer."
      ],
      "metadata": {
        "id": "coUZ-IL13Hk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Sentiment Analysis\n",
        "\n",
        "Here is an example of a sentiment analysis pipeline provided by HuggingFace:"
      ],
      "metadata": {
        "id": "MoHIs4s9Brkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_ztYuMr06K3",
        "outputId": "1fdee492-3250-493b-9b66-c6350cfcd3ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598047137260437}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Run on one sentence\n",
        "classifier(\"I've been waiting for a HuggingFace lab my whole life.\")\n",
        "\n",
        "# Run on multiple sentences\n",
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace lab my whole life.\", \"I hate blue cheese so much!\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
        "\n",
        "There are three main steps involved when you pass some text to a pipeline:\n",
        "\n",
        "1. The text is preprocessed into a format the model can understand.\n",
        "2. The preprocessed inputs are passed to the model.\n",
        "3. The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "Some of the [pipelines currently provided by HuggingFace](https://huggingface.co/transformers/main_classes/pipelines) are:\n",
        "\n",
        "- `feature-extraction`\n",
        "- `fill-mask`\n",
        "- `ner`\n",
        "- `question-answering`\n",
        "- `sentiment-analysis`\n",
        "- `summarization`\n",
        "- `text-generation`\n",
        "- `translation`\n",
        "- `zero-shot-classification`"
      ],
      "metadata": {
        "id": "liGaEIpj3VXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation\n",
        "\n",
        "Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to systems like ChatGPT, except without being trained to be conversational. This pipeline is set up to do sampling, so the output will vary each time you run it."
      ],
      "metadata": {
        "id": "IpkCJNrm3u3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjuJb1zP06K4",
        "outputId": "6d7538a0-b916-4db5-9ecb-72cfe56ee6c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to understand and use '\n",
              "                    'data flow and data interchange when handling user data. We '\n",
              "                    'will be working with one or more of the most commonly used '\n",
              "                    'data flows ‚Äî data flows of various types, as seen by the '\n",
              "                    'HTTP'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can control how many different sequences are generated with the argument `num_return_sequences` and the total length of the output text with the argument `max_length`.\n",
        "\n",
        "### Task 1\n",
        "\n",
        "Try using these argments to generate two sentences of 15 words each:"
      ],
      "metadata": {
        "id": "iHQJDBrw30Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "R379J3YZ36Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using any model from the Hub in a pipeline\n",
        "\n",
        "The previous examples used the default model for the task at hand, but you can also choose a particular model from HuggingFace's [Model Hub](https://huggingface.co/models). If you go to that page and click on a tag on the left side, you can filter to models for a sepcific task. For example, here is what you get by clicking on ['text generation'](https://huggingface.co/models?pipeline_tag=text-generation).\n",
        "\n",
        "Let‚Äôs try the [distilgpt2](https://huggingface.co/distilgpt2) model. Here‚Äôs how to load it in the same pipeline as before:"
      ],
      "metadata": {
        "id": "9k-wstyr4Asp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlWR6orP06K5",
        "outputId": "2fdfc33d-3e15-4162-8848-74bbc682a30e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to manipulate the world and '\n",
              "                    'move your mental and physical capabilities to your advantage.'},\n",
              " {'generated_text': 'In this course, we will teach you how to become an expert and '\n",
              "                    'practice realtime, and with a hands on experience on both real '\n",
              "                    'time and real'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can refine your search for a model by clicking on the language tags, and pick a model that will generate text in another language.\n",
        "\n",
        "## Mask filling\n",
        "\n",
        "The next pipeline you‚Äôll try is fill-mask. The idea of this task is to replace occurrences of `<mask>` in a given text:"
      ],
      "metadata": {
        "id": "GGF7f9Xh4R3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmjVxVTd06K5",
        "outputId": "fe10399b-b444-41ac-d531-1f6b7b2c7c9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'sequence': 'This course will teach you all about mathematical models.',\n",
              "  'score': 0.19619831442832947,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical'},\n",
              " {'sequence': 'This course will teach you all about computational models.',\n",
              "  'score': 0.04052725434303284,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This lab will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `top_k` argument controls how many possibilities you want to be displayed. Note the choice of `<mask>` is specific to the model used by this pipeline. If you change the model you should check what token it looks for to replace. This is shown on the HuggingFace page.\n",
        "\n",
        "## Named Entity Recognition\n",
        "\n",
        "Let's try HuggingFace's NER pipeline - a task we've done before with spaCy:"
      ],
      "metadata": {
        "id": "8mca04PP4p36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuVhGtIG06K5",
        "outputId": "38e1dc71-2f42-4a2e-b99b-ce523cd2f74f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, \n",
              " {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, \n",
              " {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}\n",
              "]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Jonathan K. Kummerfeld and I teach NLP at The University of Sydney in Australia.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass the option `grouped_entities=True` in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity.\n",
        "\n",
        "Try running an NER pipeline without grouped entities, and see if you can find an input that is processed differently by it:"
      ],
      "metadata": {
        "id": "yBkgkvxO5D8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "M2qus0YBExRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question answering\n",
        "\n",
        "The question-answering pipeline answers questions using information from a given context:"
      ],
      "metadata": {
        "id": "dxdX4K58ExYs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8a4-M0d06K6",
        "outputId": "78f47f12-4fb5-438c-c3eb-e60428caca19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Jonathan K. Kummerfeld and I teach NLP at The University of Sydney in Australia.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note that this pipeline works by ***extracting*** information from the provided context; it does not generate the answer.\n",
        "\n",
        "## Summarization\n",
        "\n",
        "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Here‚Äôs an example:"
      ],
      "metadata": {
        "id": "QdBhDvbT5Td7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmDVogfs06K6",
        "outputId": "05575953-38b5-4ddb-936f-7990153ac0ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' America has changed dramatically during recent years . The '\n",
              "                  'number of engineering graduates in the U.S. has declined in '\n",
              "                  'traditional engineering disciplines such as mechanical, civil '\n",
              "                  ', electrical, chemical, and aeronautical engineering . Rapidly '\n",
              "                  'developing economies such as China and India, as well as other '\n",
              "                  'industrial countries in Europe and Asia, continue to encourage '\n",
              "                  'and advance engineering .'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation\n",
        "\n",
        "For translation, you can use a default model if you provide a language pair in the task name (such as \"`translation_en_to_fr`\"), but the easiest way is to pick the model you want to use on the [Model Hub](https://huggingface.co/models). Here we‚Äôll try translating from French to English:\n",
        "\n"
      ],
      "metadata": {
        "id": "PlSJVp8T5XEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66pGUVMn06K6",
        "outputId": "dc1cf6a6-63ac-46b4-815e-c6d455b59df3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'This course is produced by Hugging Face.'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Le chocolat est toujours aussi d√©licieux!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like with text generation and summarization, you can specify a max_length or a min_length for the result.\n",
        "\n",
        "The pipelines shown so far are mostly for demonstrative purposes. They were programmed for specific tasks and cannot perform variations of them.\n",
        "\n",
        "# Bias and Limitations\n",
        "\n",
        "If your intent is to use a pretrained model or a fine-tuned version in production, be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.\n",
        "\n",
        "To give a quick illustration, let‚Äôs go back the example of a `fill-mask` pipeline with the BERT model:\n"
      ],
      "metadata": {
        "id": "K5pCgNrQ5p8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"This man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result[:4]])\n",
        "\n",
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result[:4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sVMn8cxgYJn",
        "outputId": "56af1b9c-c50c-4c87-e898-d55e3a137716"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['carpenter', 'lawyer', 'farmer', 'businessman']\n",
            "['nurse', 'maid', 'teacher', 'waitress']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When asked to fill in the missing word in these two sentences, the model gives only one or two gender-specific answers (e.g., 'businessman' and 'waitress').\n",
        "\n",
        "Most of the answers are occuptations that anyone could do. The difference between the two lists reflects the bias in the model. When you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won‚Äôt make this intrinsic bias disappear."
      ],
      "metadata": {
        "id": "TFVAdvhdg5_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Preprocessing\n",
        "\n",
        "In this section, we'll start digging into the details of peipelines by looking at how tokenisation and data processing works.\n",
        "\n"
      ],
      "metadata": {
        "id": "F15B7uHv0z7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation\n",
        "\n",
        "Like other neural networks, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
        "\n",
        "Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
        "Mapping each token to an integer\n",
        "Adding additional inputs that may be useful to the model\n",
        "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained() method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below).\n",
        "\n",
        "Since the default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english (you can see its model card here), we run the following:"
      ],
      "metadata": {
        "id": "7KhnF1xnGOcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "xXapNOLF012F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
        "\n",
        "You can use ü§ó Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or TensorFlow, or Flax for some models. However, Transformer models only accept tensors as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It‚Äôs effectively a tensor; other ML frameworks‚Äô tensors behave similarly, and are usually as simple to instantiate as NumPy arrays.\n",
        "\n",
        "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:\n"
      ],
      "metadata": {
        "id": "U2n9OVj003Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "Qa6vrO8o063r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don‚Äôt worry about padding and truncation just yet; we‚Äôll explain those below. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).\n",
        "\n",
        "Loading and saving tokenizers is based on two methods: from_pretrained() and save_pretrained(). These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of a model) as well as its vocabulary (a bit like the weights of a model).\n",
        "\n",
        "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:\n"
      ],
      "metadata": {
        "id": "-qyVMUz308yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)\n",
        "\n",
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "id": "nyTqOGAU08Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\n",
        "\n"
      ],
      "metadata": {
        "id": "6F_qgJRq1a9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "GhEMEnfP1bDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving a tokenizer is straightforward:"
      ],
      "metadata": {
        "id": "BwPh2rgh1h24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "I_nD0I4j1h7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, you saw how sequences get translated into lists of numbers. Let‚Äôs convert this list of numbers to a tensor and send it to the model:\n"
      ],
      "metadata": {
        "id": "sCaUIiLd17SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "id": "64ys2Pkr17YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding the inputs\n",
        "The following list of lists cannot be converted to a tensor:\n",
        "\n"
      ],
      "metadata": {
        "id": "iPOGa0yl2T72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)\n",
        "\n"
      ],
      "metadata": {
        "id": "xGv-BSPG2Wym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There‚Äôs something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we‚Äôve got completely different values!\n",
        "\n",
        "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.\n",
        "\n",
        "Attention masks\n",
        "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
        "\n",
        "Let‚Äôs complete the previous example with an attention mask:\n",
        "\n"
      ],
      "metadata": {
        "id": "5u-JzdZF2iAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "_RsJJa0e2iGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below shows examples of a variety of ways to configure these inputs:"
      ],
      "metadata": {
        "id": "P0pHYG9C2z7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequences)\n",
        "\n",
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs_longest = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs_model_max = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs_custom_max = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "\n",
        "# Will truncate to the max available length\n",
        "model_inputs_custom_max = tokenizer(sequences, truncation=True)"
      ],
      "metadata": {
        "id": "-iKR1GGt20AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:\n",
        "\n"
      ],
      "metadata": {
        "id": "WecxV0h83T1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "O5UcVP-U3T6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer added the special word [CLS] at the beginning and the special word [SEP] at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don‚Äôt add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you.\n",
        "\n"
      ],
      "metadata": {
        "id": "1YAl_WWp3dPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "\n",
        "Set up a BERT tokeniser that will read in the following sentences, pa the short ones to length 5 and truncate the long ones."
      ],
      "metadata": {
        "id": "w3wwgKMDiSdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"Hello!\",\n",
        "    \"Hello World\",\n",
        "    \"Hello World!\",\n",
        "    \"This is an example\",\n",
        "    \"This is an example sentence\",\n",
        "    \"This is a very long example sentence that should be truncated!\"\n",
        "]\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "CcULu9PaiQU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "As you can see, we get a DatasetDict object which contains the training set, the validation set, and the test set. Each of those contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set)."
      ],
      "metadata": {
        "id": "gGIPmJaf3rQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "print(raw_datasets)\n",
        "\n",
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "print(raw_train_dataset[0])\n",
        "\n",
        "# To know which integer corresponds to which label, we can inspect the features of our raw_train_dataset. This will tell us the type of each column:\n",
        "print(raw_train_dataset.features)"
      ],
      "metadata": {
        "id": "6CEpnGa53rXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run our tokeniser on the entire dataset:"
      ],
      "metadata": {
        "id": "ezDe7cMkjRes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_bCAQaaWjUEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the ü§ó Datasets library are Apache Arrow files stored on the disk, so you only keep the samples you ask for loaded in memory).\n",
        "\n",
        "To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so let‚Äôs define a function that tokenizes our inputs:"
      ],
      "metadata": {
        "id": "ERpNaF9Sjz1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "mP3TgeXQj3oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle padding with a dataset being mapped in this way, we can use a Data Collator:"
      ],
      "metadata": {
        "id": "sKDGViBKj9Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "\n",
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "PeiLAHRVkCzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it for now! In future weeks we will learn about fine-tuning and other capabilities of the library."
      ],
      "metadata": {
        "id": "He2RXr4BkK76"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}