{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "\n",
    "Based on [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) by [Sean Robertson](https://github.com/spro)\n",
    "\n",
    "In this lab we will be making a neural network that can translate from French to English.\n",
    "\n",
    "Here is an example of what the final system will do.\n",
    "The line with `>` is the input.\n",
    "The line with `=` is the correct translation.\n",
    "The line wth `<` is the output of a model.\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "> il est en train de peindre un tableau .\n",
    "= he is painting a picture .\n",
    "< he is painting a picture .\n",
    "\n",
    "> elle n est pas poete mais romanciere .\n",
    "= she is not a poet but a novelist .\n",
    "< she not not a poet but a novelist .\n",
    "```\n",
    "\n",
    "Note that sometimes the model is right, and sometimes it is wrong.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "==================\n",
    "\n",
    "The data for this project is a set of many thousands of English to French translation pairs.\n",
    "\n",
    "First, download [this data](https://www.manythings.org/anki/fra-eng.zip) from the Tatoeba project (learn more [here](https://tatoeba.org/en) and see more preprocessed data samples [here](https://www.manythings.org/anki/)).\n",
    "\n",
    "Unzip the file and upload it to be in the same location as this notebook. It should be called `fra.txt`.\n",
    "\n",
    "The data contains lines of tab-separated text like this:\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "See you soon!   À bientôt !     CC-BY 2.0 (France) Attribution: tatoeba.org #32672 (CK) & #337862 (sysko)\n",
    "See you soon!   À tout à l'heure !      CC-BY 2.0 (France) Attribution: tatoeba.org #32672 (CK) & #829076 (Cocorico)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we\\'ll create an index (`Lang`) that maps words to IDs and vice-versa. We'll also have it keep track of how many times a word has been added, which we'll use later to make the vocabulary smaller by ignoring rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Special token identifiers, used to mark the beginning and end of a sentence respectively.\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Defines a class 'Lang' to manage language-specific data.\n",
    "class Lang:\n",
    "    # The class constructor that initializes a new instance of the language data handler.\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # The name of the language (e.g., 'English', 'French').\n",
    "        self.word2index = {}  # A dictionary to map words to their numeric index.\n",
    "        self.word2count = {}  # A dictionary to count occurrences of each word.\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}  # A dictionary to map numeric indices back to words, pre-filled with special tokens.\n",
    "        self.n_words = 2  # The total number of unique words in the vocabulary, starting with 2 to account for the special tokens.\n",
    "\n",
    "    # Adds a sentence to the language model, incrementing the vocabulary and word counts.\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):  # Splits the sentence into words and processes each word.\n",
    "            self.addWord(word)\n",
    "\n",
    "    # Adds a word to the language model, updating the necessary mappings and counts.\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # If the word is new, it is added to all relevant dictionaries and counters.\n",
    "            self.word2index[word] = self.n_words  # Maps the word to the current count of unique words.\n",
    "            self.word2count[word] = 1  # Initializes the word's count to 1.\n",
    "            self.index2word[self.n_words] = word  # Maps the current count of unique words back to the word.\n",
    "            self.n_words += 1  # Increments the total count of unique words.\n",
    "        else:\n",
    "            # If the word already exists, just increments its count.\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode. To simplify the task, we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    # Convert a Unicode string 's' to plain ASCII.\n",
    "    # This is done by first normalizing the string into its decomposed form using 'NFD',\n",
    "    # which separates characters from their accents. Then, it filters out all nonspacing marks (Mn).\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    # First, convert the string to lowercase and strip leading and trailing whitespaces.\n",
    "    # This helps in reducing the variation between different uses of capitalization and spaces.\n",
    "    s = s.lower().strip()\n",
    "\n",
    "    # Convert the string from Unicode to ASCII, removing diacritics (e.g., accents) from characters.\n",
    "    # This is crucial for languages with accented characters, making the text processing uniform.\n",
    "    s = unicodeToAscii(s)\n",
    "\n",
    "    # Insert a space before any punctuation marks (.!?).\n",
    "    # This ensures punctuation is treated as a separate word, aiding in tokenization for NLP tasks.\n",
    "    # For example, \"hello!\" becomes \"hello !\".\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "\n",
    "    # Replace any sequence of characters that are not letters or punctuation marks (.!?)\n",
    "    # with a single space. This step removes numbers and special characters,\n",
    "    # focusing on retaining only textual information that's crucial for most NLP tasks.\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "\n",
    "    # Finally, strip leading and trailing whitespaces that might have been added\n",
    "    # during the normalization process, ensuring the output is tidy.\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs.\n",
    "The file is English → French, so this function has a `revese` flag that will flip the pairs and allow you to translate French → English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Open and read the contents of a text file named after 'lang2'. The file is expected\n",
    "    # to contain sentence pairs in 'lang1' and 'lang2', separated by tabs.\n",
    "    # The sentences are then split into a list where each element is a line from the file.\n",
    "    lines = open('%s.txt' % (lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # For each line in 'lines', split the line into parts using the tab delimiter ('\\t'),\n",
    "    # take the first two parts (assuming they are the sentences in 'lang1' and 'lang2'),\n",
    "    # and apply the 'normalizeString' function to each. The result is a list of lists,\n",
    "    # where each inner list contains a pair of normalized sentences.\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    # If the 'reverse' flag is set to True, reverse the order of sentences in each pair\n",
    "    # (i.e., make 'lang2' sentences come first). This is useful when the model needs\n",
    "    # to translate from 'lang2' to 'lang1' instead of the default 'lang1' to 'lang2'.\n",
    "    # Additionally, initialize 'Lang' objects for input and output languages accordingly.\n",
    "    if reverse:\n",
    "        pairs = [[p[1], p[0]] for p in pairs]  # Swap the sentence order in each pair.\n",
    "        input_lang = Lang(lang2)  # Initialize 'Lang' object for 'lang2' as the input language.\n",
    "        output_lang = Lang(lang1)  # Initialize 'Lang' object for 'lang1' as the output language.\n",
    "    else:\n",
    "        # If 'reverse' is False, keep the order as is and initialize 'Lang' objects\n",
    "        # with 'lang1' as the input language and 'lang2' as the output language.\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    # Return the 'Lang' objects for input and output languages, and the list of sentence pairs.\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we\\'ll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 7 words (that includes\n",
    "ending punctuation) and we\\'re filtering to sentences that start with \"I am\" or \"I'm\".\n",
    "Note that the second one is \"i m \" because the normalizeStrings code converted punctuation to spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set a maximum sentence length. Sentences longer than this limit will be excluded.\n",
    "MAX_LENGTH = 7\n",
    "\n",
    "# Define a tuple of English sentence prefixes to consider. Only sentences starting with these\n",
    "# prefixes will be kept during the filtering process.\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    # Determine if a given pair of sentences ('p') should be kept based on length and prefix criteria.\n",
    "    \n",
    "    # Check if the first sentence in the pair is longer than the MAX_LENGTH.\n",
    "    if len(p[0].split(' ')) >= MAX_LENGTH:\n",
    "        return False  # Exclude the pair if the first sentence is too long.\n",
    "    \n",
    "    # Check if the second sentence in the pair is longer than the MAX_LENGTH.\n",
    "    elif len(p[1].split(' ')) >= MAX_LENGTH:\n",
    "        return False  # Exclude the pair if the second sentence is too long.\n",
    "    \n",
    "    else:\n",
    "        # Check if the first sentence starts with any of the specified prefixes.\n",
    "        for prefix in eng_prefixes:\n",
    "            if p[0].startswith(prefix):\n",
    "                return True  # Keep the pair if the first sentence starts with a valid prefix.\n",
    "    \n",
    "    # Exclude the pair if none of the prefixes match.\n",
    "    return False\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    # Filter a list of sentence pairs using the filterPair criteria.\n",
    "    \n",
    "    keep = []  # Initialize an empty list to store pairs that meet the filtering criteria.\n",
    "    for pair in pairs:\n",
    "        # For each pair in the input list, check if it should be kept.\n",
    "        if filterPair(pair):\n",
    "            keep.append(pair)  # Add the pair to the 'keep' list if it passes the filter.\n",
    "    return keep  # Return the list of pairs that meet the filtering criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-   Read text file and split into lines, split lines into pairs\n",
    "-   Normalize text, filter by length and content\n",
    "-   Make word lists from sentences in pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    # Reads sentence pairs from a file, optionally reversing the sentence order.\n",
    "    # 'lang1' and 'lang2' are names of the languages (e.g., 'eng' for English, 'fra' for French).\n",
    "    # The 'reverse' flag, when set to True, reverses the order in which sentences are read,\n",
    "    # which can be useful for changing the direction of translation.\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    # Filter the read sentence pairs to remove those that don't meet certain criteria,\n",
    "    # such as length or specific starting phrases.\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    # Process each sentence pair, adding the words from each sentence to their respective\n",
    "    # language's vocabulary.\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# Example usage of the prepareData function.\n",
    "# Prepares the data for English to French translation (can be reversed).\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra')\n",
    "# Print a random sentence pair from the prepared data to demonstrate the outcome.\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use a GRU encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        # Inherits from nn.Module, a base class for all neural network modules.\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size  # Sets the size of the hidden layers in the GRU.\n",
    "\n",
    "        # nn.Embedding layer converts token indices to dense vectors of a fixed size,\n",
    "        # 'input_size' is the size of the input vocabulary, and 'hidden_size' is the\n",
    "        # dimensionality of the embedding vector.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # GRU layer: a type of RNN that can handle sequences of variable length.\n",
    "        # Here it is configured to have 'hidden_size' units. 'batch_first=True'\n",
    "        # indicates that the input tensors will have the batch size as the first dimension.\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Dropout layer: a regularization technique where randomly selected neurons are\n",
    "        # ignored during training. This helps prevent overfitting. 'dropout_p' specifies\n",
    "        # the probability of an element to be zeroed.\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Defines the forward pass of the encoder.\n",
    "        # 'input' is the input sequence to the encoder.\n",
    "        \n",
    "        # First, the input is passed through the embedding layer.\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # The embeddings are then passed through a dropout layer to prevent overfitting.\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # The output of the dropout layer is fed into the GRU along with the initial hidden state.\n",
    "        # The GRU returns the output and a new hidden state.\n",
    "        output, hidden = self.gru(embedded)\n",
    "        \n",
    "        # The function returns the output and the final hidden state of the GRU.\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Decoder\n",
    "\n",
    "We'll use a GRU-based decoder too, with attention.\n",
    "\n",
    "This code includes the option to specify 'no attention', in which case some code is run that returns an empty attention matrix. This allows us to vary the nature of the attention method without adjusting the rest of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    # Implements an additive (Bahdanau) attention mechanism.\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        # Linear transformations for the attention mechanism.\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "        # Output size is double the hidden size because it combines context and decoder states.\n",
    "        self.out_size = hidden_size * 2\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        # Computes attention scores and weighted sum (context vector) for the given query and keys.\n",
    "        # Query: decoder's hidden state. Keys: encoder outputs.\n",
    "        # Scores: raw attention scores for each key given the query.\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        # Adjusting dimensions for softmax operation.\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        # Softmax to obtain attention weights.\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # Weighted sum of keys to get the context vector.\n",
    "        context = torch.bmm(weights, keys)\n",
    "        return context, weights\n",
    "\n",
    "class NoAttention(nn.Module):\n",
    "    # A placeholder attention mechanism that does not actually perform attention.\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NoAttention, self).__init__()\n",
    "        self.out_size = hidden_size  # Output size matches hidden size for consistency.\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        # Returns zeros for context and weights, mimicking absence of attention.\n",
    "        context = torch.zeros([query.shape[0], query.shape[1], 0]).to(device)\n",
    "        weights = torch.zeros(keys.shape).to(device)\n",
    "        return context, weights\n",
    "\n",
    "def get_attention_module(name, hidden_size):\n",
    "    # Factory function to select and return an attention module by name.\n",
    "    if name == 'none':\n",
    "        return NoAttention(hidden_size)\n",
    "    elif name == \"additive\":\n",
    "        return AdditiveAttention(hidden_size)\n",
    "    else:\n",
    "        raise Exception(f\"Attention type {name} is not defined\")\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    # Decoder RNN that can use either no attention or additive attention.\n",
    "    def __init__(self, hidden_size, output_size, attention_type=\"none\", dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = get_attention_module(attention_type, hidden_size)\n",
    "        self.gru = nn.GRU(self.attention.out_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        # Main forward pass of the decoder.\n",
    "        # Handles both training mode (with teacher forcing) and inference mode.\n",
    "        # Loops through each time step, applying attention and GRU updates.\n",
    "\n",
    "        # Initialization steps for inputs and hidden states.\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Containers for outputs and attention weights.\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        # Iteratively generate sequence.\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            # Determine next input based on teaching forcing or inference mode.\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # Inference mode\n",
    "\n",
    "        # Concatenate and finalize outputs.\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        # Performs a single decoder step (one time step).\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        # Generate context vector using attention.\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        # Update GRU state.\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Preparing Training Data\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    # Converts a sentence into a list of word indices according to a given language's vocabulary.\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    # Converts a sentence into a PyTorch tensor of word indices, appending the EOS (End of Sentence) token.\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)  # Appends the EOS token's index to signify the end of the sentence.\n",
    "    # Converts the list of indices into a PyTorch tensor and returns it.\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    # Given a pair of sentences (input and target), this function converts both into tensors.\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])  # Input sentence tensor.\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])  # Target sentence tensor.\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    # Prepares the data and creates a DataLoader for batching during training.\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra')  # Prepares and returns language data and pairs.\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)  # Initializes a numpy array for input sentence indices.\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)  # Initializes a numpy array for target sentence indices.\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp) + [EOS_token]  # Gets input indices, appends EOS token.\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt) + [EOS_token]  # Gets target indices, appends EOS token.\n",
    "        # Fills the respective numpy arrays with indices.\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    # Converts the numpy arrays to PyTorch tensors and moves them to the specified device.\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "    # Creates a DataLoader with random sampling for batch generation.\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the `<SOS>` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state. We use 'teacher forcing` as described in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using the given dataloader, encoder, decoder, and optimizers.\n",
    "\n",
    "    Parameters:\n",
    "    - dataloader: DataLoader providing batches of input and target tensors.\n",
    "    - encoder: The encoder model which processes the input tensors.\n",
    "    - decoder: The decoder model which generates the output sequence.\n",
    "    - encoder_optimizer: Optimizer for updating the encoder's weights.\n",
    "    - decoder_optimizer: Optimizer for updating the decoder's weights.\n",
    "    - criterion: Loss function to calculate the difference between\n",
    "                 the decoder's outputs and the target tensors.\n",
    "\n",
    "    Returns:\n",
    "    - The average loss over all batches in this epoch.\n",
    "    \"\"\"\n",
    "    total_loss = 0  # Initialize total loss for this epoch.\n",
    "\n",
    "    # Iterate over batches of data in the dataloader.\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data  # Unpack the batch into input and target tensors.\n",
    "\n",
    "        # Clear gradients before processing the batch.\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Pass the input tensor through the encoder.\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        # Pass the encoder's outputs and hidden state to the decoder, along with the target tensor.\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        # Compute the loss between the decoder's output and the actual target tensor.\n",
    "        # The .view(-1, decoder_outputs.size(-1)) reshapes the decoder's output\n",
    "        # to a 2D tensor where rows correspond to batch elements concatenated together,\n",
    "        # and columns correspond to the output size. The target is similarly flattened.\n",
    "        loss = criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1))\n",
    "\n",
    "        loss.backward()  # Compute the gradient of the loss with respect to model parameters.\n",
    "\n",
    "        # Update the encoder and decoder parameters based on gradients.\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate the loss.\n",
    "\n",
    "    # Calculate the average loss per batch for this epoch.\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    \"\"\"\n",
    "    Converts seconds into a minutes and seconds format.\n",
    "    \n",
    "    Parameters:\n",
    "    - s: The time in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the time in minutes and seconds ('Xd Xm').\n",
    "    \"\"\"\n",
    "    m = math.floor(s / 60)  # Convert seconds to minutes, discarding any remainder.\n",
    "    s -= m * 60  # Calculate the remaining seconds.\n",
    "    return '%dm %ds' % (m, s)  # Format and return the string.\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    \"\"\"\n",
    "    Calculates and formats the time elapsed since a starting point and estimates remaining time.\n",
    "    \n",
    "    Parameters:\n",
    "    - since: The starting time (usually obtained via time.time()).\n",
    "    - percent: The completion percentage of the task.\n",
    "    \n",
    "    Returns:\n",
    "    - A string indicating both the elapsed time and the estimated remaining time.\n",
    "    \"\"\"\n",
    "    now = time.time()  # Get the current time.\n",
    "    s = now - since  # Calculate elapsed time since the start.\n",
    "    es = s / (percent)  # Estimate the total time based on the current progress.\n",
    "    rs = es - s  # Calculate the remaining time by subtracting elapsed time from the total estimated time.\n",
    "    \n",
    "    # Format and return the elapsed and remaining times as a string.\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-   Start a timer\n",
    "-   Initialize optimizers and criterion\n",
    "-   Create training pairs\n",
    "\n",
    "Then we call `train` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100):\n",
    "    \"\"\"\n",
    "    Trains an encoder-decoder model.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dataloader: DataLoader providing batches of data for training.\n",
    "    - encoder: The encoder part of the sequence-to-sequence model.\n",
    "    - decoder: The decoder part of the sequence-to-sequence model.\n",
    "    - n_epochs: Total number of epochs to train the models.\n",
    "    - learning_rate: Learning rate for the optimizers.\n",
    "    - print_every: Frequency of reporting the average loss.\n",
    "    \"\"\"\n",
    "    start = time.time()  # Record the start time for calculating elapsed time.\n",
    "    print_loss_total = 0  # Sum of losses, reset every 'print_every' epochs.\n",
    "\n",
    "    # Initialize optimizers for both encoder and decoder with the Adam algorithm.\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function. NLLLoss is common for classification problems.\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Training loop over the specified number of epochs.\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Perform one epoch of training and return the loss.\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder,\n",
    "                           encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss  # Accumulate loss.\n",
    "\n",
    "        # Every 'print_every' epochs, print the average loss and reset the total loss.\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every  # Calculate average loss.\n",
    "            print_loss_total = 0  # Reset total loss for the next 'print_every' epochs.\n",
    "            # Print a summary: elapsed time, current epoch, progress (%), and average loss.\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder\\'s predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder\\'s\n",
    "attention outputs for display later.\n",
    "\n",
    "We are going to use greedy decoding (top-K with a value of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    # Temporarily disables gradient calculations to save memory and computations since they are not needed.\n",
    "    with torch.no_grad():\n",
    "        # Convert the input sentence into a tensor of word indices.\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        # Pass the input tensor through the encoder to obtain its outputs and final hidden state.\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        # Pass the encoder outputs and hidden state, along with an initial decoder input (if required),\n",
    "        # into the decoder to produce the output sequence.\n",
    "        # Note: This code snippet appears to be missing the part where the initial decoder input and\n",
    "        # subsequent inputs are provided. Typically, the decoder processes one token at a time.\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        # Select the top prediction (highest probability) from the decoder's output at each time step.\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()  # Remove extraneous dimensions.\n",
    "\n",
    "        decoded_words = []  # To store the decoded words.\n",
    "        for idx in decoded_ids:\n",
    "            # Check for the EOS token. If found, append '<EOS>' to the decoded words and stop decoding.\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            # Convert each index back to a word and append to the list of decoded words.\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "\n",
    "    # Return the list of decoded words and any attention weights from the decoder.\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=5):\n",
    "    # Sets the encoder and decoder to evaluation mode, which turns off dropout and batch normalization,\n",
    "    # ensuring consistent behavior for inference.\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Loop over n examples chosen randomly.\n",
    "    for i in range(n):\n",
    "        # Randomly select a sentence pair from the global 'pairs' list.\n",
    "        pair = random.choice(pairs)\n",
    "        \n",
    "        # Print the input sentence from the pair.\n",
    "        print('>', pair[0])\n",
    "        # Print the target (correct) translation or response.\n",
    "        print('=', pair[1])\n",
    "        \n",
    "        # Use the 'evaluate' function to generate the output sentence for the input sentence.\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        # Join the list of output words into a single sentence.\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        # Print the model's translation or response.\n",
    "        print('<', output_sentence)\n",
    "        print('')  # Print a newline for readability between each evaluated pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating\n",
    "\n",
    "Time to initialize a network and start training!\n",
    "\n",
    "To make this efficient, even on a CPU, we have used a small amount of data, with short sentences, and small models. Even so, this will take a few minutes to run.\n",
    "\n",
    "First, we'll train without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the size of the hidden layers in the encoder and decoder models.\n",
    "hidden_size = 128\n",
    "# Specify the batch size for training, determining how many examples are processed together.\n",
    "batch_size = 32\n",
    "\n",
    "# Prepare the dataloader for the training process, which includes loading the dataset,\n",
    "# processing the text into tensors, and batching the data.\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "# Initialize the encoder model with the size of the input language vocabulary and the hidden size.\n",
    "# The model is moved to the 'device', which could be a GPU or CPU.\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "# Initialize the decoder model with the hidden size, the size of the output language vocabulary,\n",
    "# and the type of attention mechanism to use (\"none\" in this case).\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, \"none\").to(device)\n",
    "\n",
    "# Train the encoder and decoder models using the prepared dataloader, specifying the number\n",
    "# of epochs to train for and how frequently to print the training progress.\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=10)\n",
    "\n",
    "# After training, randomly select examples from the dataset and evaluate the performance\n",
    "# of the trained models. This provides a quick qualitative assessment of how well the models\n",
    "# are translating or responding to inputs.\n",
    "evaluateRandomly(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the hidden layers for the models. This impacts the model's capacity to learn complex patterns.\n",
    "hidden_size = 128\n",
    "# Define the batch size, which affects how many examples are processed together in each iteration of training.\n",
    "batch_size = 32\n",
    "\n",
    "# Generate the training data loader, which automates the process of loading the data in batches during training.\n",
    "# It also performs initial preprocessing like tokenizing sentences and converting them into numerical format.\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "# Initialize the encoder model. This model will process the input sequence and generate a context or a series\n",
    "# of contextual embeddings representing the input.\n",
    "# The 'input_lang.n_words' parameter ensures the embedding layer can represent any word in the input vocabulary.\n",
    "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "\n",
    "# Initialize the decoder model with an additive attention mechanism. The decoder uses the context provided by the encoder\n",
    "# to generate the output sequence. The attention mechanism allows the decoder to focus on different parts of the input\n",
    "# sequence at each step of the generation process, improving the ability to handle long sequences.\n",
    "decoder2 = AttnDecoderRNN(hidden_size, output_lang.n_words, \"additive\").to(device)\n",
    "\n",
    "# Train the encoder and decoder models using the prepared data loader. This script sets the models to train for 80 epochs,\n",
    "# and prints out the training progress and loss after every epoch.\n",
    "train(train_dataloader, encoder2, decoder2, 80, print_every=1)\n",
    "\n",
    "# After training, evaluate the model performance by randomly selecting examples from the dataset and\n",
    "# translating them using the trained models. This gives a qualitative measure of how well the model has learned\n",
    "# to translate from the input language to the output language.\n",
    "evaluateRandomly(encoder2, decoder2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Attention\n",
    "\n",
    "Let's have a look at the attention scores being calculated.\n",
    "\n",
    "This code will print a table, with one row for each output token, and values in the row indicating the attention score for each input token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    \"\"\"\n",
    "    Prints a formatted table of attention weights, showing how much focus the\n",
    "    decoder put on each input word for each output word.\n",
    "    \"\"\"\n",
    "    # Print the header row with the input sentence words.\n",
    "    for word in [''] + input_sentence.split() + [\"<EOS>\"]:\n",
    "        print(\"{:>10}\".format(word), end='')\n",
    "    print()\n",
    "    \n",
    "    # Convert the attention tensor to a list for easier processing.\n",
    "    scores = attentions.cpu().tolist()\n",
    "    \n",
    "    # For each output word and corresponding attention weights row...\n",
    "    for word, row in zip(output_words, scores[::-1]):\n",
    "        print(\"{:<10}\".format(word), end='')  # Print the output word.\n",
    "        # Then, print the attention weights for this word against all input words.\n",
    "        for val in row:\n",
    "            print(\"{:>10.1f}\".format(val * 100), end='')  # Format the weights as percentages.\n",
    "        print()\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    \"\"\"\n",
    "    Evaluates an input sentence using the trained encoder and decoder models, then\n",
    "    displays the attention weights for the generated output.\n",
    "    \"\"\"\n",
    "    # Evaluate the input sentence, returning the output words and attention weights.\n",
    "    output_words, attentions = evaluate(encoder2, decoder2, input_sentence, input_lang, output_lang)\n",
    "    \n",
    "    # Print the original input sentence and the model's output.\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    \n",
    "    # Display the attention weights between input and output words.\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
    "    \n",
    "evaluateAndShowAttention('i m quite sure')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Implement dot-product attention, train a model, and measure its performance.\n",
    "\n",
    "To help, note that we can do the dot product step like so:\n",
    "\n",
    "```\n",
    "# Do a dot product by multiplying the two matrices and summing\n",
    "out = (tensor1 * tensor2).sum(-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Modify the encoder to be a bi-directional GRU, and use the Additive attention. The documentation for the GRU should be helpful:\n",
    "\n",
    "\n",
    "To make the change simpler, use 64 dimensions for the hidden state in the encoder (that way when you combine the forward and backward states you will get a 128 dimensional vector, the same as before for the decoder).\n",
    "\n",
    "You may find these two documentation pages helpful:\n",
    "\n",
    "- GRU, for shifting to bi-directional, https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU\n",
    "- Reshape, for combining the hidden states from forward and backward, https://pytorch.org/docs/stable/generated/torch.reshape.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
