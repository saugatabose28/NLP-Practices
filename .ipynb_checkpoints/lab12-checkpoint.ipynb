{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": ""
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 11\n",
                "\n",
                "This week we are going to look at another library: Vowpal Wabbit. This one is optimised for working with HUGE amounts of data. It is rarely used in academic research on NLP, but I've heard it is frequently used in industry for simple tasks that need fast processing.\n",
                "\n",
                "These materials are based on a set of [tutorials](https://github.com/hal3/vwnlp/tree/master) by Hal Daum√© III.\n",
                "\n",
                "## Set up\n",
                "\n",
                "First, we install the library.\n",
                "\n",
                "Note - we will use a Python interface to it for the convenience of running this lab. Most actual users use the command line tool `vw`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install vowpalwabbit"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we'll get some data. We're going to use a Sentiment Analysis dataset from a [2004 paper](https://aclanthology.org/P04-1035/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir data\n",
                "!curl -o data/review_polarity.tar.gz http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
                "!tar zxC data -f data/review_polarity.tar.gz"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can take a look at the beginning of one of the positive reviews and one of the negative reviews:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!head -n3 data/txt_sentoken/pos/cv000_29590.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!head -n3 data/txt_sentoken/neg/cv000_29416.txt"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We need to change this data to the `vw` format. Luckily this data is already tokenized, so we don't have to deal with text preprocessing.\n",
                "\n",
                "The vw format is quite flexible, and we'll see additional fun things you can do later, but for now, the basic file format is one-example per line, with the label first and then a vertical bar (pipe) and then all of the features. For example, for the two above files, we'd want to create two `vw` examples like:\n",
                "\n",
                "    +1 | films adapted from comic books have had plenty of success , whether they're ...\n",
                "    -1 | plot : two teen couples go to a church party , drink and then drive . they get into ...\n",
                "    \n",
                "However, there's an issue here. There are two **reserved characters** in the `vw` format: colon (`:`) and pipe (`|`). This means if those two characters appear in a review then we need to convert them to something else.\n",
                "\n",
                "The function below will make a suitable change:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def textToVW(lines):\n",
                "    return ' '.join([l.strip() for l in lines]).replace(':','COLON').replace('|','PIPE')\n",
                "\n",
                "def fileToVW(inputFile):\n",
                "    return textToVW(open(inputFile,'r').readlines())\n",
                "\n",
                "data = fileToVW('data/txt_sentoken/neg/cv000_29416.txt')[:50]\n",
                "print(data)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here, we see the first few words of the negative review, with ':' replaced by COLON (note that all the other text in the reviews is lowercase, so these new words will not look like the regular words `colon` and `pipe`).\n",
                "\n",
                "Now we just need to read in all the positive examples and all the negative examples:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "def readTextFilesInDirectory(directory):\n",
                "    return [fileToVW(directory + os.sep + f)\n",
                "            for f in os.listdir(directory)\n",
                "            if  f.endswith('.txt')]\n",
                "\n",
                "examples = ['+1 | ' + s for s in readTextFilesInDirectory('data/txt_sentoken/pos')] + \\\n",
                "           ['-1 | ' + s for s in readTextFilesInDirectory('data/txt_sentoken/neg')]\n",
                "\n",
                "print('%d total examples read' % len(examples))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we've got all the files, we put \"`+1 | `\" at the beginning of the positive ones and \"`-1 | `\" at the beginning of the negative ones.\n",
                "\n",
                "We'll now generate some training data and some test data. To achieve this, we're going to permute the examples (after putting in a random seed for reproducability), and then taking the first 80% and train and the last 20% as test.\n",
                "\n",
                "The fact that we're permuting the data is **very important**. By default, `vw` uses an online learning strategy, and if we did something silly like putting all the positive examples before the negative examples, learning would take a LONG time. More on this later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "random.seed(1234)\n",
                "random.shuffle(examples)   # this does in-place shuffling\n",
                "# print out the labels of the first 50 examples to be sure they're sane:\n",
                "print(''.join([s[0] for s in examples[:50]]))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we can write the first 1600 to a training file and the last 400 to a test file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def writeToVWFile(filename, examples):\n",
                "    with open(filename, 'w') as h:\n",
                "        for ex in examples:\n",
                "            print(ex, file=h)\n",
                "\n",
                "writeToVWFile('data/sentiment.tr', examples[:1600])\n",
                "writeToVWFile('data/sentiment.te', examples[1600:])\n",
                "\n",
                "!wc -l data/sentiment.tr data/sentiment.te"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "At this point, everything is properly set up and we can run `vw`!\n",
                "\n",
                "# \u003ca id='run'\u003e\u003c/a\u003e  Running VW for the First Time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 1\", enable_logging=True)\n",
                "\n",
                "for line in model.get_log():\n",
                "    print(line)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This output consists of two parts:\n",
                "\n",
                "1. The header, which displays some information about the parameters `vw` is using to do the learning (number of bits, learning rate, ..., number of sources). We'll discuss (some) of these later.\n",
                "2. The progress list (the lines with lots of numbers); much more on this below.\n",
                "\n",
                "One important note is that when we set up the workspace, we used the argument `binary`, which instructs `vw` to report all losses as zero-one loss.\n",
                "\n",
                "Let's look first at the first four lines of the progress list:\n",
                "\n",
                "    average  since         example        example  current  current  current\n",
                "    loss     last          counter         weight    label  predict features\n",
                "    1.000000 1.000000            1            1.0         1.0000        -1.0000      782\n",
                "    1.000000 1.000000            2            2.0        -1.0000         1.0000      627\n",
                "    0.500000 0.000000            4            4.0        -1.0000        -1.0000      440\n",
                "    0.625000 0.750000            8            8.0         1.0000        -1.0000     1073\n",
                "    \n",
                "The columns are labeled, which gives some clue as to what's being printed out. The way `vw` works internally is that it processes one example at a time. At every $2^k$th example (examples 1, 2, 4, 8, 16, ...), it prints out a status update. This way you get lots of updates early (as a sanity check) and fewer as time goes on. The third column gives you the example number. The fourth column tells you the total \"weight\" of examples so far; right now all examples have a weight of 1.0, but for some problems (e.g., imbalanced data), you might want to give different weight to different examples. The fifth column tells you the true current label (+1 or -1) and the sixth column tells you the models' current prediction. Lastly, it tells you how many features there are in this example.\n",
                "\n",
                "The first two columns deserve some explanation. In \"default\" mode, `vw` reports \"progressive validation loss.\" This means that when `vw` sees a training example, it *first* makes a prediction. It then computes a loss on that single prediction. Only after that does it \"learn\". The average loss computed in this case is the **progressive validation loss.** It has a nice property that it's a good estimate of test loss, *provided you only make one pass over the data*, **and** it's efficient to compute. The first column tells you the average progressive loss over the *entire* run of `vw`; the second column tells you the average progressive loss *since the last time `vw` printed something*.\n",
                "\n",
                "In practice, this second column is what you want to look at for telling how well your model is doing.\n",
                "\n",
                "# Your Second Run of VW\n",
                "\n",
                "There are a couple of things we need to do to get a useful system. The first is that for most data sets, a single online pass over the data is insufficient -- we need to run more than one. The second is that we actually need to store the model somewhere so that we can make predictions on test data! We'll go through these in order.\n",
                "\n",
                "## \u003ca id='passes'\u003e\u003c/a\u003e  Running More than One Pass\n",
                "\n",
                "On the surface, running more than one pass seems like an easy thing to ask `vw` to do. It's a bit more complicated than it might appear.\n",
                "\n",
                "The first issue is that one of the main speed bottlenecks for `vw` is file IO. Reading, and parsing, your input data is incredibly time consuming. In order to get around this, when multiple passes over the data are requested, `vw` will create and use a **cache file**, which is basically a second copy of your data stored in a `vw`-friendly, efficient, binary format. So if you want to run more than one pass, you have to tell `vw` to create a cache file.\n",
                "\n",
                "Here's an example running 5 passes:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 5 -c -k\", enable_logging=True)\n",
                "\n",
                "for line in model.get_log():\n",
                "    print(line)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this command, we added three new command-line options:\n",
                "\n",
                "* `--passes 5`: this is the most obvious one: it tells `vw` to run five passes over the data.\n",
                "* `-c`: this tells `vw` to automatically create and use a cache file\n",
                "* `-k`: by default, if `vw` uses a cache file, it *first* checks to see if the file exists. If the cache file already exists, it completely ignores the data file (`sentiment.tr`) and *just* uses the cache file. This is great if your data never changes because it makes the first pass slightly faster, but it can trip you up if you make changes to your data and forget. `-k` tells `vw` to \"kill\" the old cache file: even if it exists, it should be recreated from scratch.\n",
                "\n",
                "(Warning: if you're running multiple jobs on the same file in parallel, you will get clashes on the cache file. You should either create a single cache file ahead of time and use it for all jobs [remove `-k` in that case], *or* you should explicitly give your own file names to the cache by saying `--cache myfilename0.cache` instead of `-c`.)\n",
                "\n",
                "If you're particularly attentive, you might have noticed that there are a few \"`h`\"s in the progress list (and in the printing of the average loss at the end).\n",
                "\n",
                "This is **holdout** loss. Remember all that discussion of progressive validation loss? Well, it's useless when you're making more than one pass. That's because on the second pass, you'll already have trained on all the training data, so your model is going to be exceptionally good at making predictions.\n",
                "\n",
                "`vw`'s default solution to this is to holdout a fraction of the training data as validation data. By default, it will hold out **every 10th example** as test. The holdout loss (signaled by the `h`) is then the average loss, *limited to these 10% of the training examples*. (Note that on the first pass, it still prints progressive validation loss because this is a safe thing to do.)\n",
                "\n",
                "## \u003ca id='save'\u003e\u003c/a\u003e  Saving the Model and Making Test Predictions\n",
                "\n",
                "Now that we know how to do several passes and get heldout losses, we might want to actually save the learned model to a file so we can make predictions on test data! This is easy: we just tell `vw` where to save the final model using `-f file` (`-f` means \"final\"). Let's do this, and crank up the number of passes to 20:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model\", enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And now, we have a model:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ls -l data/sentiment.model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One thing you might have noticed is that even though we asked `vw` for 20 passes, it actually only did 9! This happens because by default `vw` does early stopping: if the holdout loss ceases to improve for three passes over the data, it stops optimizing and stores the *best* model found so far. We will later see how to adjust these defaults.\n",
                "\n",
                "## \u003ca id='test'\u003e\u003c/a\u003e Making Predictions\n",
                "\n",
                "Now we want to make predictions. In order to do this, we have to (a) tell `vw` to load a model, (b) tell it only to make predictions (and not to learn), and (c) tell it where to store the predictions. (Ok, technically we don't need to store the predictions anywhere if all we want to know is our error rate, but I'll assume we actually care about the output of our system.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "model = vowpalwabbit.Workspace(\"--binary -t -i data/sentiment.model --predictions data/sentiment.te.pred data/sentiment.te\", enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))\n",
                "\n",
                "counts = {}\n",
                "out = open(\"data/sentiment.te.pred\", 'w')\n",
                "for line in open(\"data/sentiment.te\"):\n",
                "    prediction = model.predict(line.strip())\n",
                "    print(prediction, file=out)\n",
                "    pair = (int(prediction), int(line.split(\"|\")[0]))\n",
                "    counts[pair] = 1 + counts.get(pair, 0)\n",
                "\n",
                "matching = 0\n",
                "total = 0\n",
                "for pair, count in counts.items():\n",
                "    print(count, pair)\n",
                "    total += count\n",
                "    if pair[0] == pair[1]:\n",
                "        matching += count\n",
                "\n",
                "print(\"Accuracy:\", matching / total)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's go through these options in turn:\n",
                "\n",
                "* `--binary`: as before, tell `vw` that this is a binary classification problem and to report loss as a zero-one value\n",
                "* `-t`: put `vw` in test mode. You might assume that because we're loading a model to start with, `vw` would be in test mode by default. You would be wrong. Sometimes it's useful to start from a pre-trained model and continue training later.\n",
                "* `-i data/sentiment.model`: tell `vw` to load an **i**nitial model from the specified file\n",
                "\n",
                "\n",
                "We can now take a look at the predictions:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!head data/sentiment.te.pred"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And yay, we've successfully made predictions!\n",
                "\n",
                "You're now in a position where you can successfully: download data, process it into `vw` format, train a predictor on it, and use that predictor to make test predictions.\n",
                "\n",
                "## Task 1\n",
                "\n",
                "Try running this model on the training data.\n",
                "\n",
                "This should give an accuracy of 98%+. Why would we want to do this? Sometimes it is informative to check training performance. If a model as simple as the one used here is getting 100% then it is probably overfitting. If it is getting a very low score then something is wrong."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Improving performance\n",
                "\n",
                "`vw` has *lots* of command-line arguments. For some of them you have to learn a little bit about how `vw` works internally.\n",
                "\n",
                "### Increasing Representational Capacity (and memory usage)\n",
                "\n",
                "Let's start with our previous training example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "model = vowpalwabbit.Workspace(\"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model\", enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Internally, in order to be fast, `vw` never stores any strings. When it reads your input (in which your features were represented as strings!), it *immediately* hashes the strings to some feature index. By default, it uses $2^{18}$ possible feature indices; this magic number $18$ is the \"number of bits\" used to store the weights in the model. (This is something of a misnomer: it's really the number of parameters in the learning algorithm, which is roughly the number of floats.)\n",
                "\n",
                "What does this hashing accomplish? Well, it accomplishes speed because no string manipulation ever happens. However, it comes with two downsides. The first is that you can get hash collisions. In particular, you might have to different features (i.e., different strings) that hash to the same location. From the learning algorithm's perspective, this means these two features are indistinguishable.\n",
                "\n",
                "Remember that hash collisions are incredibly common. In a bag of words, we often have several hundred thousand unique features. The probability of collision when you have $k$ items into $N$ buckets is approximately $1-\\exp\\left[\\frac {k(k-1)} {2N}\\right]$. In this case, with $N=2^{18}$, even with only $2000$ unique features, the probability of collision is already 99.95%. With $100k$ features it's basically guaranteed.\n",
                "\n",
                "The solution is to increase the number of bits used in the representation. This will (a) reduce the number of collisions, (b) make `vw` take more RAM, (c) make `vw` somewhat slower, and (d) make the resulting models larger on disk. Currently, the maximum number of bits that `vw` will let you use is 31. I don't suggest using this; it means `vw` will consume about 8GB of memory while running and the resulting file may take as much as 2GB of disk space. [Runtime memory will be 4 times larger than disk space because the optimization algorithms need extra working memory.] But note that with 100k unique features, even with 31 bits, the probability of collision is 99.1%. It will happen.\n",
                "\n",
                "Let's try running with 24 bits:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this case, increasing the number of bits did not help test accuracy (in fact, it may have hurt!), but we will see that, when we add additional features, it becomes more important.\n",
                "\n",
                "###  Fun NLP-esque Features for Free\n",
                "\n",
                "One nice thing about `vw` is that it internally supports \"extra feature\" generation. The main useful features are: word prefixes and suffixes, spelling features, and ngram features.\n",
                "\n",
                "#### Word Affixes\n",
                "\n",
                "For NLP tasks that mostly depend on the *meaning* (aka \"semantics\") of words, we often don't care about the funny little things that come at the ends of words. For instance, for sentiment classification, the words `awesome` and `awesomeness` are likely to roughly mean the same thing. For other tasks, like part of speech tagging, it's the suffixes that might matter most: words that end in `-ness` are much more likely to be adjectives than anything else.\n",
                "\n",
                "`vw` can automatically generate word prefixes and suffixes for you, using the `--affix` feature. For instance, if you add \"`--affix +5,-3`\" to the command line, this says to automatically compute (and add as new features) 5-character prefixes (that's the `+`) and three character suffixes (that's the `-`).\n",
                "\n",
                "Let's try it:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --affix +6\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "That was (somewhat) helpful.\n",
                "\n",
                "#### Spelling Features\n",
                "\n",
                "Spelling features are *super* useful for tasks where things like capitalization, years, numbers, etc. matter a lot. In other words, tasks *not at all* like sentiment classification.\n",
                "\n",
                "In `vw`, the spelling features option tells it to generate new features based on the word forms seen. For example, a word \"Alice\" has the word form \"Aaaaa\" (meaning: a capital letter followed by four lowercase letters); \"VanBuren\" has the form \"AaaAaaaa\". The general rule is that digits 0-9 get mapped to \"0\", letters a-z to \"a\", letters A-Z to \"A\", period to \".\" and anything else to \"#\". Thus, \"xY9s,3.80vaq\" gets mapped to \"aA0a#0.00aaa\" and this new \"word form\" is used as a new feature.\n",
                "\n",
                "To turn on spelling features, you simply add `--spelling _` to the command line. We can do this with little expectation that it will help:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \" --binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --spelling _\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Nope, indeed it did not help and it actually hurt slightly (see the bottom of the 'since last' column).\n",
                "\n",
                "You might wonder what the \"`_`\" in the arguments means; for now, don't worry about it. We'll come back to it below when we introduce namespaces.\n",
                "\n",
                "#### N-gram Features\n",
                "\n",
                "Our current representation for learning is bag of words. As we've seen at many points in the course, n-grams can be usefull to consider."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 2\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Wow, that was super useful!\n",
                "\n",
                "We can try trigram features too:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 24 --ngram 3\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Okay, that didn't help any more.\n",
                "\n",
                "We can, however, now see that the number of bits matters. If we drop the number of bits back down to 18, we get:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model -b 18 --ngram 3\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is no better than we started with.\n",
                "\n",
                "More specifically: **if we hadn't increased the number of bits, we would have concluded that ngram features weren't useful!**\n",
                "\n",
                "Finally, `vw` can do \"skip ngrams\" too. This means that instead of only looking at bigrams of adjacent words, you can look at bigrams with some gap. For instance, if you say `--ngram 2 --skips 1`, this means \"compute all bigrams that have at most one gap in them.\" For our favorite sentence \"the monster ate a sandwich\", you would get the default bigram features (\"the_monster monster_ate ate_a a_sandwich\") and *also* the skips (\"the_ate monster_a ate_sandwich\").\n",
                "\n",
                "Note: as you increase to, say, four-grams, this automatically includes bigrams and trigrams. As you increase number of skips, you get all the lower order skips too.\n",
                "\n",
                "#### Changing the Loss Function\n",
                "\n",
                "By default, `vw` optimizes squared loss. This means that if the correct label for an example is +1, and the model predicts -1, the error is 4.0. However, if the model predicts +3, the error is still 4.0, even though it's making the right binary prediction. Squared loss has the nice property that it estimates means. But it's not necessarily the most natural loss for classification problems.\n",
                "\n",
                "Many people prefer logistic loss (which gives a nice probabilistic interpretation) or hinge loss (which, when combined with regularization, yields support vector machines). You can switch the loss function with a simple flag:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function logistic\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can alternatively switch to hinge loss:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --loss_function hinge\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Default Holdout Settings\n",
                "\n",
                "Earlier, we mentioned that the default way `vw` works for doing multiple passes is: on the first pass, perform progressive validation; on subsequent passes, use every 10th example as a heldout \"validation\" example. And to stop optimizing when things don't improve for three passes.\n",
                "\n",
                "These are reasonable defaults, but somewhat at odds with the behavior we often want.\n",
                "\n",
                "First, we often *don't* want `vw` to do early stopping. If we tell it to do 20 passes, then it should do 20 passes. This is easy. We just say `--early_terminate 999`. This means that instead of needing 3 passes of no-improvement in order to terminate, it now needs 999. Since we never run that many passes, this is a good default to say \"don't stop early.\" However, it *will* still output only the best model found.\n",
                "\n",
                "More relevant, often in NLP we have training data, development data, and test data. And I want to get validation performance on the development data rather than every-10th-example. You can accomplish this with `--holdout_after N`. What this means is: instead of doing every-10th-example as validation, use the first (N-1) examples as training data, and anything after that as development data.\n",
                "\n",
                "Putting these together, we can do something like:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment.tr --passes 20 -c -k -f data/sentiment.model --early_terminate 999 --holdout_after 1401\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here, the important thing is that the first 1400 examples are used as training data, the the remaining examples (in this case, 200) are used as heldout data. The average loss reported is then *precisely* the average loss on this heldout data.\n",
                "\n",
                "#### Namespaces and quadratic features\n",
                "\n",
                "For this part of the tutorial to make sense, we have to make our task a little more interesting by using a *sentiment lexicon*: basically, a list of positive-ish and negative-ish words. There are [lots of sentiment lexicons](http://sentiment.christopherpotts.net/lexicons.html). We will use [the one from Bing Liu](http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). First, let's download it and decompress it:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install unrar\n",
                "!rm -f data/*words.txt\n",
                "!curl -o data/opinion-lexicon-English.rar https://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\n",
                "!unrar x data/opinion-lexicon-English.rar  data\n",
                "!ls -l data/*/*words.txt"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can look at some of the positive words:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!head -n50 data/opinion-lexicon-English/positive-words.txt | grep -v '^;'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this, I dropped lines that begin with \"`;`\" because these are comments in the files.\n",
                "\n",
                "We now want to go back and generate some new data files for `vw` that include lexicon features. In particular, we will include *both* the bag of words representation *as well as* lexicon features. The lexicon features we will use are very simple: the log of the count of words in the document that are on the positive list, and the log of the count on the negative list. We use logs because getting more positive words has diminishing returns.\n",
                "\n",
                "To do this, we'll write a bit more python:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loadLexicon(filename):\n",
                "    with open(filename, encoding = \"ISO-8859-1\") as h:\n",
                "        return set([l.strip()\n",
                "                    for l in h.readlines()\n",
                "                    if  not l.startswith(';') and len(l) \u003e 1])\n",
                "\n",
                "import math\n",
                "def countLexiconWords(text, lexicon):\n",
                "    return math.log(1.0 + len([w for w in text if w in lexicon]))\n",
                "\n",
                "positiveLexicon = loadLexicon('data/opinion-lexicon-English/positive-words.txt')\n",
                "negativeLexicon = loadLexicon('data/opinion-lexicon-English/negative-words.txt')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now have a copy of the two lexicons and we want to generate `vw` examples.\n",
                "\n",
                "But we have two different types of features. We have the original bag of words features. And we have these lexicon features. We'd like to keep them separate.\n",
                "\n",
                "This is where feature namespaces come in. We're going to create examples with *two* namespaces, one for the bag of words (let's call it the `w` namespace) and one for the lexicon features (let's call that the `l` namespace). In `vw`, namespaces are separated by pipes, so an example might look like:\n",
                "\n",
                "    +1 |l pos:5 neg:2 |w some words might go here ...\n",
                "    \n",
                "In addition to having two namespaces, this example also shows how to use feature values. By default, all features in a `vw` example get a value of one. If you want to override this, you can say something like \"`pos:5`\", which means that there's a single feature (called \"`pos`\") that has a feature value of 5.\n",
                "\n",
                "Let's generate data like this. Some of the code is copied from the Getting Started tutorial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def textToVW(lines):\n",
                "    return ' '.join([l.strip() for l in lines]).replace(':','COLON').replace('|','PIPE')\n",
                "\n",
                "def fileToVW(inputFile, posLex, negLex):\n",
                "    text     = textToVW(open(inputFile,'r').readlines())\n",
                "    words    = text.split()\n",
                "    posCount = countLexiconWords(words, posLex)\n",
                "    negCount = countLexiconWords(words, negLex)\n",
                "    return '|l pos:%g neg:%g |w ' % (posCount,negCount) + text\n",
                "\n",
                "import os\n",
                "def readTextFilesInDirectory(directory):\n",
                "    return [fileToVW(directory + os.sep + f, positiveLexicon, negativeLexicon)\n",
                "            for f in os.listdir(directory)\n",
                "            if  f.endswith('.txt')]\n",
                "\n",
                "examples = ['+1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/pos')] + \\\n",
                "           ['-1 ' + s for s in readTextFilesInDirectory('data/txt_sentoken/neg')]\n",
                "\n",
                "print('{} total examples read'.format(len(examples)))\n",
                "print('first example: {}...'.format(examples[ 0][:70]))\n",
                "print('last  example: {}...'.format(examples[-1][:70]))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "At least based on these two examples, this seems promising: the positive/negative lexicon features seem to correlate with the labels!\n",
                "\n",
                "Let's generate a new `vw` training file:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "random.seed(1234)\n",
                "random.shuffle(examples)   # this does in-place shuffling\n",
                "\n",
                "def writeToVWFile(filename, examples):\n",
                "    with open(filename, 'w') as h:\n",
                "        for ex in examples:\n",
                "            print(ex, file=h)\n",
                "\n",
                "writeToVWFile('data/sentiment-lex.tr', examples[:1600])\n",
                "writeToVWFile('data/sentiment-lex.te', examples[1600:])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we're in a position where we can train a model. Let's use exactly the same command as earlier, but with the new data:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Disappointingly, we aren't doing any better.\n",
                "\n",
                "One thing we can do that is useful in some cases is *turn off* a subset of the namespaces. For instance, if we want to *only* use the lexicon features, we can tell `vw` to ignore the `w` namespace:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 --ignore w\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Well, the error is way higher in this case. On the other hand, this is just using two features (plus a bias).\n",
                "\n",
                "##### Quadratic features\n",
                "\n",
                "The real magic comes from *feature combination*. For instance, the first example from above looks like:\n",
                "\n",
                "    +1 |l pos:4.54329 neg:3.4012 |w note COLON some may consider portions ...\n",
                "    \n",
                "There might be reason to believe that looking at *pairs* of features between the `l` and `w` namespaces would be useful. In this case, these features would be things like:\n",
                "\n",
                "    note_pos:4.5 note_neg:3.4 COLON_pos:4.5 COLON_neg:3.4 some_pos:4.5 ...\n",
                "    \n",
                "(I've rounded 4.54329 to 4.5 and 3.4012 to 3.4 for brevity.)\n",
                "\n",
                "This allows you to model interactions among features. `vw` will do this automatically for you with `-q` (quadratic) features. For example, you can ask for all pairs of features between these two namespaces as:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can go crazy if you want and add quadratic features between the `l` namespace and itself and the `w` namespace and itself too. However, it would be significantly slower *and* significantly worse, basically because there are now hundreds of thousands of features, and the model has overfit.\n",
                "\n",
                "Minor note: when creating quadratic features, you can use `:` as a wildcard to refer to \"any namespace\", for instance \"`-q l:`\" pairs `l` with all other namespaces; \"`-q ::`\" pairs all namespaces with all other namespaces.\n",
                "\n",
                "#### Regularization\n",
                "\n",
                "Regularization is a sometime-helpful method for preventing your model from overfitting to the training data. Once you have a reasonable amount of data, regularization in `vw` is relatively **un**helpful, largely because the underlying learning algorithm is quite good. But for small or modest data set sizes, like this sentiment data, it is plausibly useful.\n",
                "\n",
                "`vw` has two built-in forms of regularization: $\\ell_2$ (\"Gaussian\") regularization and $\\ell_1$ (\"sparse\") regularization. You can combing them if you want to get \"elastic net\" regularization. Both forms for regularization require a strength parameter, which usually should be quite small and must be tuned carefully. Doing $\\ell_1$ has the advantage of often producing models with lots of zeros. Here are some runs with both, where the regularization strengths are ones that were identified as effective through experimentation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l2 0.0001 -f data/sentiment-lex.model\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also try $\\ell_1$:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import vowpalwabbit\n",
                "\n",
                "args = \"--binary data/sentiment-lex.tr --passes 20 -c -k --loss_function logistic -b 24 -q wl --l1 0.000001 -f data/sentiment-lex.model\"\n",
                "model = vowpalwabbit.Workspace(args, enable_logging=True)\n",
                "print(\"\\n\".join(model.get_log()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003ca id='summary'\u003e\u003c/a\u003eSummary\n",
                "\n",
                "In this notebook, we've learned lots of ways to get extra features from `vw`. Here's a brief summary:\n",
                "\n",
                "* Using `-b 24` to increase the size of the model, something you should always do\n",
                "* `--affix +6,-2w` to add six character prefixes to features from all namespaces and two character suffixes to features from the w namespace\n",
                "* `--spelling w` to add spelling features to the w namespace (use `--spelling _`) to add spelling features to the default namespace\n",
                "* `--ngram 3 --skips 1` to add one-skip, trigram features to all namespaces\n",
                "* `--loss_function logistic/hinge` to switch the loss function\n",
                "* Using `--early_terminate 999 --holdout_after 1401` to treat the last 200 examples as development data and turn off early stopping\n",
                "* Using namespaces and `-q` for quadratic features (there's also `--cubic` for cubic features!)\n",
                "* Using `--l2` or `--l1` to regularize the model\n",
                "\n",
                "One important thing to remember is that arguments that affect the features that `vw` use get stored in saved models. This means that if you train with `-f model` and then test with `-t -i model`, when you load the model (`-i model`), you *also* load all of the feature generators. This ensures that training and testing use a consistent feature representation, and also means you don't have to remember what arguments you used to train the model.\n",
                "\n",
                "# Task 2\n",
                "\n",
                "Identify which of the variations above does best on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO"
            ]
        }
    ]
}
