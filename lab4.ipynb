{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "\n",
    "We will be building and training a basic character-level Recurrent Neural\n",
    "Network (RNN) to classify words. This lab is based on Based on [\"NLP From Scratch: Classifying Names with a Character-Level RNN\"](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) by [Sean Robertson](https://github.com/spro).\n",
    "\n",
    "A character-level RNN reads words as a series of characters and for each one it (a) produces an output, and (b) updates a hidden state vector. The output hidden state from one step is in the input to the next step. In this lab, the final prediction will be made based on the last output.\n",
    "\n",
    "The task we'll consider is predicting the language of origin of a name.\n",
    "We'll train on a few thousand surnames from 18 languages\n",
    "of origin, and predict which language a name is from based on the\n",
    "spelling. For example:\n",
    "\n",
    "```sh\n",
    "$ python predict.py Hinton\n",
    "(-0.47) Scottish\n",
    "(-1.52) English\n",
    "(-3.57) Irish\n",
    "\n",
    "$ python predict.py Schmidhuber\n",
    "(-0.19) German\n",
    "(-2.48) Czech\n",
    "(-2.68) Dutch\n",
    "```\n",
    "\n",
    "\n",
    "## Download the Data\n",
    "\n",
    "Download the data and extract it:\n",
    "\n",
    "1. Open a terminal (purple + button in the top left, then select Terminal in the bottom row)\n",
    "2. Run:\n",
    "\n",
    "```\n",
    "wget https://download.pytorch.org/tutorial/data.zip\n",
    "unzip data.zip\n",
    "```\n",
    "\n",
    "## Preparing the Data\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "``[Language].txt``. Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found:\n",
      "['data/names/Arabic.txt', 'data/names/Chinese.txt', 'data/names/Czech.txt', 'data/names/Dutch.txt', 'data/names/English.txt', 'data/names/French.txt', 'data/names/German.txt', 'data/names/Greek.txt', 'data/names/Irish.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Portuguese.txt', 'data/names/Russian.txt', 'data/names/Scottish.txt', 'data/names/Spanish.txt', 'data/names/Vietnamese.txt']\n",
      "\n",
      "Example conversion from Unicode to ASCII:\n",
      "Input: Ślusàrski\n",
      "Output: Slusarski\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get files\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "print(\"Files found:\")\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(\"\\nExample conversion from Unicode to ASCII:\")\n",
    "print(\"Input: Ślusàrski\")\n",
    "print(\"Output:\", unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ``category_lines``, a dictionary mapping each category\n",
    "(language) to a list of lines (names). We also kept track of\n",
    "``all_categories`` (just a list of languages) and ``n_categories`` for\n",
    "later reference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Names into Tensors\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a one-hot vector of size\n",
    "`<1 x n_letters>`. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>`.\n",
    "In lecture 1, we noted that usually we use special data structures to avoid memory overhead (e.g., dictionaries or sparse vectors).\n",
    "In this lab, we'll use a normal vector for convenience.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the tensor for 'J':\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "\n",
      "This is the dimensionality of the matrix for 'Jones':\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(\"This is the tensor for 'J':\")\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(\"\\nThis is the dimensionality of the matrix for 'Jones':\")\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Network\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "This RNN module is just 2 linear layers which operate on an input and hidden state, with\n",
    "a ``LogSoftmax`` layer after the output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define the layers of the model\n",
    "        # These also create the weights where needed\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # Set the weights to some initial values\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialise the weights to be random values in the matrices and zero for the biases\n",
    "        initrange = 0.1\n",
    "        self.i2h.weight.data.uniform_(-initrange, initrange)\n",
    "        self.i2h.bias.data.zero_()\n",
    "        self.h2o.weight.data.uniform_(-initrange, initrange)\n",
    "        self.h2o.bias.data.zero_()\n",
    "        \n",
    "    def initHidden(self):\n",
    "        # Define the initial hidden state\n",
    "        # Here we use an all zero vector\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor, hidden):\n",
    "        # Given an input, compute the steps defined by the model\n",
    "\n",
    "        # Concatenate the input and hidden vectors\n",
    "        combined = torch.cat((input_tensor, hidden), 1)\n",
    "        \n",
    "        # Apply a linear layer to get the new hidden vector\n",
    "        hidden = self.i2h(combined)\n",
    "\n",
    "        # Apply a linear layer to get the output scores\n",
    "        output = self.h2o(hidden)\n",
    "\n",
    "        # Use softmax to turn the scores into probabilities\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "Note - we haven't trained the model yet, so it's outputs will be random.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = letterToTensor('A')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input_tensor, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for\n",
    "every step, so we will use ``lineToTensor`` instead of\n",
    "``letterToTensor`` and use slices. This could be further optimized by\n",
    "precomputing batches of Tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'Albert' through the RNN\n",
      "\n",
      "Likelihood for each label is:\n",
      "tensor([[-2.8910, -2.8665, -2.8854, -2.9692, -2.9065, -2.8366, -2.8959, -2.9115,\n",
      "         -2.8974, -2.8806, -2.9244, -2.8601, -2.9092, -2.9157, -2.8429, -2.8944,\n",
      "         -2.9200, -2.8295]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running 'Albert' through the RNN\")\n",
    "input_tensor = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input_tensor[0], hidden)\n",
    "\n",
    "print(\"\\nLikelihood for each label is:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "# Task 1\n",
    "\n",
    "Write code using pytorch operators to convert these to probabilities by exponentiating them (ie, f(x) = exp(x)). Print the result. You should find that they are all around 5% - 6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0555, 0.0569, 0.0558, 0.0513, 0.0547, 0.0586, 0.0552, 0.0544, 0.0552,\n",
      "         0.0561, 0.0537, 0.0573, 0.0545, 0.0542, 0.0583, 0.0553, 0.0539, 0.0590]],\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "probs = torch.exp(output)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Preparing for Training\n",
    "----------------------\n",
    "\n",
    "Before going into training we should make a few helper functions. The\n",
    "first is to interpret the output of the network, which we know to be a\n",
    "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
    "of the greatest value:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vietnamese', 17)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its\n",
    "language). We use randomness here as training on the same instances in the same order can lead to worse results as we overfit that particular sequence of samples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 examples of randomly choosing data samples:\n",
      "category = Polish / line = Pakulski\n",
      "category = Russian / line = Bazilevsky\n",
      "category = Arabic / line = Kattan\n",
      "category = Czech / line = Klineberg\n",
      "category = German / line = Lawrenz\n",
      "category = French / line = Deschamps\n",
      "category = Italian / line = Ventimiglia\n",
      "category = Czech / line = Camfrlova\n",
      "category = Scottish / line = Millar\n",
      "category = Czech / line = Safko\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "print(\"Here are 10 examples of randomly choosing data samples:\")\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples,\n",
    "have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each loop of training will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and do the calculation, keeping the hidden state for the next letter\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the\n",
    "``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss for plotting. Since there are 1000s\n",
    "of examples we print only every ``print_every`` examples, and take an\n",
    "average of the loss.\n",
    "\n",
    "**NOTE:** This can take 5-10 minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 6s) 2.1383 Connolly / Dutch ✗ (Irish)\n",
      "10000 10% (0m 12s) 2.3332 Arian / Irish ✗ (Arabic)\n",
      "15000 15% (0m 18s) 0.2672 Kringos / Greek ✓\n",
      "20000 20% (0m 24s) 2.5975 Mooney / Czech ✗ (Irish)\n",
      "25000 25% (0m 30s) 0.1170 O'Ryan / Irish ✓\n",
      "30000 30% (0m 37s) 0.1450 Tselios / Greek ✓\n",
      "35000 35% (0m 43s) 0.5168 Sanchez / Spanish ✓\n",
      "40000 40% (0m 49s) 2.0970 Foss / Greek ✗ (French)\n",
      "45000 45% (0m 55s) 0.9471 Bazzi / Arabic ✓\n",
      "50000 50% (1m 1s) 0.9894 Mcguire / Irish ✓\n",
      "55000 55% (1m 7s) 2.8014 Anzai / Arabic ✗ (Japanese)\n",
      "60000 60% (1m 13s) 2.2204 Tamboia / Portuguese ✗ (Italian)\n",
      "65000 65% (1m 20s) 1.3016 Barros / Portuguese ✓\n",
      "70000 70% (1m 26s) 0.2120 Kouros / Greek ✓\n",
      "75000 75% (1m 32s) 0.1702 Mach / Vietnamese ✓\n",
      "80000 80% (1m 38s) 0.3100 Armati / Italian ✓\n",
      "85000 85% (1m 44s) 2.7322 Fleming / German ✗ (Scottish)\n",
      "90000 90% (1m 50s) 0.4360 Vela / Spanish ✓\n",
      "95000 95% (1m 56s) 1.2676 Zuraw / Polish ✓\n",
      "100000 100% (2m 3s) 2.5157 Jia / Japanese ✗ (Chinese)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Results\n",
    "\n",
    "Plotting the historical loss from ``all_losses`` shows the network\n",
    "learning.\n",
    "\n",
    "Note that learning is fast and fairly smooth at first, but then the improvements become smaller and more variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f182268ddc0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Results\n",
    "\n",
    "Let's see how well the model is doing on the training data. We can get a reasonable estimate with just part of the data, so we'll run 1000 samples through the network with `evaluate()`, which is the same as `train()` minus the backpropagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.9%\n"
     ]
    }
   ],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "total = 1000\n",
    "correct = 0\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(total):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    if category_i == guess_i:\n",
    "        correct += 1\n",
    "print(\"{}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score should be around 50-60%, which may seem low, but consider how tricky this task can be!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on User Input\n",
    "\n",
    "This function shows the output for a sample input you can provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Kummerfeld\n",
      "(-0.49) English\n",
      "(-1.82) Polish\n",
      "(-2.69) Greek\n",
      "\n",
      "> Kay\n",
      "(-1.26) Scottish\n",
      "(-1.71) English\n",
      "(-2.12) Irish\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Kummerfeld')\n",
    "predict('Kay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "For more on RNNs, see:\n",
    "\n",
    "- [Chapter 9 of J+M](https://web.stanford.edu/~jurafsky/slp3/9.pdf)\n",
    "- [Chapter 7, section 6 of E](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
    "-  [The Unreasonable Effectiveness of Recurrent Neural\n",
    "   Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)_\n",
    "   shows a bunch of real life examples\n",
    "-  [Understanding LSTM\n",
    "   Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)_\n",
    "   is about LSTMs specifically but also informative about RNNs in\n",
    "   general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Above, we trained and tested on the same data. That is misleading, because the model saw those examples during training.\n",
    "\n",
    "In this task:\n",
    "1. Modify the data reading process to split the data randomly into a test set (\\~10% of the data) and train set (\\~90% of the data). Train and test again.\n",
    "2. Modify `randomTrainingExample` to sample from your training data. Implement a `randomTestExample` to sample from your test data.\n",
    "3. Create a new instance of the model.\n",
    "4. Train that instance with the training data you created.\n",
    "5. Test it with the test data you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 6s) 2.4777 Jivilo / Italian ✗ (Russian)\n",
      "10000 10% (0m 12s) 0.1989 Mcgregor / Scottish ✓\n",
      "15000 15% (0m 18s) 0.1212 Davidson / Scottish ✓\n",
      "20000 20% (0m 24s) 0.3510 Awad / Arabic ✓\n",
      "25000 25% (0m 30s) 0.0289 Seok / Korean ✓\n",
      "30000 30% (0m 37s) 0.8365 Ahearn / Irish ✓\n",
      "35000 35% (0m 43s) 3.1502 Notaro / Japanese ✗ (Italian)\n",
      "40000 40% (0m 49s) 0.0326 Karnoupakis / Greek ✓\n",
      "45000 45% (0m 55s) 1.4151 Sandoval / Irish ✗ (Spanish)\n",
      "50000 50% (1m 1s) 2.1369 Medina / Japanese ✗ (Spanish)\n",
      "55000 55% (1m 8s) 0.8034 Souza / Portuguese ✓\n",
      "60000 60% (1m 14s) 5.4947 Albuquerque / Portuguese ✗ (Spanish)\n",
      "65000 65% (1m 20s) 2.0467 Crocker / Polish ✗ (English)\n",
      "70000 70% (1m 26s) 0.1829 Cathan / Irish ✓\n",
      "75000 75% (1m 32s) 0.1102 Jaskulski / Polish ✓\n",
      "80000 80% (1m 39s) 1.7714 Espino / Italian ✗ (Spanish)\n",
      "85000 85% (1m 45s) 0.2118 Romijnsen / Dutch ✓\n",
      "90000 90% (1m 51s) 1.2395 Aller / Dutch ✓\n",
      "95000 95% (1m 57s) 0.1145 Baldinotti / Italian ✓\n",
      "100000 100% (2m 3s) 2.3663 Costa / Spanish ✗ (Portuguese)\n",
      "46.4%\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import random\n",
    "\n",
    "category_lines_train = {}\n",
    "category_lines_test = {}\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    random.shuffle(lines)\n",
    "    split_point = int(len(lines) * 0.1)\n",
    "    category_lines_train[category] = lines[split_point:]\n",
    "    category_lines_test[category] = lines[:split_point]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines_train[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "    \n",
    "def randomTestExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines_test[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "    \n",
    "new_rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "start = time.time()\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "\n",
    "        \n",
    "total = 1000\n",
    "correct = 0\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(total):\n",
    "    category, line, category_tensor, line_tensor = randomTestExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    if category_i == guess_i:\n",
    "        correct += 1\n",
    "print(\"{}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "The model is entirely linear so far. Modify it to be the basic RNN introduced in lecture 4.\n",
    "\n",
    "In the process, also change the weight initialisation to set them to be random values uniformly distributed in the range (-sqrt(k), sqrt(k)) where k is 1/hidden_size.\n",
    "\n",
    "The cells below contains all the key code from above for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Get files\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Model and Inference\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define the structure of the model\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = math.sqrt(1 / self.hidden_size)\n",
    "        self.i2h.weight.data.uniform_(-initrange, initrange)\n",
    "        self.i2h.bias.data.zero_()\n",
    "        self.h2o.weight.data.uniform_(-initrange, initrange)\n",
    "        self.h2o.bias.data.zero_()\n",
    "        self.h2h.weight.data.uniform_(-initrange, initrange)\n",
    "        self.h2h.bias.data.zero_()\n",
    "        \n",
    "    def initHidden(self):\n",
    "        # Define the initial hidden state for an input as all zeros\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def forward(self, input_tensor, hidden):\n",
    "        # Given an input, compute the steps defined by the model\n",
    "        new_hidden = torch.tanh(self.i2h(input_tensor) + self.h2h(hidden))\n",
    "        output = torch.tanh(self.h2o(new_hidden))\n",
    "        output = self.softmax(output)\n",
    "        return output, new_hidden\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0% (0m 0s) 2.7957 Mcintyre / Scottish ✓\n",
      "1000 1% (0m 1s) 2.8795 Totolos / Scottish ✗ (Greek)\n",
      "1500 1% (0m 2s) 2.8675 Miyake / Scottish ✗ (Japanese)\n",
      "2000 2% (0m 3s) 2.8303 Slusser / German ✓\n",
      "2500 2% (0m 4s) 3.0446 Basara / Japanese ✗ (Arabic)\n",
      "3000 3% (0m 5s) 2.4003 Rudawski / Polish ✓\n",
      "3500 3% (0m 6s) 2.9387 Lieu / Spanish ✗ (Vietnamese)\n",
      "4000 4% (0m 6s) 2.6066 Moon / Chinese ✗ (Korean)\n",
      "4500 4% (0m 7s) 2.1325 Dubanowski / Polish ✓\n",
      "5000 5% (0m 8s) 3.0999 Timpy / Arabic ✗ (Czech)\n",
      "5500 5% (0m 9s) 2.5018 Traversa / Czech ✗ (Italian)\n",
      "6000 6% (0m 10s) 2.4733 Travers / Dutch ✗ (French)\n",
      "6500 6% (0m 11s) 2.4486 Cuocco / Portuguese ✗ (Italian)\n",
      "7000 7% (0m 12s) 2.0431 Yong / Chinese ✓\n",
      "7500 7% (0m 12s) 1.9658 Wan / Korean ✗ (Chinese)\n",
      "8000 8% (0m 13s) 2.6405 Kolen / Arabic ✗ (Dutch)\n",
      "8500 8% (0m 14s) 2.2362 Kouches / Greek ✓\n",
      "9000 9% (0m 15s) 2.3719 Campbell / English ✓\n",
      "9500 9% (0m 16s) 2.2705 Lecuyer / German ✗ (French)\n",
      "10000 10% (0m 17s) 2.3217 Piontek / Czech ✗ (Polish)\n",
      "10500 10% (0m 18s) 3.1668 Futabatei / Italian ✗ (Japanese)\n",
      "11000 11% (0m 18s) 2.0847 Urbina / Italian ✗ (Spanish)\n",
      "11500 11% (0m 19s) 1.7720 Jong / Chinese ✗ (Korean)\n",
      "12000 12% (0m 20s) 2.1267 Stluka / Polish ✗ (Czech)\n",
      "12500 12% (0m 21s) 2.9264 Noakes / Arabic ✗ (English)\n",
      "13000 13% (0m 22s) 1.9204 Hruska / Polish ✗ (Czech)\n",
      "13500 13% (0m 23s) 3.6431 Lis / Korean ✗ (Polish)\n",
      "14000 14% (0m 24s) 2.1497 Grygarova / Japanese ✗ (Czech)\n",
      "14500 14% (0m 24s) 1.7880 Mazuka / Japanese ✓\n",
      "15000 15% (0m 25s) 1.7153 Gu / Korean ✓\n",
      "15500 15% (0m 26s) 1.7881 Shan / Korean ✗ (Chinese)\n",
      "16000 16% (0m 27s) 1.8005 Simonek / Polish ✗ (Czech)\n",
      "16500 16% (0m 28s) 2.3072 Mill / German ✗ (English)\n",
      "17000 17% (0m 29s) 2.2558 Linville / Irish ✗ (French)\n",
      "17500 17% (0m 30s) 1.6424 Takano / Japanese ✓\n",
      "18000 18% (0m 30s) 2.0004 Alves / Portuguese ✓\n",
      "18500 18% (0m 31s) 2.0497 Niadh / Arabic ✗ (Irish)\n",
      "19000 19% (0m 32s) 2.1271 Amelsvoort / German ✗ (Dutch)\n",
      "19500 19% (0m 33s) 1.7663 Won / Korean ✗ (Chinese)\n",
      "20000 20% (0m 34s) 1.6657 Luu / Korean ✗ (Vietnamese)\n",
      "20500 20% (0m 35s) 1.5973 Martelli / Italian ✓\n",
      "21000 21% (0m 36s) 2.0223 Mclaughlin / Russian ✗ (Scottish)\n",
      "21500 21% (0m 36s) 2.8672 Rompaey / Irish ✗ (Dutch)\n",
      "22000 22% (0m 37s) 2.0197 Chaput / Arabic ✗ (French)\n",
      "22500 22% (0m 38s) 3.0179 Tiedeman / Scottish ✗ (German)\n",
      "23000 23% (0m 39s) 1.5395 Marchegiano / Italian ✓\n",
      "23500 23% (0m 40s) 1.9768 Gerald / Scottish ✗ (Irish)\n",
      "24000 24% (0m 41s) 1.6537 Lam / Korean ✗ (Chinese)\n",
      "24500 24% (0m 42s) 1.6616 Yu / Vietnamese ✗ (Korean)\n",
      "25000 25% (0m 42s) 2.0207 Harb / Arabic ✓\n",
      "25500 25% (0m 43s) 1.7877 Romao / Portuguese ✓\n",
      "26000 26% (0m 44s) 1.7085 Yoon / Korean ✓\n",
      "26500 26% (0m 45s) 2.0180 Favre / English ✗ (French)\n",
      "27000 27% (0m 46s) 1.3979 Abramkov / Russian ✓\n",
      "27500 27% (0m 47s) 1.8883 Gladchenko / Italian ✗ (Russian)\n",
      "28000 28% (0m 48s) 2.0711 Bertrand / German ✗ (French)\n",
      "28500 28% (0m 48s) 2.4347 Houte / English ✗ (Dutch)\n",
      "29000 28% (0m 49s) 2.5459 Duchamps / Portuguese ✗ (French)\n",
      "29500 29% (0m 50s) 1.7053 Noguerra / Spanish ✓\n",
      "30000 30% (0m 51s) 1.5196 Paszek / Polish ✓\n",
      "30500 30% (0m 52s) 3.5601 Mas / Chinese ✗ (Dutch)\n",
      "31000 31% (0m 53s) 1.6948 Palmeiro / Italian ✗ (Portuguese)\n",
      "31500 31% (0m 54s) 3.1696 Noach / Arabic ✗ (English)\n",
      "32000 32% (0m 54s) 2.0550 Bacon / Scottish ✗ (English)\n",
      "32500 32% (0m 55s) 1.4489 Kuijpers / Dutch ✓\n",
      "33000 33% (0m 56s) 2.6498 Cousineau / Greek ✗ (French)\n",
      "33500 33% (0m 57s) 2.7092 Austen / Dutch ✗ (English)\n",
      "34000 34% (0m 58s) 1.6479 Medeiros / Portuguese ✓\n",
      "34500 34% (0m 59s) 2.4577 Hardy / Irish ✗ (French)\n",
      "35000 35% (1m 0s) 2.3330 Lorentz / Spanish ✗ (German)\n",
      "35500 35% (1m 1s) 2.0683 Garb / Arabic ✗ (German)\n",
      "36000 36% (1m 1s) 1.2930 Eliopoulos / Greek ✓\n",
      "36500 36% (1m 2s) 1.8180 Weeber / German ✓\n",
      "37000 37% (1m 3s) 1.8539 Kerwer / German ✓\n",
      "37500 37% (1m 4s) 1.6798 Curran / Irish ✓\n",
      "38000 38% (1m 5s) 1.7459 Pho / Chinese ✗ (Vietnamese)\n",
      "38500 38% (1m 6s) 1.7863 Thao / Chinese ✗ (Vietnamese)\n",
      "39000 39% (1m 6s) 1.2254 Kotsiopoulos / Greek ✓\n",
      "39500 39% (1m 7s) 3.1385 Tsai  / Arabic ✗ (Korean)\n",
      "40000 40% (1m 8s) 2.0686 Desrosiers / Dutch ✗ (French)\n",
      "40500 40% (1m 9s) 2.1550 Gray / English ✗ (Scottish)\n",
      "41000 41% (1m 10s) 1.6706 Mansour / Arabic ✓\n",
      "41500 41% (1m 11s) 2.3905 Lecce / English ✗ (Italian)\n",
      "42000 42% (1m 12s) 2.0937 Grant / French ✗ (Scottish)\n",
      "42500 42% (1m 13s) 3.0335 Penzig / Scottish ✗ (German)\n",
      "43000 43% (1m 13s) 1.4645 Manos / Greek ✓\n",
      "43500 43% (1m 14s) 1.8031 Pinho / Italian ✗ (Portuguese)\n",
      "44000 44% (1m 15s) 1.6442 Coelho / Italian ✗ (Portuguese)\n",
      "44500 44% (1m 16s) 1.7493 Huynh / Vietnamese ✓\n",
      "45000 45% (1m 17s) 3.5366 Park  / Polish ✗ (Korean)\n",
      "45500 45% (1m 18s) 1.4003 Krakowski / Polish ✓\n",
      "46000 46% (1m 19s) 3.3615 Dale / French ✗ (Dutch)\n",
      "46500 46% (1m 19s) 1.5951 Turati / Japanese ✗ (Italian)\n",
      "47000 47% (1m 20s) 1.6049 D'cruz / Spanish ✓\n",
      "47500 47% (1m 21s) 1.4670 Macghabhann / Irish ✓\n",
      "48000 48% (1m 22s) 1.8687 Farber / German ✓\n",
      "48500 48% (1m 23s) 2.1649 Schwenke / English ✗ (German)\n",
      "49000 49% (1m 24s) 1.9318 Szewc / German ✗ (Polish)\n",
      "49500 49% (1m 25s) 2.6574 Tsai  / Arabic ✗ (Korean)\n",
      "50000 50% (1m 25s) 1.7203 Eoin / Irish ✓\n",
      "50500 50% (1m 26s) 2.0121 Widerlechner / German ✗ (Czech)\n",
      "51000 51% (1m 27s) 1.2363 Drivakis / Greek ✓\n",
      "51500 51% (1m 28s) 2.5622 Haruguchi / Italian ✗ (Japanese)\n",
      "52000 52% (1m 29s) 1.6899 Yun / Chinese ✗ (Korean)\n",
      "52500 52% (1m 30s) 3.6436 Changli / Italian ✗ (Russian)\n",
      "53000 53% (1m 31s) 1.2697 Christodoulou / Greek ✓\n",
      "53500 53% (1m 31s) 1.2320 Chrysanthopoulos / Greek ✓\n",
      "54000 54% (1m 32s) 1.5752 Gim / Korean ✗ (Chinese)\n",
      "54500 54% (1m 33s) 1.4870 Piatek / Polish ✓\n",
      "55000 55% (1m 34s) 1.5096 Dvorak / Polish ✗ (Czech)\n",
      "55500 55% (1m 35s) 1.8490 Hunter / German ✗ (Scottish)\n",
      "56000 56% (1m 36s) 1.7315 Chun / Korean ✓\n",
      "56500 56% (1m 37s) 1.5488 Yim / Korean ✓\n",
      "57000 56% (1m 38s) 1.8167 Caron / Irish ✗ (French)\n",
      "57500 57% (1m 38s) 1.3740 Simpson / Scottish ✓\n",
      "58000 57% (1m 39s) 1.7117 Luc / Korean ✗ (Vietnamese)\n",
      "58500 58% (1m 40s) 1.6556 Van / Chinese ✗ (Vietnamese)\n",
      "59000 59% (1m 41s) 1.8218 Koumans / Dutch ✓\n",
      "59500 59% (1m 42s) 1.8864 Samuel / Arabic ✗ (French)\n",
      "60000 60% (1m 43s) 1.7048 Rodrigues / Portuguese ✓\n",
      "60500 60% (1m 44s) 1.6395 Mai / Chinese ✗ (Vietnamese)\n",
      "61000 61% (1m 44s) 1.6399 Luu / Korean ✗ (Vietnamese)\n",
      "61500 61% (1m 45s) 2.0919 Hartl / German ✗ (Czech)\n",
      "62000 62% (1m 46s) 3.0279 Tomas / Arabic ✗ (Spanish)\n",
      "62500 62% (1m 47s) 1.5391 Kijek / Polish ✓\n",
      "63000 63% (1m 48s) 1.6303 Jin / Korean ✗ (Chinese)\n",
      "63500 63% (1m 49s) 1.5955 Coelho / Portuguese ✓\n",
      "64000 64% (1m 50s) 1.5221 Won / Korean ✗ (Chinese)\n",
      "64500 64% (1m 50s) 2.8769 Qureshi / Japanese ✗ (Arabic)\n",
      "65000 65% (1m 51s) 1.6341 Zhang / Vietnamese ✗ (Chinese)\n",
      "65500 65% (1m 52s) 3.8060 Richard / Scottish ✗ (Dutch)\n",
      "66000 66% (1m 53s) 1.3619 Shamon / Arabic ✓\n",
      "66500 66% (1m 54s) 1.3428 Zharihin / Russian ✓\n",
      "67000 67% (1m 55s) 1.6577 Dao / Chinese ✗ (Vietnamese)\n",
      "67500 67% (1m 56s) 1.2184 Karnoupakis / Greek ✓\n",
      "68000 68% (1m 56s) 3.3399 Virvitsiotti / Japanese ✗ (Russian)\n",
      "68500 68% (1m 57s) 1.4741 Kwang  / Korean ✓\n",
      "69000 69% (1m 58s) 1.4918 Nunes / Portuguese ✓\n",
      "69500 69% (1m 59s) 2.0474 Aiza / Japanese ✗ (Spanish)\n",
      "70000 70% (2m 0s) 1.6350 Ventura / Spanish ✗ (Portuguese)\n",
      "70500 70% (2m 1s) 2.0061 Kerr / German ✗ (Scottish)\n",
      "71000 71% (2m 2s) 1.6850 Graner / German ✓\n",
      "71500 71% (2m 2s) 2.1965 Clark / Scottish ✗ (Irish)\n",
      "72000 72% (2m 3s) 2.1202 Mcintosh / Arabic ✗ (Scottish)\n",
      "72500 72% (2m 4s) 2.2626 Brodbeck / Polish ✗ (German)\n",
      "73000 73% (2m 5s) 2.1483 Martell / English ✗ (German)\n",
      "73500 73% (2m 6s) 1.6298 De santigo / Spanish ✓\n",
      "74000 74% (2m 7s) 1.8286 Wright / English ✗ (Scottish)\n",
      "74500 74% (2m 8s) 1.9145 Magalhaes / Portuguese ✓\n",
      "75000 75% (2m 8s) 2.2782 Mulder / German ✗ (Dutch)\n",
      "75500 75% (2m 9s) 1.6463 Cardozo / Italian ✗ (Portuguese)\n",
      "76000 76% (2m 10s) 2.7124 Moreno / Italian ✗ (Portuguese)\n",
      "76500 76% (2m 11s) 1.3535 Urogataya / Japanese ✓\n",
      "77000 77% (2m 12s) 1.5777 Ko / Korean ✓\n",
      "77500 77% (2m 13s) 1.5712 Geracimos / Greek ✓\n",
      "78000 78% (2m 14s) 1.6597 Taira / Japanese ✓\n",
      "78500 78% (2m 14s) 2.1448 Garbett / German ✗ (English)\n",
      "79000 79% (2m 15s) 2.7198 Marquerink / Irish ✗ (Dutch)\n",
      "79500 79% (2m 16s) 1.3558 Ziemniak / Polish ✓\n",
      "80000 80% (2m 17s) 2.0656 Atter / Arabic ✗ (English)\n",
      "80500 80% (2m 18s) 1.3776 Kowalski / Polish ✓\n",
      "81000 81% (2m 19s) 1.5779 Hoang / Chinese ✗ (Vietnamese)\n",
      "81500 81% (2m 20s) 1.9114 Leccese / French ✗ (Italian)\n",
      "82000 82% (2m 20s) 2.4752 Malec / Spanish ✗ (Czech)\n",
      "82500 82% (2m 21s) 1.7847 Naser / Arabic ✓\n",
      "83000 83% (2m 22s) 1.9748 Tifft / Czech ✗ (German)\n",
      "83500 83% (2m 23s) 3.1467 Stegon / English ✗ (Czech)\n",
      "84000 84% (2m 24s) 1.4375 Waclauska / Polish ✗ (Czech)\n",
      "84500 84% (2m 25s) 1.5691 Santos / Greek ✗ (Portuguese)\n",
      "85000 85% (2m 26s) 1.5799 Duryagin / Irish ✗ (Russian)\n",
      "85500 85% (2m 26s) 1.5674 Pan / Chinese ✓\n",
      "86000 86% (2m 27s) 2.7992 Santos / Portuguese ✗ (Spanish)\n",
      "86500 86% (2m 28s) 1.4196 Kalogeria / Greek ✓\n",
      "87000 87% (2m 29s) 1.2275 Zdunowski / Polish ✓\n",
      "87500 87% (2m 30s) 1.2060 Papadopulos / Greek ✓\n",
      "88000 88% (2m 31s) 1.4933 Slepicka / Czech ✓\n",
      "88500 88% (2m 32s) 1.6440 Salib / Arabic ✓\n",
      "89000 89% (2m 33s) 1.2780 Shimaoka / Japanese ✓\n",
      "89500 89% (2m 33s) 1.2094 Bencivenni / Italian ✓\n",
      "90000 90% (2m 34s) 1.3032 Trieu / Vietnamese ✓\n",
      "90500 90% (2m 35s) 1.5190 Touma / Arabic ✓\n",
      "91000 91% (2m 36s) 3.0082 Hughes / English ✗ (Scottish)\n",
      "91500 91% (2m 37s) 3.5611 Mencher / German ✗ (Polish)\n",
      "92000 92% (2m 38s) 1.6586 Obando / Italian ✗ (Spanish)\n",
      "92500 92% (2m 39s) 1.4294 Youj / Korean ✓\n",
      "93000 93% (2m 39s) 1.6586 Saliba / Arabic ✓\n",
      "93500 93% (2m 40s) 1.6904 Bohmer / German ✓\n",
      "94000 94% (2m 41s) 1.6219 Luu / Korean ✗ (Vietnamese)\n",
      "94500 94% (2m 42s) 1.6183 Casales / Spanish ✓\n",
      "95000 95% (2m 43s) 1.3453 Adamou / Greek ✓\n",
      "95500 95% (2m 44s) 1.6177 Jedynak / Czech ✗ (Polish)\n",
      "96000 96% (2m 45s) 1.7225 Mcintyre / French ✗ (Scottish)\n",
      "96500 96% (2m 45s) 1.6328 Dong / Chinese ✓\n",
      "97000 97% (2m 46s) 1.6101 Cassano / Italian ✓\n",
      "97500 97% (2m 47s) 1.7773 Ponec / Dutch ✗ (Czech)\n",
      "98000 98% (2m 48s) 2.0499 Duarte / French ✗ (Portuguese)\n",
      "98500 98% (2m 49s) 1.2524 Cheptsov / Russian ✓\n",
      "99000 99% (2m 50s) 1.7113 Soto / Italian ✗ (Spanish)\n",
      "99500 99% (2m 51s) 1.4694 Favreau / French ✓\n",
      "100000 100% (2m 51s) 1.4988 Santos / Greek ✗ (Portuguese)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 500 # note, we decreased this just to get more frequent updates, leaving it at 5000 is fine\n",
    "plot_every = 1000\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.1%\n"
     ]
    }
   ],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "total = 1000\n",
    "correct = 0\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(total):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    if category_i == guess_i:\n",
    "        correct += 1\n",
    "print(\"{}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 8s) 1.8619 Baroch / Irish ✗ (Czech)\n",
      "10000 10% (0m 17s) 1.3567 Chavarria / Spanish ✓\n",
      "15000 15% (0m 25s) 1.6375 Que / Chinese ✓\n",
      "20000 20% (0m 34s) 1.5945 Ryoo / Korean ✓\n",
      "25000 25% (0m 42s) 3.1197 Royer / German ✗ (French)\n",
      "30000 30% (0m 51s) 1.6196 Rong / Chinese ✓\n",
      "35000 35% (1m 0s) 1.5369 Halin / Russian ✓\n",
      "40000 40% (1m 8s) 1.9003 Gilmour / French ✗ (English)\n",
      "45000 45% (1m 17s) 1.5100 Reid / Scottish ✓\n",
      "50000 50% (1m 25s) 1.3344 Cermak / Czech ✓\n",
      "55000 55% (1m 34s) 1.4657 Colombo / Italian ✓\n",
      "60000 60% (1m 43s) 1.4152 Hoang / Vietnamese ✓\n",
      "65000 65% (1m 51s) 1.5286 Hang / Chinese ✓\n",
      "70000 70% (2m 0s) 1.2001 Ferreira / Portuguese ✓\n",
      "75000 75% (2m 8s) 1.2943 Raffel / Czech ✓\n",
      "80000 80% (2m 17s) 1.3050 Lucassen / Dutch ✓\n",
      "85000 85% (2m 26s) 1.6947 Wagner / Czech ✗ (German)\n",
      "90000 90% (2m 34s) 1.4337 Hanek / Czech ✓\n",
      "95000 95% (2m 43s) 1.5544 Maria / Spanish ✓\n",
      "100000 100% (2m 51s) 1.2064 Kurmochi / Japanese ✓\n",
      "51.3%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "category_lines_train = {}\n",
    "category_lines_test = {}\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    random.shuffle(lines)\n",
    "    split_point = int(len(lines) * 0.1)\n",
    "    category_lines_train[category] = lines[split_point:]\n",
    "    category_lines_test[category] = lines[:split_point]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines_train[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "    \n",
    "def randomTestExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines_test[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "    \n",
    "new_rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "start = time.time()\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "\n",
    "total = 1000\n",
    "correct = 0\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(total):\n",
    "    category, line, category_tensor, line_tensor = randomTestExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    if category_i == guess_i:\n",
    "        correct += 1\n",
    "print(\"{}%\".format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
