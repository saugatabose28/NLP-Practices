{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 10\n",
                "\n",
                "Today, we'll explore a vector database - Pinecone. It is based on [two tutorials](https://docs.pinecone.io/examples/notebooks) from the Pinecone developers.\n",
                "\n",
                "## Setup\n",
                "\n",
                "If you haven't already, set up accounts with Pinecone and OpenAI:\n",
                "\n",
                "Pinecone - https://app.pinecone.io\n",
                "\n",
                "Once you create an account they will ask you a few questions to get it set up. Choose Python as the language. The rest you can answer however you like.\n",
                "\n",
                "OpenAI API - https://platform.openai.com/overview\n",
                "\n",
                "Note, if you are creating a new account you should get some free credits that you can use. If you already have an account, the cost for the lab should be very small.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# api key from app.pinecone.io\n",
                "pinecone_api_key = 'INSERT_YOUR_PINECONE_KEY_HERE'\n",
                "\n",
                "# api key from platform.openai.com\n",
                "openai_api_key = 'INSERT_YOUR_OPENAI_KEY_HERE'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's install the libraries we need:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -qU \\\n",
                "  openai==0.27.7 \\\n",
                "  pinecone-client==3.1.0 \\\n",
                "  pinecone-datasets==0.7.0 \\\n",
                "  sentence-transformers==2.2.2 \\\n",
                "  tqdm"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note, some Windows users have also found that they needed to isntall `pyarrow-11.0.0`\n",
                "\n",
                "## Data Download"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this notebook we will use a pre-processed dataset from Pinecone Datasets.\n",
                "\n",
                "If you are curious about what pre-processing they did. see [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/search/semantic-search/semantic-search.ipynb)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pinecone_datasets import load_dataset\n",
                "\n",
                "dataset = load_dataset('quora_all-MiniLM-L6-bm25')\n",
                "\n",
                "# we drop metadata as will use blob column\n",
                "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
                "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
                "\n",
                "# we will use 80K rows of the dataset between rows 240K -\u003e 320K\n",
                "dataset.documents.drop(dataset.documents.index[320_000:], inplace=True)\n",
                "dataset.documents.drop(dataset.documents.index[:240_000], inplace=True)\n",
                "\n",
                "# Print out a sample from the dataset to show what we are working with\n",
                "dataset.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(len(dataset))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating an Index\n",
                "\n",
                "Now the data is ready, we can set up our index to store it.\n",
                "\n",
                "We begin by initializing our connection to Pinecone. This is where your API key is needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pinecone import Pinecone\n",
                "\n",
                "# configure client\n",
                "pc = Pinecone(api_key=api_key)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we set up our index specification. This allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all available providers and regions [here](https://docs.pinecone.io/docs/projects)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pinecone import ServerlessSpec\n",
                "\n",
                "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
                "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
                "\n",
                "spec = ServerlessSpec(cloud=cloud, region=region)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we create a new index called `semantic-search-fast`. It's important that we align the index `dimension` and `metric` parameters with those required by the `MiniLM-L6` model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "index_name = 'semantic-search-fast'\n",
                "\n",
                "existing_indexes = [\n",
                "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
                "]\n",
                "\n",
                "# check if index already exists (it shouldn't if this is first time)\n",
                "if index_name not in existing_indexes:\n",
                "    # if does not exist, create index\n",
                "    pc.create_index(\n",
                "        index_name,\n",
                "        dimension=384,  # dimensionality of minilm\n",
                "        metric='dotproduct',\n",
                "        spec=spec\n",
                "    )\n",
                "    # wait for index to be initialized\n",
                "    while not pc.describe_index(index_name).status['ready']:\n",
                "        time.sleep(1)\n",
                "\n",
                "# connect to index\n",
                "index = pc.Index(index_name)\n",
                "time.sleep(1)\n",
                "# view index stats\n",
                "index.describe_index_stats()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Upsert the data to put it in the database (this can take 2-5 minutes):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.auto import tqdm\n",
                "\n",
                "for batch in tqdm(dataset.iter_documents(batch_size=500), total=160):\n",
                "    index.upsert(batch)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Making Queries"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that our index is populated we can begin making queries. We are performing a semantic search for *similar questions*, so we should embed and search with another question.\n",
                "\n",
                "Note that we use the same model as the one used above. That's critical - otherwise the vector spaces will not be meaningfully comparable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "import torch\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
                "model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"which city has the highest population in the world?\"\n",
                "\n",
                "# create the query vector\n",
                "xq = model.encode(query).tolist()\n",
                "\n",
                "# now query\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "xc"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the returned response `xc` we can see the most relevant questions to our particular query — we don't have any exact matches but we can see that the returned questions are similar in the topics they are asking about. We can reformat this response to be a little easier to read:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These are good results, let's try and modify the words being used to see if we still surface similar results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"which metropolis has the highest number of people?\"\n",
                "\n",
                "# create the query vector\n",
                "xq = model.encode(query).tolist()\n",
                "\n",
                "# now query\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we used different terms in our query than that of the returned documents. We substituted **\"city\"** for **\"metropolis\"** and **\"populated\"** for **\"number of people\"**.\n",
                "\n",
                "Despite these very different terms and *lack* of term overlap between query and returned documents — we get highly relevant results — this is the power of *semantic search*.\n",
                "\n",
                "## Task 1\n",
                "\n",
                "Try changing the model and querying again. You can find alternative models [here](https://sbert.net/docs/pretrained_models.html). Note that you will need to choose one with the same dimensionality (384). Clicking on the \"info\" symbol next to the model names will tell you information including their dimensionality.\n",
                "\n",
                "Find a model that gives similar results and a model that gives different results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO\n",
                "\n",
                "# Solution\n",
                "\n",
                "query = \"which metropolis has the highest number of people?\"\n",
                "\n",
                "# Similar\n",
                "model2 = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2', device=device)\n",
                "xq = model2.encode(query).tolist()\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "print(\"Similar\")\n",
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")\n",
                "print()\n",
                "\n",
                "# Different\n",
                "model3 = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2', device=device)\n",
                "xq = model3.encode(query).tolist()\n",
                "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
                "print(\"Different\")\n",
                "for result in xc['matches']:\n",
                "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Retrieval Enhanced Generative Question Answering\n",
                "\n",
                "Next, we will see how these queries can be used with an LLM to generate better outputs.\n",
                "\n",
                "We will again use data that has already been prepared (for details, see [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/generation/openai/gen-qa-openai.ipynb))."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pinecone_datasets import load_dataset\n",
                "\n",
                "dataset = load_dataset('youtube-transcripts-text-embedding-ada-002')\n",
                "\n",
                "# we drop sparse_values as they are not needed for this example\n",
                "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
                "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
                "\n",
                "# Print a sample of the data\n",
                "dataset.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Again, we will set up a pinecone database:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "index_name = 'gen-qa-openai-fast'\n",
                "# check if index already exists (it shouldn't if this is first time)\n",
                "if index_name not in pc.list_indexes().names():\n",
                "    # if does not exist, create index\n",
                "    pc.create_index(\n",
                "        index_name,\n",
                "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
                "        metric='cosine',\n",
                "        spec=spec\n",
                "    )\n",
                "# connect to index\n",
                "index = pc.Index(index_name)\n",
                "# view index stats\n",
                "index.describe_index_stats()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As in the previous section, we'll insert the data into the database (this can take 5-10 minutes):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "for batch in dataset.iter_documents(batch_size=100):\n",
                "    index.upsert(batch)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation.\n",
                "\n",
                "## Retrieval\n",
                "\n",
                "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs. To create that query vector we must initialize a `text-embedding-ada-002` embedding model with OpenAI. For this, you need an [OpenAI API key](https://platform.openai.com/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import openai\n",
                "\n",
                "openai.api_key = openai_api_key\n",
                "\n",
                "embed_model = \"text-embedding-ada-002\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = (\n",
                "    \"Which training method should I use for sentence transformers when \" +\n",
                "    \"I only have pairs of related sentences?\"\n",
                ")\n",
                "\n",
                "res = openai.Embedding.create(\n",
                "    input=[query],\n",
                "    engine=embed_model\n",
                ")\n",
                "\n",
                "# retrieve from Pinecone\n",
                "xq = res['data'][0]['embedding']\n",
                "\n",
                "# get relevant contexts (including the questions)\n",
                "res = index.query(vector=xq, top_k=2, include_metadata=True)\n",
                "\n",
                "res"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We write some functions to handle the retrieval and completion steps:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "limit = 3750\n",
                "\n",
                "import time\n",
                "\n",
                "def retrieve(query):\n",
                "    res = openai.Embedding.create(\n",
                "        input=[query],\n",
                "        engine=embed_model\n",
                "    )\n",
                "\n",
                "    # retrieve from Pinecone\n",
                "    xq = res['data'][0]['embedding']\n",
                "\n",
                "    # get relevant contexts\n",
                "    contexts = []\n",
                "    time_waited = 0\n",
                "    while (len(contexts) \u003c 3 and time_waited \u003c 60 * 12):\n",
                "        res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
                "        contexts = contexts + [\n",
                "            x['metadata']['text'] for x in res['matches']\n",
                "        ]\n",
                "        print(f\"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...\")\n",
                "        time.sleep(15)\n",
                "        time_waited += 15\n",
                "\n",
                "    if time_waited \u003e= 60 * 12:\n",
                "        print(\"Timed out waiting for contexts to be retrieved.\")\n",
                "        contexts = [\"No contexts retrieved. Try to answer the question yourself!\"]\n",
                "\n",
                "\n",
                "    # build our prompt with the retrieved contexts included\n",
                "    prompt_start = (\n",
                "        \"Answer the question based on the context below.\\n\\n\"+\n",
                "        \"Context:\\n\"\n",
                "    )\n",
                "    prompt_end = (\n",
                "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
                "    )\n",
                "    # append contexts until hitting limit\n",
                "    for i in range(1, len(contexts)):\n",
                "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) \u003e= limit:\n",
                "            prompt = (\n",
                "                prompt_start +\n",
                "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
                "                prompt_end\n",
                "            )\n",
                "            break\n",
                "        elif i == len(contexts)-1:\n",
                "            prompt = (\n",
                "                prompt_start +\n",
                "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
                "                prompt_end\n",
                "            )\n",
                "    return prompt\n",
                "\n",
                "\n",
                "def complete(prompt):\n",
                "    # instructions\n",
                "    sys_prompt = \"You are a helpful assistant that always answers questions.\"\n",
                "    # query text-davinci-003\n",
                "    res = openai.ChatCompletion.create(\n",
                "        model='gpt-3.5-turbo-0613',\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": sys_prompt},\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ],\n",
                "        temperature=0\n",
                "    )\n",
                "    return res['choices'][0]['message']['content'].strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# first we retrieve relevant items from Pinecone\n",
                "query_with_contexts = retrieve(query)\n",
                "query_with_contexts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# then we complete the context-infused query\n",
                "complete(query_with_contexts)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And we get a pretty great answer straight away, specifying to use _multiple-rankings loss_ (also called _multiple negatives ranking loss_).\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2\n",
                "\n",
                "Try adjusting the number of contexts down to 1, to see the impact on retrieval quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO\n",
                "\n",
                "# Solution\n",
                "\n",
                "limit = 1\n",
                "\n",
                "query_with_contexts = retrieve(query)\n",
                "print(query_with_contexts)\n",
                "\n",
                "complete(query_with_contexts)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pack up\n",
                "\n",
                "Once you're done with the lab, delete the indices to save resources:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pc.delete_index('gen-qa-openai-fast')\n",
                "pc.delete_index('semantic-search-fast')"
            ]
        }
    ]
}
